---
title: "ARMA/ARIMA/SARIMA Models"
navbar:
    left:
      
      - about.qmd
      
      - Introduction.qmd
      - DataSources.qmd
      - DataVis.qmd
      - EDA.qmd
      - ARModels.qmd
      - ASV.qmd
    
      - GARCH.qmd
      - TS.qmd
      - conclusion.qmd
      - dv.qmd

format:
  html:
    theme: sandstone
    css: ./styles/layout.css
    code-fold: true
    toc: true
---

After Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation. 

To be more specific, we need to understand the concepts about the models:
Certainly! Here's a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:

1. **ARMA**:

   - **Components**:
     - **AR(p)**: AutoRegressive term of order \( p \). It describes the relationship of the current observation with its previous values.
     - **MA(q)**: Moving Average term of order \( q \). It captures the relationship between an observation and the white noise or error terms from previous time points.
     
   - **Equation**:
     \[
     X_t = c + \phi_1X_{t-1} + \phi_2X_{t-2} + \dots + \phi_pX_{t-p} + \theta_1a_{t-1} + \theta_2a_{t-2} + \dots + \theta_qa_{t-q} + a_t
     \]
     where \( a_t \) is white noise.

2. **ARIMA**:

   - **Components**:
     - **AR(p)**: AutoRegressive term.
     - **I(d)**: Integrated term of order \( d \). It represents the number of differences needed to make the series stationary.
     - **MA(q)**: Moving Average term.
     
   - **Equation**:
     Once the series is differenced \( d \) times and becomes stationary, its ARIMA representation becomes the same as ARMA, but applied to the differenced series.

3. **SARIMA**:

   - **Components**:
     - **AR(p)**: AutoRegressive term.
     - **I(d)**: Integrated term.
     - **MA(q)**: Moving Average term.
     - **SAR(P)**: Seasonal AutoRegressive term of order \( P \).
     - **SI(D)**: Seasonal Integrated term of order \( D \). It indicates the number of seasonal differences required.
     - **SMA(Q)**: Seasonal Moving Average term of order \( Q \).
     
   - **Equation**:
     The SARIMA model combines both non-seasonal and seasonal components. Once both seasonal and non-seasonal differencing are applied (if needed), the SARIMA equation becomes an extension of the ARMA equation but with additional seasonal terms.
     \[
     X_t = c + \phi_1X_{t-1} + \dots + \phi_pX_{t-p} + \theta_1a_{t-1} + \dots + \theta_qa_{t-q} + \Phi_1X_{t-P} + \dots + \Phi_PX_{t-PS} + \Theta_1a_{t-Q} + \dots + \Theta_Qa_{t-QS} + a_t
     \]
     where \( S \) represents the seasonal period and \( a_t \) is white noise.


# ARMA/ARIMA Analysis (With Covid-19 Impacts)

## The Household Saving Data Analysis

Due to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(reticulate)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(forecast)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(lubridate)
library(gridExtra)
library(plotly)
library(TTR) # For the SMA function

```


```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
df <- read.csv("../Dataset/project/household_saving.csv")
df$DATE <- as.Date(df$DATE)
head(df)
```

```{r,warning=FALSE,echo=FALSE}
library(ggplot2)


ggplot(data = df, aes(x = DATE, y = W398RC1A027NBEA)) +
  geom_line(color = "DarkViolet") +
  labs(title = "Time Series Plot of Household Saving",
       x = "Date", 
       y = "The Household Saving") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))


```






### ACF & PACF
```{r, echo=FALSE}
library(forecast)

# ACF Plot
ggAcf(df$W398RC1A027NBEA) +
  labs(title = "ACF of Household Saving Time Series") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        plot.background = element_rect(fill = "#E0E0E0"))

# PACF Plot
ggPacf(df$W398RC1A027NBEA) +
  labs(title = "PACF of Household Saving Time Series") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        plot.background = element_rect(fill = "#E0E0E0"))

```

From here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.


### Validation with ADF Test
```{r, echo=FALSE}
library(tseries)
adf.test(df$W398RC1A027NBEA)


```

Based on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.

Then, to explore more, we utilize detrended and log-transformation to see about the patterns


### Detrended and Log-transformed
```{r, echo=FALSE}
detrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$W398RC1A027NBEA))

# Enhanced Detrended Plot
ggplot(data = detrended_data, aes(x = Date, y = Detrended)) +
  geom_line(color = "blue") +
  labs(title = "Detrended Household Saving Time Series",
       x = "Date",
       y = "Detrended Value") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

```


```{r,warning=FALSE, echo=FALSE}
log_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$W398RC1A027NBEA))

# Enhanced Log-transformed Plot with Custom Background
ggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +
  geom_line(color = "blue") +
  labs(title = "Log-transformed Time Series of Household Saving",
       x = "Date",
       y = "Log-transformed Value") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

```

From here, we can see that after detrending and transforming, the household Saving remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.

### Test it stationary again
```{r, echo=FALSE}

adf.test(log_transformed_data$LogTransformed)


```

After the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.

```{r, echo = FALSE}
fit = lm(df$W398RC1A027NBEA~time(df$W398RC1A027NBEA), na.action=NULL) 

```

### Make it Stationary by using differencing

We will try the second order differencing to see the result since the dataset seems to be overly not stationary.

```{r, echo = FALSE}
#install.packages("gridExtra")

```

```{r, echo = FALSE}

t = ts(diff(diff(df$W398RC1A027NBEA)))
# Create the time series plot with the desired background and borders
ts_plot <- autoplot(t) +
  labs(title = "Time Series Plot With two Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Extract the ACF and PACF plots without the theme
acf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +
  labs(title = "ACF for two Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +
  labs(title = "PACF for two Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)

```

After the second orders of differencing, we can see the changes regarding the plots as a whole.


```{r, warning=FALSE, echo = FALSE}
adf.test(diff(df$W398RC1A027NBEA, differences = 2))


```

However, the second order of differencing still cannot make it stationary.
We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.

### Third Differencing
```{r, echo = FALSE}

t = ts(diff(diff(diff(df$W398RC1A027NBEA))))
# Create the time series plot with the desired background and borders
ts_plot <- autoplot(t) +
  labs(title = "Time Series Plot With three Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Extract the ACF and PACF plots without the theme
acf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +
  labs(title = "ACF for three Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +
  labs(title = "PACF for three Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)

```





```{r, warning=FALSE, echo = FALSE}
# Load required libraries
library(ggplot2)


# First, perform the augmented Dickey-Fuller test
adf_result <- adf.test(diff(df$W398RC1A027NBEA, differences = 3))

# Print the ADF test result
print(adf_result)
```

Now, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.

### Evaluate the values for p & q
```{r, warning=FALSE, echo = FALSE}
# Set the theme
theme_demo <- theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

# Plot the ACF and PACF of the differenced series with the demo theme
par(mfrow=c(2,1)) # Set up a 2x1 grid for plotting
diff_series = diff(df$W398RC1A027NBEA, differences = 3)
# ACF Plot
acf_plot <- acf(diff_series, plot = FALSE)
print(acf_plot, main="ACF of Differenced Household Saving Time Series")
ggAcf(diff_series) + theme_demo

# PACF Plot
pacf_plot <- pacf(diff_series, plot = FALSE)
print(pacf_plot, main="PACF of Differenced Household Saving Time Series")
ggPacf(diff_series) + theme_demo

```

From the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.





### Fit Model

After we determing our parameters, we can start fitting the models.
```{r, echo = FALSE}
library(forecast)
diff = diff(df$W398RC1A027NBEA, differences = 3)
model <- Arima(diff, order=c(1,3,2))
summary(model)

```

From here, we get a summary about the aic and bic values.

### Equation of the Model

Given the specified values \(p = 1\), \(q = 2\), and \(d = 3\), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:

\[ X_t = c + \phi_1X_{t-1} + \theta_1a_{t-1} + \theta_2a_{t-2} + a_t \]

In my case the third difference is represented as \( \nabla^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} \). The ARIMA equation provided is built upon this differenced series.

### Model diagnostics

Model diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.

```{r,warning=FALSE, echo = FALSE}
## empty list to store model fits
set.seed((150))
arma14 =arima.sim(list(order=c(1,3,1), ar=-.4, ma=c(-.3)), n=10000)
arma14 %>% ggtsdisplay()
ARMA_res <- list()

## set counter
cc <-1

## loop over AR
for(p in 0:3){
  ## loop over MA
  for(q in 0:4){
    
  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,3,q))
  cc<- cc+1
  }
}

## get AIC values for model evaluation

ARMA_AIC<-sapply(ARMA_res,function(x) x$aic)

ARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]
```



```{r,warning=FALSE, echo = FALSE}
d=3
i=1
temp= data.frame()
ls=matrix(rep(NA,6*17),nrow=17) # roughly nrow = 3x4x2


for (p in 3:5)# p=1,2,3 : 3
{
  for(q in 3:5)# q=1,2,3,4 :4
  {
    for(d in 1:3)# d=1,2 :2
    {
      
      if(p-1+d+q-1<=8) #usual threshold
      {
        
        model<- Arima(df$W398RC1A027NBEA,order=c(p-1,d,q-1),include.drift=TRUE) 
        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)
        i=i+1
        #print(i)
        
      }
      
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)
```

From the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.

```{r, echo = FALSE}
temp[which.min(temp$AIC),]
```
```{r, echo = FALSE}
temp[which.min(temp$BIC),]
```
```{r, echo = FALSE}
temp[which.min(temp$AICc),]
```

In addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.

### Model Compare

Now, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):

```{r,warning=FALSE, echo = FALSE}
######### compare 3 models
set.seed(345)
model_output <- capture.output(sarima(df$W398RC1A027NBEA, 2,3,2))
```

```{r, echo = FALSE}
model_output2 <- capture.output(sarima(df$W398RC1A027NBEA, 1,3,2))
```

```{r,warning=FALSE, echo = FALSE}
#################### fitted vs. actual plot  ##############
fit1=Arima(df$W398RC1A027NBEA,order=c(1,3,2),include.drift = TRUE)
fit2=Arima(df$W398RC1A027NBEA,order=c(2,3,2),include.drift = TRUE) #warning for the drift, no drift 
# Set background color
par(bg = "#E0E0E0")

# Plotting the data and model fits with custom background and border colors
plot(df$W398RC1A027NBEA, col="blue", main="Time Series with SARIMA Fits", 
     ylab="Value", xlab="Time", 
     panel.first = rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], 
                        col = "#E0E0E0", border = "#E0E0E0"))

lines(fitted(fit1), col="green")
lines(fitted(fit2), col="red") 

# Adding a legend
legend("topleft", 
       legend = c("The Time Series Of the Data", "fit1", "fit2"), 
       col = c("blue", "green", "red"), 
       lty=1, # Type of the line (1 is for "solid")
       cex=0.8, # Font size of the legend text
       box.col="#E0E0E0", bg="#E0E0E0") 

```

Here, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.


### Forecasting for the dataset using the best parameters
```{r, echo = FALSE}
forecast(fit2,10) # Forecasting for the next 10 minutes
```

```{r, echo = FALSE}


# Autoplot with custom colors
plot_fit2 <- autoplot(forecast(fit2)) + 
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.key = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Display the plot
print(plot_fit2)
```

From the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.

### BENCHMARK

Now for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.

```{r, echo = FALSE}
library(forecast)
diff = diff(df$W398RC1A027NBEA, differences = 3)
model <- Arima(diff, order=c(1,3,2))

```

#### The meanf model with residual plot
```{r, echo = FALSE}
f1<-meanf(df$W398RC1A027NBEA, h=11) #mean

checkresiduals(f1)
```

#### The Arima model
```{r, echo = FALSE}
df$W398RC1A027NBEA %>%
  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% 
  residuals() %>% 
  ggtsdisplay()
```

```{r, echo = FALSE}
fit=Arima(df$W398RC1A027NBEA, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) 
summary(fit)
```

```{r, echo = FALSE}
f1 <- meanf(df$W398RC1A027NBEA, h=10) 

#checkresiduals(f1)

f2 <- naive(df$W398RC1A027NBEA, h=10) 

#checkresiduals(f2)

f3 <- rwf(df$W398RC1A027NBEA,drift=TRUE, h=10) 
```

#### Accuracy of the fitted models
```{r, echo = FALSE}
pred=forecast(fit2,20)
accuracy(pred)
pred[['mean']]
```

#### Accuracy of the based models
```{r, echo = FALSE}
accuracy(f1)
```

```{r, echo = FALSE}
accuracy(f2)
```

```{r, echo = FALSE}
accuracy(f3)
```

Based on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.

```{r, echo = FALSE}
gdp.ts=ts(df$W398RC1A027NBEA)
```

```{r, echo = FALSE}
autoplot(gdp.ts) +
  autolayer(meanf(gdp.ts, h=11),
            series="Mean", PI=FALSE) +
  autolayer(naive(gdp.ts, h=11),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(gdp.ts, h=11),
            series="fit", PI=FALSE) +
  autolayer(forecast(fit2, h=11),
            series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for Household Saving") +
  xlab("Year") + ylab("Household Saving") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    panel.background = element_rect(fill = "#E0E0E0"),
    panel.border = element_rect(fill = NA, color = "#E0E0E0")
  )
```

```{r, echo = FALSE}
autoplot(gdp.ts) +
  autolayer(meanf(gdp.ts, h=20),
            series="Mean.tr", PI=FALSE) +
  autolayer(naive(gdp.ts, h=20),
            series="Naïve.tr", PI=FALSE) +
  autolayer(rwf(gdp.ts, drift=TRUE, h=40),
            series="Drift.tr", PI=FALSE) +
  autolayer(forecast(fit2,20), 
            series="fit",PI=FALSE) +
  ggtitle("Household Saving in US") +
  xlab("Time") + ylab("Household Saving") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    panel.background = element_rect(fill = "#E0E0E0"),
    panel.border = element_rect(fill = NA, color = "#E0E0E0")
  )
```

Based on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending.

## The Median House Sale Price Data Analysis
Due to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.

```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
df <- read.csv("../Dataset/project/MSPUS.csv")
df$DATE <- as.Date(df$DATE)
head(df)
```

```{r,warning=FALSE,echo=FALSE}
library(ggplot2)


ggplot(data = df, aes(x = DATE, y = MSPUS)) +
  geom_line(color = "DarkViolet") +
  labs(title = "Time Series Plot of Household Sale Price",
       x = "Date", 
       y = "The Household Sale Price") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))


```






### ACF & PACF
```{r, echo=FALSE}
library(forecast)

# ACF Plot
ggAcf(df$MSPUS) +
  labs(title = "ACF of Household Sale Price Time Series") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        plot.background = element_rect(fill = "#E0E0E0"))

# PACF Plot
ggPacf(df$MSPUS) +
  labs(title = "PACF of Household Sale Price Time Series") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        plot.background = element_rect(fill = "#E0E0E0"))

```

From here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.


### Validation with ADF Test
```{r, echo=FALSE}
library(tseries)
adf.test(df$MSPUS)


```

Based on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.

Then, to explore more, we utilize detrended and log-transformation to see about the patterns


### Detrended and Log-transformed
```{r, echo=FALSE}
detrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$MSPUS))

# Enhanced Detrended Plot
ggplot(data = detrended_data, aes(x = Date, y = Detrended)) +
  geom_line(color = "blue") +
  labs(title = "Detrended Household Sale Price Time Series",
       x = "Date",
       y = "Detrended Value") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

```


```{r,warning=FALSE, echo=FALSE}
log_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$MSPUS))

# Enhanced Log-transformed Plot with Custom Background
ggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +
  geom_line(color = "blue") +
  labs(title = "Log-transformed Time Series of Household Sale Price",
       x = "Date",
       y = "Log-transformed Value") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

```

From here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.

### Test it stationary again
```{r, echo=FALSE}
library(tseries)
adf.test(log_transformed_data$LogTransformed)


```

After the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.

### Make it Stationary by using differencing


```{r, echo = FALSE}

t = ts(diff(df$MSPUS))
# Create the time series plot with the desired background and borders
ts_plot <- autoplot(t) +
  labs(title = "Time Series Plot With first Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Extract the ACF and PACF plots without the theme
acf_plot <- ggAcf(diff(df$MSPUS)) +
  labs(title = "ACF for first Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(diff(df$MSPUS)) +
  labs(title = "PACF for first Orders of Differencing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)

```


After the first order of differencing, we can see the changes regarding the plots as a whole.


```{r, warning=FALSE, echo = FALSE}
adf.test(diff(df$MSPUS, differences = 1))


```

We can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.

```{r, warning=FALSE, echo = FALSE}
# Load required libraries

# First, perform the augmented Dickey-Fuller test
adf_result <- adf.test(diff(df$MSPUS, differences = 1))



# Set the theme
theme_demo <- theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

# Plot the ACF and PACF of the differenced series with the demo theme
par(mfrow=c(2,1)) # Set up a 2x1 grid for plotting
diff_series = diff(df$MSPUS, differences = 3)
# ACF Plot
acf_plot <- acf(diff_series, plot = FALSE)
print(acf_plot, main="ACF of Differenced Household Sale Price Time Series")
ggAcf(diff_series) + theme_demo

# PACF Plot
pacf_plot <- pacf(diff_series, plot = FALSE)
print(pacf_plot, main="PACF of Differenced Household Sale Price Time Series")
ggPacf(diff_series) + theme_demo

```

From the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.

### Fit model
```{r, echo = FALSE}
library(forecast)
diff = diff(df$MSPUS, differences = 1)
model <- Arima(diff, order=c(1,1,1))
summary(model)

```

### Equation of the Model

Given the specified values \(p = 1\), \(q = 1\), and \(d = 1\), we can write out the ARIMA(1,1,1) model equation using the general equation based on the results of the model:

\[ X_t = c + \phi_1X_{t-1} + \theta_1a_{t-1} + \theta_2a_{t-2} + a_t \]

In my case the third difference is represented as \( \nabla^1 X_t = X_t + 0.0092X_{t-1} - 0.9827Y_{t-1} \). The ARIMA equation provided is built upon this differenced series.

### Model diagnostics

```{r,warning=FALSE, echo = FALSE}
## empty list to store model fits
set.seed((150))
arma14 =arima.sim(list(order=c(1,1,1), ar=-.1, ma=c(-.1)), n=10000)
arma14 %>% ggtsdisplay()
ARMA_res <- list()

## set counter
cc <-1

## loop over AR
for(p in 0:3){
  ## loop over MA
  for(q in 0:4){
    
  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,1,q))
  cc<- cc+1
  }
}

## get AIC values for model evaluation

ARMA_AIC<-sapply(ARMA_res,function(x) x$aic)

ARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]
```

```{r,warning=FALSE, echo = FALSE}
d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2


for (p in 2:3)# p=1,2,3 : 3
{
  for(q in 2:4)# q=1,2,3,4 :4
  {
    for(d in 1:3)# d=1,2 :2
    {
      
      if(p-1+d+q-1<=8) #usual threshold
      {
        
        model<- Arima(df$MSPUS,order=c(p-1,d,q-1),include.drift=TRUE) 
        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)
        i=i+1
        #print(i)
        
      }
      
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)
```


From the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.

```{r, echo = FALSE}
temp[which.min(temp$AIC),]
```
```{r, echo = FALSE}
temp[which.min(temp$BIC),]
```
```{r, echo = FALSE}
temp[which.min(temp$AICc),]
```

Since we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.

### Model Compare

Now, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):
```{r,warning=FALSE, echo = FALSE}
######### compare 3 models
set.seed(345)
model_output <- capture.output(sarima(df$MSPUS, 2,2,3))
```

```{r, echo = FALSE}
model_output2 <- capture.output(sarima(df$MSPUS, 1,1,1))
```

```{r,warning=FALSE, echo = FALSE}
#################### fitted vs. actual plot  ##############
fit1=Arima(df$MSPUS,order=c(2,2,3),include.drift = TRUE)
fit2=Arima(df$MSPUS,order=c(1,1,1),include.drift = TRUE) #warning for the drift, no drift 
# Set background color
par(bg = "#E0E0E0")

# Plotting the data and model fits with custom background and border colors
plot(df$MSPUS, col="blue", main="Time Series with SARIMA Fits", 
     ylab="Value", xlab="Time", 
     panel.first = rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], 
                        col = "#E0E0E0", border = "#E0E0E0"))

lines(fitted(fit1), col="black")
lines(fitted(fit2), col="red") 

# Adding a legend
legend("topleft", 
       legend = c("The Time Series Of the Data", "fit1", "fit2"), 
       col = c("blue", "black", "red"), 
       lty=1, # Type of the line (1 is for "solid")
       cex=0.8, # Font size of the legend text
       box.col="#E0E0E0", bg="#E0E0E0") 

```

Here, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.

### Forecasting for the dataset 
```{r, echo = FALSE}
forecast(fit2,50) # Forecasting for the next 10 minutes
```

```{r, echo = FALSE}


# Autoplot with custom colors
plot_fit2 <- autoplot(forecast(fit2)) + 
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.key = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Display the plot
print(plot_fit2)
```

From the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.

### BENCHMARK

Now for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.

```{r, echo = FALSE}
library(forecast)
diff = diff(df$MSPUS, differences = 1)
model <- Arima(diff, order=c(2,2,3))

```

#### The meanf model with residual plot

```{r, echo = FALSE}
f1<-meanf(df$MSPUS, h=11) #mean

checkresiduals(f1)
```

#### The Arima model

```{r, echo = FALSE}
df$MSPUS %>%
  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% 
  residuals() %>% 
  ggtsdisplay()
```

```{r, echo = FALSE}
fit=Arima(df$MSPUS, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) 
summary(fit)
```

```{r, echo = FALSE}
f1 <- meanf(df$MSPUS, h=10) 

#checkresiduals(f1)

f2 <- naive(df$MSPUS, h=10) 

#checkresiduals(f2)

f3 <- rwf(df$MSPUS,drift=TRUE, h=10) 
```

#### Accuracy of the fitted models
```{r, echo = FALSE}
pred=forecast(fit2,20)
accuracy(pred)
pred[['mean']]
```

#### Accuracy of the based models

```{r, echo = FALSE}
accuracy(f1)
```

```{r, echo = FALSE}
accuracy(f2)
```

```{r, echo = FALSE}
accuracy(f3)
```

Based on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.


```{r, echo = FALSE}
gdp.ts=ts(df$MSPUS)
```

```{r, echo = FALSE}
autoplot(gdp.ts) +
  autolayer(meanf(gdp.ts, h=11),
            series="Mean", PI=FALSE) +
  autolayer(naive(gdp.ts, h=11),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(gdp.ts, h=11),
            series="fit", PI=FALSE) +
  autolayer(forecast(fit2, h=11),
            series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for Household Sale Price") +
  xlab("Year") + ylab("Household Sale Price") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    panel.background = element_rect(fill = "#E0E0E0"),
    panel.border = element_rect(fill = NA, color = "#E0E0E0")
  )
```

```{r, echo = FALSE}
autoplot(gdp.ts) +
  autolayer(meanf(gdp.ts, h=20),
            series="Mean.tr", PI=FALSE) +
  autolayer(naive(gdp.ts, h=20),
            series="Naïve.tr", PI=FALSE) +
  autolayer(rwf(gdp.ts, drift=TRUE, h=40),
            series="Drift.tr", PI=FALSE) +
  autolayer(forecast(fit2,20), 
            series="fit",PI=FALSE) +
  ggtitle("Household Sale Price in US)") +
  xlab("Time") + ylab("Household Sale Price") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    panel.background = element_rect(fill = "#E0E0E0"),
    panel.border = element_rect(fill = NA, color = "#E0E0E0")
  )
```

Based on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the  trending pattern.



# SARIMA Analysis　(Before Covid-19)

## GDP Deflator SARIMA Analysis


```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
df <- read.csv("../Dataset/project/A191RI1Q225SBEA.csv")
df$DATE <- as.Date(df$DATE)
head(df)
```



### Before Covid Period
```{r}
gdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)
gdp_df
```

```{r}


library(ggplot2)


ggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +
  geom_line(color = "DarkViolet") +
  labs(title = "Time Series Plot of GDP Deflator",
       x = "Date", 
       y = "GDP Deflator") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))
   
```

### Check Decomposition

```{r}
# Decompose the time series data
dec <- decompose(gdp_df, type = "multiplicative")  # Choose either "additive" or "multiplicative"

# Set the graphical parameters for the plot
par(bg = "#E0E0E0", col.axis = "#E0E0E0", col.lab = "black", col.main = "black", col.sub = "black")

# Plot the decomposed object
plot(dec)

# Reset the graphical parameters to default
par(bg = "white", col.axis = "black", col.lab = "black", col.main = "black", col.sub = "black")

```

### Check Lag Plot
```{r}
gglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))
```

### Seasonal Difference AND ACF & PACF
This shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.
```{r}
ts_plot <- autoplot(gdp_df) +
  labs(title = "Time Series Plot ") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Extract the ACF and PACF plots without the theme
acf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +
  labs(title = "ACF for first Order of Differencing and Seasonal Differenceing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +
  labs(title = "PACF for first Orders of Differencing and Seasonal Differenceing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             

```


Most spikes are within range, it is stationary. 
ACF Plot :

The sharp drop after lag 1 and some lags across the range. This gives us q = 1,2.
Since there's a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1.
PACF Plot:

The sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1.
The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1.
Order of Differencing:

You mentioned that you applied first order differencing, so d = 1.
You also mentioned seasonal differencing with a lag of 4, so D = 1.
Combining these, we get:

Non-seasonal parameters: p = 1, d = 1, q = 1,2
Seasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4.
Therefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].

WE can continue analysis




### Model Diagnostics

```{r}
######################## Check for different combinations ########
SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  temp=c()
  d=1
  D=1
  s=4
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*35),nrow=35)
  
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  temp
}


  

```


```{r}
# Based on the analysis:

output=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)
knitr::kable(output)

```

### Compare the results
```{r}
output[which.min(output$AIC),] 
```

```{r}
output[which.min(output$BIC),]
```

```{r}
output[which.min(output$AICc),]
```


```{r}

set.seed(236)
model_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))
model_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))
```
The second one is a little better.

```{r}
cat(model_output1[50:80], model_output1[length(model_output1)], sep = "\n") 
```

```{r}
cat(model_output2[40:55], model_output2[length(model_output2)], sep = "\n") 
```

The Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.

The Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.

The Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.

The Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.

$ttable: all coefficients are significant.



### Fit model & Forecasting

```{r}
fit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )
summary(fit2)

```


```{r}
# Autoplot with custom colors
plot_fit_ <- autoplot(forecast(fit2,120)) + 
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.key = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Display the plot
print(plot_fit_)

```



```{r}
sarima.for(gdp_df, 36, 0,1,1,0,1,1,4)
```

### BenchMark Comparsion
```{r}
autoplot(gdp_df) +
  autolayer(forecast(fit2,36), 
            series="fit",PI=FALSE) +
  autolayer(meanf(gdp_df, h=36),
            series="Mean", PI=FALSE) +
  autolayer(naive(gdp_df, h=36),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(gdp_df, h=36),
            series="SNaïve", PI=FALSE)+
  autolayer(rwf(gdp_df, h=36, drift=TRUE),
            series="Drift", PI=FALSE)+
  
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
f2 <- snaive(gdp_df, h=36) 

accuracy(f2)
```

```{r}
summary(fit2)
```

Our model fitting is much better than benchmark methods

### Cross validation

#### One Step ahead
```{r}
n <- length(gdp_df)
n 
```

```{r}
k <- 89 # Use enough number of data for model: 30% of my whole dataset

n-k # rest of the observations
```

```{r,warning=FALSE}
i=1
err1 = c()
err2 = c()

for(i in 1:(n-k))
{
  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75
  xtest <- gdp_df[k+i] #76th observation as the test set

  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )
  fcast1 <- forecast(fit, h=1)
  
  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )
  fcast2 <- forecast(fit2, h=1)
  
  #capture error for each iteration
  # This is mean absolute error
  err1 = c(err1, abs(fcast1$mean-xtest)) 
  err2 = c(err2, abs(fcast2$mean-xtest))
  
  # This is mean squared error
  err3 = c(err1, (fcast1$mean-xtest)^2)
  err4 = c(err2, (fcast2$mean-xtest)^2)
  
}


```

```{r}
MAE1=mean(err1) 
MAE2=mean(err2)
MSE1=mean(err3)
MSE2=mean(err4)
```

```{r}
# Create a dataframe
error_metrics <- data.frame(
  MAE1 = MAE1,
  MAE2 = MAE2,
  MSE1 = MSE1,
  MSE2 = MSE2
)

# View the dataframe
print(error_metrics)
```
We can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.


#### 4 step ahead in my case
```{r}

farima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}

# Compute cross-validated errors for up to 4 steps ahead
e <- tsCV(gdp_df, forecastfunction = farima1, h = 4)
 
length(e) 
```

```{r,warning=FALSE}
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:4, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()
```

From here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case.

## Real Median Household Income in the United States SARIMA Analsyis
```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
df <- read.csv("../Dataset/project/MEHOINUSA672N.csv")
df$DATE <- as.Date(df$DATE)
head(df)
```


### Before Covid Period
```{r}
rmhi <- ts(df$MEHOINUSA672N,frequency = 2)
rmhi
```

```{r}
library(ggplot2)


ggplot(data = df, aes(x = DATE, y = MEHOINUSA672N)) +
  geom_line(color = "DarkViolet") +
  labs(title = "Time Series Plot of Household Income",
       x = "Date", 
       y = "Household Income") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "#E0E0E0"),
        panel.grid.major = element_line(color = "grey", size = 0.1),
        panel.grid.minor = element_line(color = "grey", size = 0.05),
        plot.background = element_rect(fill = "#E0E0E0"))

```

### Check with decomposition

```{r}
# Decompose the time series data
dec <- decompose(rmhi, type = "multiplicative")  # Choose either "additive" or "multiplicative"

# Set the graphical parameters for the plot
par(bg = "#E0E0E0", col.axis = "#E0E0E0", col.lab = "black", col.main = "black", col.sub = "black")

# Plot the decomposed object
plot(dec)

# Reset the graphical parameters to default
par(bg = "white", col.axis = "black", col.lab = "black", col.main = "black", col.sub = "black")

```

### Check Seasonal Pattern using Lag
```{r}
gglagplot(rmhi, do.lines=FALSE, set.lags = c(2, 4, 8, 12))
```

### Seasonal Difference and ACF, PACF
This shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 2 relatively.
```{r}
ts_plot <- autoplot(rmhi) +
  labs(title = "Time Series Plot ") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Extract the ACF and PACF plots without the theme
acf_plot <- ggAcf(diff(diff(rmhi, lag=2), differences = 1)) +
  labs(title = "ACF for first Order of Differencing and Seasonal Differenceing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(diff(diff(rmhi, lag=2), differences = 1)) +
  labs(title = "PACF for first Orders of Differencing and Seasonal Differenceing") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             

```


Most spikes are within range, it is stationary. 
ACF Plot (Autocorrelation Function Plot):

The sharp drop after lag 2. This gives us q = 1,2.
Since there's a noticeable autocorrelation at lag 2,5, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 1, which implies Q = 1,2.
PACF Plot (Partial Autocorrelation Function Plot):

The sharp drop after lag 2 in the PACF plot indicates a possible AR(1) process. This gives us p = 1,2.
The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. However, the dataset did not have a strong seasonal pattern. We can also consider P to be 0
Order of Differencing:

We applied first order differencing, so d = 1.
You also mentioned seasonal differencing with a lag of 4, so D = 1.
Combining these, we get:

Non-seasonal parameters: p = 1,2, d = 1, q = 1,2
Seasonal parameters: P = 0,1 D = 1, Q = 1, and the seasonal period (or frequency) is 2.
Therefore, the ARIMA model can be represented as ARIMA(2,1,1)(0,1,1)[2].

WE can continue analysis




### Model Diagnostic

```{r}
######################## Check for different combinations ########
SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  temp=c()
  d=1
  D=1
  s=2
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*23),nrow=23)
  
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  temp
}


  

```


### Compare Results

```{r}
# Based on the analysis:

output=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=rmhi)
knitr::kable(output)

```


```{r}
output[which.min(output$AIC),] 
```

```{r}
output[which.min(output$BIC),]
```

```{r}
output[which.min(output$AICc),]
```


```{r}

set.seed(236)
model_output1 <- capture.output(sarima(rmhi, 2,1,1,0,1,1,2))
model_output2 <- capture.output(sarima(rmhi, 0,1,1,0,1,1,2))
```
The second one is a little better.

```{r}
cat(model_output1[50:67], model_output1[length(model_output1)], sep = "\n") 
```

```{r}
cat(model_output2[40:55], model_output2[length(model_output2)], sep = "\n") 
```

The Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.

The Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.

The Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.

The Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.

$ttable: all coefficients are significant.
The second one is better indeed.


### Fit model & Forecast

```{r}
fit2=arima(rmhi, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )
summary(fit2)

```


```{r}
# Autoplot with custom colors
plot_fit_ <- autoplot(forecast(fit2,36)) + 
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.key = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Display the plot
print(plot_fit_)

```

### BenchMark Comparsion

```{r}
sarima.for(rmhi, 36, 0,1,1,0,1,1,2)
```


```{r}
autoplot(rmhi) +
  autolayer(forecast(fit2,36), 
            series="fit") +
  autolayer(meanf(rmhi, h=36),
            series="Mean", PI=FALSE) +
  autolayer(naive(rmhi, h=36),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(rmhi, h=36),
            series="SNaïve", PI=FALSE)+
  autolayer(rwf(rmhi, h=36, drift=TRUE),
            series="Drift", PI=FALSE)+
  
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
f1 <- meanf(rmhi, h=36)
f2 <- snaive(rmhi, h=36) 
f3 <-naive(rmhi, h=36)
accuracy(f1)
accuracy(f2)
accuracy(f3)
```

```{r}
summary(fit2)
```

Our model fitting is better than benchmark methods with smaller RMSE

### Cross validation

#### One Step ahead
```{r}
n <- length(rmhi)
n * 0.3
```

```{r}
k <- 12 # Use enough number of data for model: 30% of my whole dataset

n-k # rest of the observations
```

```{r,warning=FALSE}
i=1
err1 = c()
err2 = c()

for(i in 1:(n-k))
{
  xtrain <- rmhi[1:(k-1)+i] 
  xtest <- rmhi[k+i] 
  fit <- arima(xtrain, order = c(2,1,1),seasonal = list(order=c(0,1,1), period=2) )
  fcast1 <- forecast(fit, h=1)
  
  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )
  fcast2 <- forecast(fit2, h=1)
  
  #capture error for each iteration
  # This is mean absolute error
  err1 = c(err1, abs(fcast1$mean-xtest)) 
  err2 = c(err2, abs(fcast2$mean-xtest))
  
  # This is mean squared error
  err3 = c(err1, (fcast1$mean-xtest)^2)
  err4 = c(err2, (fcast2$mean-xtest)^2)
  
}


```

```{r}
MAE1=mean(err1) 
MAE2=mean(err2)
MSE1=mean(err3)
MSE2=mean(err4)
```

```{r}
# Create a dataframe
error_metrics <- data.frame(
  MAE1 = MAE1,
  MAE2 = MAE2,
  MSE1 = MSE1,
  MSE2 = MSE2
)

# View the dataframe
print(error_metrics)
```
We can see that the corresponding results for model 2: (2,1,1)(0,1,1) is slightly better. Which is not the same as the conclusion from above, the reason could be that the data points are relatively small. The train and test split can not capture the datasets effectively, which can cause the different conclusion. The model diagnotics from previsou section indicates that the (0,1,1)(0,1,1) is better with smaller errors.


#### 2 step ahead in my case
```{r}

farima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) ),h=h)}

# Compute cross-validated errors for up to 2 steps ahead
e <- tsCV(rmhi, forecastfunction = farima1, h = 2)
 
length(e) 
```

```{r,warning=FALSE}
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:2, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()
```

From here we can see that the one step ahead has lower MSE, which is better than two step ahead in my case.







# References and Codes
- Codes: [Rmd, Python & Qmd](https://github.com/zy236yuz5/ANLY5600-TimeSeries)
- U.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.
- U.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.
- U.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.
- Zillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/.