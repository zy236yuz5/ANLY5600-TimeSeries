[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ABOUT ME",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "ARCH/GARCH.html",
    "href": "ARCH/GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARCH/TS.html",
    "href": "ARCH/TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARModels.html",
    "href": "ARModels.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "After Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation.\nTo be more specific, we need to understand the concepts about the models: Certainly! Here’s a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:"
  },
  {
    "objectID": "ASV.html",
    "href": "ASV.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "DataSources.html",
    "href": "DataSources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Data Gathering\n\n\nData Discription & Source\nIn this section, different datasets are gained to help provide different analysis in later sections. Each dataset serves to answer questions that are defined in introduction section. To be more specifc, there are datasets regarding the household savings, incomes, sale prices, Gini idex, and housing affordability which can help us analysis the impact of the income disparities and housing affordability throughout USA. The detialed description of each dataset is provided.\n\n1. Household Savings\n\nLink: Household Saving Dataset\nDescription: This dataset contains information related to household savings in the United States throughout the years from 1992 to 2021. It includes data information on the saving, which can be an important economic indicator for me to make analysis. The Units for dataset are Billions of Dollars, Not Seasonally Adjusted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Real Median Household Income in the United States\n\nLink: Real Median Household Income Dataset\nDescription: This dataset represents the real median household income dataset. This is an estimation of Median Incomes. The Census Bureau has computed medians using either Pareto interpolation or linear interpolation. Currently, we are using linear interpolation to estimate all medians. Pareto interpolation assumes a decreasing density of population within an income interval, whereas linear interpolation assumes a constant density of population within an income interval. The Census Bureau calculated estimates of median income and associated standard errors for 1979 through 1987 using Pareto interpolation if the estimate was larger than $20,000 for people or $40,000 for families and households. This is because the width of the income interval containing the estimate is greater than $2,500.\n\n\n\n\n\n\n\n\n\n\n\n3. Median Sales Price of Houses Sold for the United States\n\nLink: Median Sales Price of Houses Dataset\nDescription: This dataset contains information on the median sales price of houses sold in the United States. It can be a valuable indicator of the state of the real estate market.\nThis dataset can provide me with a direct view about the sale price of the house in USA. We can see clearly about the price disparities. Then, by comparing with the income and saving, we can know about the patterns regarding the impact of disparities.\n\n\n\n\n\n\n\n\n\n\n\n4. GINI Index for the United States\n\nLink: GINI Index Dataset\nDescription: This dataset shows the Gini index for United States. Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. In addition, a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nData are based on primary household survey data obtained from government statistical agencies and World Bank country departments.\nWorld Bank collection of development indicators, compiled from officially recognized international sources. It presents the most current and accurate global development data available and includes national, regional, and global estimates. The World Bank labels these annual series, but several observations are missing..\n\n Source Indicator: SI.POV.GINI\n\n\n5. Housing Affordability Index\n\nLink: Housing Affordability Index Dataset\nDescription: This dataset contains data on the Housing Affordability Index. The housing affordability index measures the degree to which a typical family can afford the monthly mortgage payments on a typical home. Value of 100 means that a family with the median income has exactly enough income to qualify for a mortgage on a median-priced home. An index above 100 signifies that family earning the median income has more than enough income to qualify for a mortgage loan on a median-priced home, assuming a 20 percent down payment. This index is calculated for fixed mortgages.\nThis dataset can directly tell us if one can easily afford a house. This can help me make analysis the resources disparities and the possible reasons for the differences in housing affordability\n\n\n\n\n\n\n\n\n\n\n\n6. Personal Saving Rate\n\nLink: Personal Saving Rate Dataset\nDescription: This dataset is to provide information on the personal saving rate. The personal saving rate measures the percentage of disposable income that individuals save, which can be indicative of financial health. In here, personal saving as a percentage of disposable personal income (DPI), frequently referred to as “the personal saving rate,” is calculated as the ratio of personal saving to DPI.\nFrom this dataset, I can evaluate the overall changes throughout years regarding the personal savings. Then, by comparing with the income and house prices, we can see if there are any patterns or relationships.\n\n\n\n\n\n\n\n\n\n\n\n7. Gross Domestic Product: Implicit Price Deflator\n\nLink: GDP: Implicit Price Deflator Dataset\nDescription: This dataset represents the Gross Domestic Product (GDP) Implicit Price Deflator for USA, which is a measure of inflation in the economy and is used to adjust GDP for price changes.\nThis dataset aims to cover the overall GDP changes quetsions that can be utilized to make analysis on the impact of the incomes and housing prices.\n\n\n\n\n8. All Home Prices in States in the USA\n\nLink: Zillow Home Prices Dataset\nDescription: This dataset provides information on home prices in various states in the United States. It is sourced from Zillow, a well-known real estate and rental marketplace.\nThese housing datasets (from zillow group) not only provide sufficient information concerning the house values and sales prices, but also include the different regions. I wish to compare different states or regions to find if there are any patterns in house values, sales prices, and rental prices. I want to find correlations among them. In addition, some of the datasets cover a great range of time even from 2000 to 2023. This can provide with a broader view of the dataset when making visualizations across the time. We can see clearly how the housing values and sales prices changed throughout the years and make forecasts for the future.\n\nA Screen shot for the dataset: \n\n\n\nReference:\n\nCodes: Rmd, Python & Qmd\nU.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.\nU.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.\nU.S. Census Bureau and U.S. Department of Housing and Urban Development, Median Sales Price of Houses Sold for the United States [MSPUS], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MSPUS, September 19, 2023.\nWorld Bank, GINI Index for the United States [SIPOVGINIUSA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/SIPOVGINIUSA, September 19, 2023.\nNational Association of Realtors, Housing Affordability Index (Fixed) [FIXHAI], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/FIXHAI, September 19, 2023.\nU.S. Bureau of Economic Analysis, Personal Saving Rate [PSAVERT], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/PSAVERT, September 19, 2023.\nU.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.\nZillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/."
  },
  {
    "objectID": "DataVis.html",
    "href": "DataVis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we will explore the datasets that we created by using visualizations. The goal is to gain insights and ideas before applying models and analysis. The data visualization and storytelling are very crucial for us to understand the impact of the income and houseing prices disparities. In this section, by utilizing different visualizations with different tools, we endeavor to delve into the depths of the datasets, finding potential trends, seasonality, and other features.\n \n\n\nFirst of all, we need to have a basic look at for the household income, household savings, and the sale price of the house. By putting the figures together, we can have a more direct idea about the trending and oscciliation throughout the time. Through interactive tableau visualization, we can see the temporal trends and oscillations that may be important for our analysis.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the above plots, we can see that the household savings throughout the years generally follow an upwarding trend, which means that more and more people are willingly to save moneny. For Median household income, however, we can see that from 1984 to 2021, it has a lot of osccilations. Bascilly, the median household income did not change too much. And then! for sale prices of the houses, we can see that it follows a positive trend as well.\nFrom here, we can have a basic insight: while sale price increases, the median household income still remains the same throughout the years. This give us questions, why? And if this is the case, can people nowadays afford the house? In order to answer these quetsions, we need to dive into other datasets as well.      \n\n\n\nFirst, we need to have a look at of the GDP as a whole. Since GDP represent economic evaluations, which can offer us invaluable insights into the financial health and trajectory of a nation. With the GDP Implicit Price Deflator data, we aim to have insights on the potential reason of the income and sale price of house. The Deflator helps to discern between changes in GDP due to alterations in prices and changes due to alterations in quantities.\n\nBased on the interactive plotly plot, we can see the percent change from preceding period. Generally from 1960 to 1980, it follows an upwarding trend which aligns with the household savings and sale prices. Then from 1980 to 2020, it follows a downwarding trend and appears to have fluctuations from 2020 to now. From 2020, the Covid-19 surely had huge impact of the general economic. However, the decreasing of the percent change from 1980 to 2020 indicates that the percent changes are generally small, which means that for these years the GDP remains roughly the same, which aligns with the incomes from previous figure.\nThis also shows that while the GDP did not change too much, the household median incomes did not change as well. However, the sale prices and savings keep increasing. In addition, as we mentioned from introduction, the other important aspect that we want to explore is about the disparity. From the visulizations, although we see that the income keep increasing, there are still disparities in distribution of the income.      \n\n\n\nTo have a brief recap, Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. A Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nAs we all aware, the disparity between geographic area exists. The Urban areas, especially metropolitan regions, are often have more economic. In this section, we want to find the extents of disparities and correlations between the income and other factors. From exploring the Gini coefficient, we can gain insights with income disparities, housing affordability as well as the trend and seasonality.\n\nBased on the GINI index over time, we can see that it follows an upwarding trend. Which is not a good thing, this means that we are towarding more inequality throughout the time. This means that the distribution of income within an economy deviates is not equal. We can see that from 1975 to 2019, the GINI index has increased about 8. However, from 2020, it seems that we have a decline of the GINI idex. Again, the Covid also had impacts of the generaly distribution of the incomes.\nFrom here, we can see that while the median income remains roughly the same, the distribution of the income did not remain constant. Which means that while many people gaining wealth, there are still more and more proverties. This can also imply that for a part of the people, they have got more resources while the others have troubles with housing affordability. To be more specific, we need to have a look at about the relationship between personal saving rate and the house price.      \n\n\n\nIn this section, we have a focused on understanding how personal savings rates interface with average sales prices of houses in the USA. From here, we aim to identify the trend and pattern such that we can see the direct impact of the relationship between saving rate and the sale price. We can understand how changes in saving rates might reflect or impact shifts in housing market trends.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the average personal saving rate and the average sales price of the houses, we can see that the personal saving rate fluctuates throughout the years while the average sales price follows an upwarding trend. The increasing of the house prices may be attributed to factors such as population growth, urbanization. However, as population grows, the saving rate did not change too much. Again, this could also means that the disparities in geographic locations play an important role.      \n\n\n\nTaking a step further into our exploration, this section presents a bar visualization, providing a categorical breakdown of pivotal economic indicators and metrics. Our goal is to compare, contrast, and evaluate different states and MSA, identifying links and disparities among districts.\nNow, we wish to have a more specific look at about the disparities among regions. Here, we have selected some Meyropolitan Areas with states and MSAs. In here, we can see that which states or regions generally have a higher prices. Which can help us find the disparities.\n\n\nBased on the linked figure, we can see that states for CA, CO, and HI generally have higher sale prices of houses. This means that the Geographic Disparities in Housing Markets did exist throughout the time. From 2018 to 2023, most of the places also have an upwarding trend in sale prices. This can make the housing affordability to change as well.\nIn addition, the real estate landscapes of CA, CO, and HI have been prospered among economists. The increasing in housing prices in these regions indicates several factors and disparities:\nHigh demand in states like CA and HI, may be the cause of increasing prices. In addition, the economic landscapes of these states, often associated with sectors like advanced technology and high tourism that can boost the economy which makes the prices higher. However the average incomes did not change too much. This could bring affordability crisis in the future."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For Exploratory Data Analysis (EDA), we aim to have a deeper understanding of the time series data. This section involves multiple medthods and datasets for us to gain insights and understandings such as decompositions, lag plots, ACF, PACF, ADF tests, detrending, and others. For these methods, we are going to identify correlations, trends, seasonalities, and stationaries. Which can help us to make analysis and apply models in later sections. To be more specific, the lag plots and decomposing methods will allow us to discover dependencies and components. ACF and PACF can help us see the correlations and stationary, and the Augmented Dickey-Fuller Test to empirically probe its stationarity."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The Impact of Income Disparities and Housing Affordability in the United States\n\nProject Backgroung:\nWith the dramatic development of the technology and society, we have witnessed the continuous increasing in economies. However, such developments did not spread through every corner in USA. The disparities of resources still exist. There are housing price disparities, rental sales disparities, and income disparities for different regions and states. These differents play a very crucial role in our society and they bring huge impact and massive pressure to people nowadays.\nThe central focus of this project lies in understanding the relationship between the income disparities between and housing affordability in the United States. I aim to help us understand and gain new insights on how varying income levels within society directly affect the ability of individuals and families to secure housing that is both suitable and affordable. Throughout the analysis, different smaller aspects and factors will be analyzed one by one. \n\n\nThe Big Picture & Literature Review:\nIncome disparity and housing affordability are not just matters of individual but also broader societal concerns that need to be attended to. The influences of such disparities and results are not only the quality of life for countless Americans citizens but also the overall stability of communities and the economic health of the nation. As I help us explore this issue, we must recognize that it’s not simply about monetary figures; it’s about the pursuit of a fundamental human need: shelter and how income inequality, housing price, rental price, GDP, personal saveing and regional difference influence us as a whole in the United States.\nIn order to bring a more comprehensive understanding of this topic, we must delve into the body of research and discourse that precedes us. Existing literature reveals the persistent challenge of housing affordability, marked by the growing gap between incomes and housing costs.(Jajtner et al., 2020). Scholars have examined the historical trends, policy interventions, and socioeconomic factors. In this exploration of the impact of income disparities on housing affordability, we will adopt multiple analytical angles to provide a well-rounded perspective.\n\nMutiple datasets are analyzed thoroughly to provide insights and conclusions. Different methods such as Exploratory Data Analysis, ARMA Models, ARIMAX Models, Spectral Analysis and Filtering,, Financial Time Series Models, and Deep Learning methods are conducted to help me achieve the goal.\n\n\nGuiding Questions that need to be answered:\nTo guide the explorations, we have conducted some questions that would be answered using different analysis and dataset:\n\nHow have income inequality trends in the United States impacted the individuals?\nWhat are the historical trends in housing prices?\nWhat is the definition of housing affordability, and what metrics are commonly used to assess it?\nHow do income disparities vary from one region to another?\nWhat regions or states have greater disparities?\nWhat impact have the economy had on the housing market?\nIs there any trends or seasonal patterns regarding the income and housing price?\nIn what ways does housing affordability intersect with broader economic and social issues, such as workforce mobility and education outcomes?\nWhat innovative solutions and strategies can be employed to improve housing affordability for all Americans?\nWhat are the correlations between the sale price, housing price, and rental price?\nWhat future trends should we anticipate in the context of income disparities and housing affordability?\nHow can addressing income disparities lead to a more equitable and sustainable housing market in the United States?\n\nThese guiding questions will serve as different milestones throughout this exploration. The questions might be changed and increased as the project proceeds."
  },
  {
    "objectID": "SAF.html",
    "href": "SAF.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "GARCH.html",
    "href": "GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "TS.html",
    "href": "TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Author: Zonghong Yu"
  },
  {
    "objectID": "dv.html#question-2",
    "href": "dv.html#question-2",
    "title": "Data Vizes in TS",
    "section": "Question 2",
    "text": "Question 2\n\n1. Try to reproduce the Data Vizes similar to in Lab0/1 “Data Viz Examples” but with a different set of stock prices. (use the quantmod package to get stock prices from yahoo finance https://finance.yahoo.com/lookup/)\n\na.\nIn here, I utilize three different set of stock prices: GOOGL, MSFT - Microsoft Corporation, and NFLX - Netflix Inc. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretations: The three stocks as showned in figure above represent:Google, MicroSoft, and Netflix. We can see that in general, Netflix had an overall highiest stock prices throughout the years. And Google has the least over the years. However, all three of them had fluctuations especially in year 2022, which could be the impact of the Covid-19.\n\n\nb. Plot the climate data (climate.csv) using plotly. https://plotly.com/r/\n\n\nInterpretations: By using the plotly on the climate dataset provided, we can see some interesting features. From above interaction figure, we can see the relation between the Date and the max temperature. We can see that during July and August in 2021, the max temperature is higher in general while lower during October and December which is reasonable.\n\n\nc. Get any economic data / macroeconomic indicators and plot using plotly https://fred.stlouisfed.org/ https://www.bea.gov/\nI utilized the GEPUCURRENT dataset, which is Global Economic Policy Uncertainty Index: Current Price Adjusted GDP, from https://fred.stlouisfed.org/series/GEPUCURRENT\n\n\nInterpretations: By using the plotly on the economic dataset from the website givem, I chose the GDP index of some global economic policy. From above interaction figure, we can see the relation between the Date and the index. We can see that the index follows an upwarding trend throughout the years. However, there are many local maximums and fluctuations throughout the years. In addition, the idex reached at its peak on 2020/05/01.\n\n\n\n2. Make only the plots visible in your webpage. (set echo=FALSE in your R code chunck)\nOnly plots visible as required.\n\n\n3. Add interpretations to all the plots in the webpage.\nAdded as required."
  },
  {
    "objectID": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "ABOUT ME",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "Time Series",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "EDA.html#the-gdp-exploratory-data-analysis",
    "href": "EDA.html#the-gdp-exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The GDP Exploratory Data Analysis",
    "text": "The GDP Exploratory Data Analysis\nThen, after making analysis for income, sale price, and houseing affordability. We should have a look at for analyzing the GDP, which can represent the economy as a whole. By analyzing the GDP data, we can have a more generalized view and gain more insights about the patterns, seasonalities, and stationaries.\n\nTime Series Plot of GDP\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1980, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Which is similar to the houseing affordability index. I think we can see that there is a positive correlation between GDP and housing affordability index. When GDP gets higher, people can afford a house more easily. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow a multiplicative pattern. Because it seems that it does not have constant amplitude and frequency. In order to prove these, we need to explore more using different methods.\n\n\nLag for GDP\n\n\n\n\n\nBased on the lag plot, there’s a correlation but not too strong. This shows potential linearity. However, it could also mean that the dataset is not stationary but we can not decide yet.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has almost constant amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the acf decays very quickly, which also means that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$A191RI1Q225SBEA\nDickey-Fuller = -3.8947, Lag order = 6, p-value = 0.01466\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we reject the Null hypothesis. The time series dataset is stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformation\n\n\n\n\n\n\n\n\n\n\nSince, the original dataset is already stationary. Therefore, no further action is needed to make it stationary. The detrend and log transformation is just to explore the dataset.\n\n\nMoving Average\n\n\nThe “Original” line shows the raw GDP changes. The MA(5), MA(20), and MA(50) represent moving averages with windows of 5, 20, and 50 time units, respectively. It’s clear that as the window size increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-mean-sale-price-analysis",
    "href": "EDA.html#the-mean-sale-price-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean Sale Price Analysis",
    "text": "The Mean Sale Price Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822"
  },
  {
    "objectID": "EDA.html#the-mean-house-sale-price-data-analysis",
    "href": "EDA.html#the-mean-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean House Sale Price Data Analysis",
    "text": "The Mean House Sale Price Data Analysis\n\n\n\n\n\n\n\n\nFrom the plot, you can visually inspect:\nTrend: Any consistent upward or downward direction. Seasonality: Any repeating patterns or cycles. Variation: Fluctuations in the data. Periodic fluctuations: Spikes or drops at consistent intervals. Determine if the time series looks multiplicative or additive. An additive time series has constant amplitude and frequency, while a multiplicative one has varying amplitude/frequency.\n\n\n\n\n\nYou’d interpret the lag plot by looking for any structure. If the points cluster along a diagonal line from bottom-left to top-right, there’s a positive correlation. Any other pattern might suggest non-linearity or some pattern not captured by mere linear correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary"
  },
  {
    "objectID": "EDA.html#the-median-house-sale-price-data-analysis",
    "href": "EDA.html#the-median-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nSince our key is to discover the impact of the income and house price, we certainly need to make analysis for the sale price data.\n\nThe Time Series\n\n\n\n\n\n\n\n\nFrom the plot, the results are the same with the previous section. It has an upwarding trend.\nTo be more specific, from the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists but not too much. Periodic fluctuations: The fluctuations are presented in 2010 and 2021. multiplicative or additive： Only based on the timeseries plot, I think this follows additive, because it does not have too much variance.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plots\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. In addition, this aligns with the same conclusion and it has a strong positive correlation. This shows linearity.\n\n\nDecomposition for the data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. In addition, it has roughly constant amplitude/frequency. Therefore, I think this follows additive.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF of the data\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log transformation\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending, the sale prices remains fluctuations espectially from 2000 to 2020 period. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$MSPUS)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Sale Price Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThe graph shows the House Sale Price from 1970 to 2020. The blue line represents the original data. MA(2), MA(8), and MA(16) depict the moving averages over 2, 8, and 16 periods respectively. The smoothed lines show the underlying trend of house sale prices over time. The higher the period of the moving average, the smoother the line, which can help in understanding long-term trends."
  },
  {
    "objectID": "EDA.html#the-housing-affordability-index-data-analysis",
    "href": "EDA.html#the-housing-affordability-index-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Housing Affordability Index Data Analysis",
    "text": "The Housing Affordability Index Data Analysis\nThen, after we have a basic understanding about the income and sale price. We need to dive into the impact. We utilize the housing affordability index to see the impact of the income and sale price. Let us firt explore the houseing affordability index first.\n\nTime Series Plot for Housing Affordability Index\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = PSAVERT)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Time Series Plot of Housing Affordability Index\",\n       x = \"Date\", \n       y = \"The Housing Affordability Index\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1970, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow an additive pattern. Because it seems that it has constant amplitude and frequency although with some fluctuations. In order to prove these, we need to explore more using different methods.\n\n\nLag Plot for Houseing Affordability\n\n\nCode\n# Lag plot\nlagged_data <- data.frame(value = df$PSAVERT[-1],\n                          lagged_value = df$PSAVERT[-length(df$PSAVERT)])\n\n# Enhanced Lag Plot\nggplot(data = lagged_data, aes(x = lagged_value, y = value)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Lag Plot\",\n       x = \"Value at t-1\",\n       y = \"Value at t\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Housing Affordability\n\n\nCode\n#library(ggfortify)\n\n# Decomposition using ggplot2 styling\ndecomposed <- decompose(ts(df$PSAVERT, frequency=12), type = \"additive\")\nautoplot(decomposed) + \n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, mostly it has constant amplitude/frequency. Therefore, I think this follows additive pattern.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF Analysis\n\n\nCode\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$PSAVERT) +\n  labs(title = \"ACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# PACF Plot\nggPacf(df$PSAVERT) +\n  labs(title = \"PACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nThe ACF plot can help determine if the series is stationary. From here, we can see that the dataset has a correlation, and it is decaying slowly which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\nCode\nlibrary(tseries)\nadf.test(df$PSAVERT)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$PSAVERT\nDickey-Fuller = -3.543, Lag order = 9, p-value = 0.03817\nalternative hypothesis: stationary\n\n\nBased on the ADF test, since the p-value is smaller than the threshold value, we should reject the null hypothesis, which means that the dataset is stationary! However, the value is close to 0.05. In addition, the ADF test is not as reliable as the ACF test. Therefore, since the ACF decays slowly, and it showed strong correlation. This means that, the dataset is not stationary.\n\n\nDetrened and Log-transformation\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nCode\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$PSAVERT))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCode\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$PSAVERT))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Housing Affordability Index\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nENsure it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$PSAVERT)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Housing Affordability IndexTime Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThis graph presents the Housing Affordability Index from 1960 to 2020. The blue line is the original data. MA(2), MA(10), and MA(20) represent moving averages over 2, 10, and 20 periods. The plot illustrates how housing affordability has changed over time, and the smoothed lines can help identify broader trends or shifts in the data."
  },
  {
    "objectID": "EDA.html#the-household-income-data-analysis",
    "href": "EDA.html#the-household-income-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nAs we did in previous section, we start with the household income. In here, we have a basic look about the household income time series.\n\n\n\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： Only based on the timeseries plot, I think this follows multiplicative, because it has varying amplitude/frequency.\n \nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\nLAG Plot\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has varying amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$W398RC1A027NBEA)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Household Income Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Household Income Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: spikes all fall within the blue shaded confidence intervals, which represent the region where correlations are not statistically significant.\n\n\nMoving Average\n\n\nThe graph plots the Household Income from 1995 to 2020. The blue line represents the original data. MA(2), MA(4), and MA(8) are the moving averages taken over 2, 4, and 8 periods respectively. As the period of the moving average increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-sale-price-disparity-analysis",
    "href": "EDA.html#the-sale-price-disparity-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Sale Price Disparity Analysis",
    "text": "The Sale Price Disparity Analysis\nLastly, after analyzing the corresponding numeric datasets, we will explore more about the disparitis by using the Mean Sale price datasets which involves with different regions and states. From here, by making analysis on the pattern and seasonality, it can help us provide with more insights and knowledge in determining the impact of the disparities of the income and sale prices.\n\nTime Series Decomposition Plot\n\n\n\n\n\n\n\n\n\n\n\nFrom the decomposition plot, we can visually inspect: Trend: There is no clear trend. Seasonality: It seems that it has seasonal patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： I think this follows multiplicative, because it has varying amplitude/frequency.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plot\n\n\n\n\n\nBased on the lag plot, there’s a strong positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity.\nIn order to explore more, we utilize the acf and pacf to check the accuracy.\n\n\nACF and PACF plots\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation and decays slowly. Howeverm for PACF, it decays dramatically. Therefore, we can say that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF tests\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we can reject the Null hypothesis. The time series dataset is stationary.\nThen, we wish to make analysis for different values to see if they have correlations among each other. A heatmap can also help us to determine the correlations among the sale price, rental price, and homevalue of the dataset.\n\n\nCorrelation heatmap\n\n\n\n\n\n\n\n\n\n\nMoving Average\n\n\n\n\n\n\n\n\nFrom here, we can see that there is a strong positive correlation between the Sale price and home value. The higher of the home value, the higher of the sale price. However, the rental price did not have too much correlations with the other two. The possible reason could be the cause of the geographic difference and this could also affect the housing affordability.\nIn conclusion, in this section, we provide different observations. And these observations underscore the nature of real estate pricing, incomes, GDP changes througout the years as well as the housing affordability. In addition, different patterns and features are found to identify the trend, seasonality, patterns, and correlations. which allow us to have a more understanding about the dataset."
  },
  {
    "objectID": "ARModels.html#the-household-income-data-analysis",
    "href": "ARModels.html#the-household-income-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and transforming, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.934, Lag order = 3, p-value = 0.598\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\n\n\n\nMake it Stationary by using differencing\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n\n\n\n\n\n\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 2)\nDickey-Fuller = -2.9811, Lag order = 3, p-value = 0.1972\nalternative hypothesis: stationary\n\n\nHowever, the second order of differencing still cannot make it stationary. We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n\nThird Differencing\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 3)\nDickey-Fuller = -5.488, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n\nEvaluate the values for p & q\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.340 -0.006  0.060 -0.087 -0.038  0.241 -0.349  0.214 -0.036 -0.050 \n    11     12     13     14 \n 0.007  0.065 -0.115  0.150 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.340 -0.137  0.013 -0.069 -0.100  0.211 -0.232  0.062 -0.004 -0.007 -0.053 \n    12     13     14 \n 0.017  0.015  0.007 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\nFit model\nAfter we determing our parameters, we can start fitting the models.\n\n\nSeries: diff \nARIMA(1,3,2) \n\nCoefficients:\n          ar1      ma1     ma2\n      -0.7911  -1.8982  0.9996\ns.e.   0.2096   0.4024  0.4155\n\nsigma^2 = 919174:  log likelihood = -202.53\nAIC=413.07   AICc=415.17   BIC=417.78\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set -104.3663 845.5254 428.3458 153.1425 163.6866 0.695587 -0.3066825\n\n\nFrom here, we get a summary about the aic and bic values.\n\n\nModel diagnostics\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 3, q))\n\nCoefficients:\n          ar1      ma1      ma2\n      -0.4402  -0.2544  -0.0395\ns.e.   0.0377   0.0394   0.0262\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.19,  aic = 28412.39\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n422.0558\n430.2596\n425.8740\n\n\n2\n2\n2\n413.6960\n420.3570\n416.4232\n\n\n2\n3\n2\n405.0099\n411.4891\n407.8670\n\n\n2\n1\n3\n423.9044\n433.4754\n429.2377\n\n\n2\n2\n3\n411.0795\n419.0727\n415.0795\n\n\n2\n3\n3\n407.0239\n414.7989\n411.2239\n\n\n2\n1\n4\n425.8596\n436.7980\n433.0596\n\n\n2\n2\n4\n413.0766\n422.4020\n418.6766\n\n\n3\n1\n2\n423.9406\n433.5117\n429.2739\n\n\n3\n2\n2\n415.6154\n423.6086\n419.6154\n\n\n3\n3\n2\n406.9074\n414.6824\n411.1074\n\n\n3\n1\n3\n426.0532\n436.9916\n433.2532\n\n\n3\n2\n3\n413.0776\n422.4030\n418.6776\n\n\n3\n1\n4\n427.4187\n439.7243\n436.8923\n\n\n4\n1\n2\n425.4315\n436.3699\n432.6315\n\n\n4\n2\n2\n415.1668\n424.4922\n420.7668\n\n\n4\n1\n3\n427.4290\n439.7347\n436.9027\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\nForecasting for the dataset using the best parameters\n\n\n   Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n31       4278.130 3813.470  4742.791 3567.493  4988.767\n32       3458.665 2921.482  3995.847 2637.115  4280.215\n33       5473.540 4496.374  6450.705 3979.094  6967.985\n34       4679.682 3529.482  5829.881 2920.603  6438.760\n35       6742.303 5099.447  8385.159 4229.772  9254.834\n36       5988.600 4080.957  7896.243 3071.112  8906.088\n37       8093.980 5637.246 10550.714 4336.729 11851.230\n38       7382.141 4577.705 10186.577 3093.127 11671.156\n39       9529.693 6118.454 12940.932 4312.654 14746.732\n40       8859.921 5024.086 12695.756 2993.518 14726.325\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 42.947, df = 6, p-value = 1.195e-07\n\nModel df: 0.   Total lags used: 6\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$W398RC1A027NBEA \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.2008  -0.0093\ns.e.   0.1702   0.2238\n\nsigma^2 = 111877:  log likelihood = -208.7\nAIC=423.4   AICc=424.36   BIC=427.5\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE    MAPE      MASE      ACF1\nTraining set 83.97069 317.3154 158.0274 3.851463 17.2448 0.9308784 -0.111543\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 32.73409 312.2898 183.7851 -1.612877 23.50814 1.082607 -0.08540118\n\n\nTime Series:\nStart = 31 \nEnd = 50 \nFrequency = 1 \n [1]  4278.130  3458.665  5473.540  4679.682  6742.303  5988.600  8093.980\n [8]  7382.141  9529.693  8859.921 11049.575 10421.894 12653.642 12068.054\n[15] 14341.896 13798.400 16114.336 15612.934 17970.962 17511.654\n\n\n\n\nAccuracy of the based models\n\n\n                        ME     RMSE      MAE       MPE     MAPE MASE\nTraining set -2.643441e-14 635.2356 470.2565 -51.63239 78.34547    1\n\n\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE       ACF1\nTraining set 66.26655 330.1867 169.7616 2.867066 18.06051    1 -0.2730483\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.350993e-14 323.4687 166.4792 -9.624318 20.73922 0.9806647\n                   ACF1\nTraining set -0.2730483\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending."
  },
  {
    "objectID": "ARModels.html#the-median-house-sale-price-data-analysis",
    "href": "ARModels.html#the-median-house-sale-price-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.7007, Lag order = 6, p-value = 0.7017\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\nMake it Stationary by using differencing\n\n\n\n\n\nAfter the first order of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$MSPUS, differences = 1)\nDickey-Fuller = -6.3462, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.625  0.098  0.079 -0.054  0.021  0.024 -0.095  0.071  0.094 -0.222 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.106  0.112 -0.158  0.057  0.021 -0.059  0.114 -0.106 -0.050  0.225 -0.228 \n    22     23 \n 0.130 -0.083 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.625 -0.481 -0.278 -0.165 -0.069  0.047 -0.070 -0.119  0.136 -0.028 -0.186 \n    12     13     14     15     16     17     18     19     20     21     22 \n 0.022  0.044 -0.013  0.008 -0.042  0.052  0.040 -0.118  0.082 -0.035  0.115 \n    23 \n-0.010 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.\n\n\nFit model\n\n\nSeries: diff \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.0092  -0.9827\ns.e.  0.0663   0.0133\n\nsigma^2 = 46013474:  log likelihood = -2458.55\nAIC=4923.1   AICc=4923.2   BIC=4933.54\n\nTraining set error measures:\n                   ME     RMSE      MAE  MPE MAPE      MASE        ACF1\nTraining set 542.7894 6740.971 3865.506 -Inf  Inf 0.6808965 -0.01153987\n\n\n\n\nModel diagnostics\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 1, q))\n\nCoefficients:\n          ma1\n      -0.1934\ns.e.   0.0098\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.07,  aic = 28408.15\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n4938.630\n4952.569\n4938.799\n\n\n1\n2\n1\n4923.100\n4933.542\n4923.202\n\n\n1\n3\n1\n4976.098\n4986.528\n4976.200\n\n\n1\n1\n2\n4937.123\n4954.547\n4937.379\n\n\n1\n2\n2\n4924.722\n4938.645\n4924.892\n\n\n1\n3\n2\n4915.708\n4929.614\n4915.879\n\n\n1\n1\n3\n4921.957\n4942.866\n4922.316\n\n\n1\n2\n3\n4926.078\n4943.482\n4926.335\n\n\n1\n3\n3\n4918.622\n4936.004\n4918.879\n\n\n2\n1\n1\n4938.832\n4956.256\n4939.087\n\n\n2\n2\n1\n4933.117\n4947.039\n4933.287\n\n\n2\n3\n1\n4931.865\n4945.771\n4932.036\n\n\n2\n1\n2\n4918.505\n4939.414\n4918.864\n\n\n2\n2\n2\n4924.928\n4942.331\n4925.185\n\n\n2\n3\n2\n4923.269\n4940.651\n4923.526\n\n\n2\n1\n3\n4918.739\n4943.132\n4919.219\n\n\n2\n2\n3\n4903.774\n4924.657\n4904.134\n\n\n2\n3\n3\n4913.868\n4934.727\n4914.230\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\nSince we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.\n\n\nForecasting for the dataset\n\n\n    Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n243       415627.1 406980.8 424273.4 402403.7 428850.4\n244       415720.2 403091.1 428349.3 396405.7 435034.7\n245       416226.1 400354.8 432097.4 391953.1 440499.2\n246       417033.1 398322.0 435744.1 388416.9 445649.2\n247       418059.5 396783.9 439335.2 385521.2 450597.8\n248       419246.0 395615.0 442877.1 383105.4 455386.6\n249       420549.3 394731.0 446367.6 381063.5 460035.0\n250       421937.7 394071.9 449803.4 379320.7 464554.7\n251       423388.1 393593.8 453182.5 377821.6 468954.7\n252       424883.9 393263.4 456504.3 376524.6 473243.1\n253       426412.6 393055.6 459769.6 375397.5 477427.7\n254       427965.4 392950.6 462980.2 374415.0 481515.9\n255       429535.8 392933.2 466138.3 373557.0 485514.5\n256       431118.9 392991.1 469246.7 372807.5 489430.4\n257       432711.4 393114.6 472308.3 372153.3 493269.6\n258       434310.7 393295.7 475325.8 371583.7 497037.8\n259       435915.0 393528.0 478302.1 371089.6 500740.4\n260       437522.9 393806.0 481239.8 370663.7 504382.1\n261       439133.4 394125.3 484141.5 370299.5 507967.4\n262       440745.9 394482.2 487009.6 369991.7 511500.1\n263       442359.7 394873.3 489846.2 369735.5 514984.0\n264       443974.6 395296.0 492653.3 369527.0 518422.2\n265       445590.3 395747.7 495432.8 369362.6 521817.9\n266       447206.4 396226.4 498186.4 369239.2 525173.6\n267       448823.0 396730.3 500915.7 369154.1 528491.9\n268       450439.9 397257.7 503622.1 369104.7 531775.0\n269       452056.9 397807.1 506306.8 369089.0 535024.9\n270       453674.2 398377.3 508971.1 369104.9 538243.5\n271       455291.5 398967.0 511616.0 369150.6 541432.4\n272       456908.9 399575.2 514242.7 369224.5 544593.3\n273       458526.4 400200.9 516851.9 369325.2 547727.6\n274       460143.9 400843.2 519444.6 369451.3 550836.5\n275       461761.5 401501.3 522021.6 369601.6 553921.4\n276       463379.1 402174.5 524583.6 369774.8 556983.3\n277       464996.6 402862.1 527131.2 369970.0 560023.3\n278       466614.3 403563.4 529665.1 370186.3 563042.3\n279       468231.9 404277.8 532185.9 370422.6 566041.2\n280       469849.5 405004.8 534694.2 370678.1 569020.9\n281       471467.1 405743.9 537190.4 370952.1 571982.2\n282       473084.8 406494.5 539675.0 371243.8 574925.7\n283       474702.4 407256.4 542148.4 371552.6 577852.2\n284       476320.0 408028.9 544611.1 371877.8 580762.3\n285       477937.7 408811.8 547063.6 372218.7 583656.6\n286       479555.3 409604.6 549506.0 372574.9 586535.7\n287       481172.9 410407.0 551938.8 372945.8 589400.0\n288       482790.6 411218.8 554362.4 373330.9 592250.2\n289       484408.2 412039.5 556776.9 373729.8 595086.6\n290       486025.8 412868.9 559182.8 374141.9 597909.8\n291       487643.5 413706.7 561580.3 374566.9 600720.1\n292       489261.1 414552.6 563969.7 375004.3 603518.0\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1986.7, df = 10, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$MSPUS \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n         ma1    sma1\n      0.0210  0.1671\ns.e.  0.0665  0.0793\n\nsigma^2 = 47248340:  log likelihood = -2470.36\nAIC=4946.73   AICc=4946.83   BIC=4957.18\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 1355.307 6831.004 4055.777 1.052941 2.484314 0.9922265 -0.04153127\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 2.720481 6690.746 3914.234 -0.9087146 2.734111 0.9575986\n                    ACF1\nTraining set -0.05135495\n\n\nTime Series:\nStart = 243 \nEnd = 262 \nFrequency = 1 \n [1] 415627.1 415720.2 416226.1 417033.1 418059.5 419246.0 420549.3 421937.7\n [9] 423388.1 424883.9 426412.6 427965.4 429535.8 431118.9 432711.4 434310.7\n[17] 435915.0 437522.9 439133.4 440745.9\n\n\n\n\nAccuracy of the based models\n\n\n                       ME     RMSE      MAE       MPE     MAPE MASE\nTraining set 3.001896e-12 110997.1 92601.82 -122.4252 153.9077    1\n\n\n\n\n                   ME    RMSE      MAE    MPE     MAPE MASE      ACF1\nTraining set 1652.697 6932.02 4087.552 1.2553 2.554247    1 0.0208634\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -4.089888e-12 6732.124 4019.955 -1.162417 2.944873 0.9834627\n                  ACF1\nTraining set 0.0208634\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the trending pattern."
  }
]