[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ABOUT ME",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "ARCH/GARCH.html",
    "href": "ARCH/GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARCH/TS.html",
    "href": "ARCH/TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARModels.html",
    "href": "ARModels.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "After Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation.\nTo be more specific, we need to understand the concepts about the models: Certainly! Here’s a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:"
  },
  {
    "objectID": "ASV.html",
    "href": "ASV.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "Discription:"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "DataSources.html",
    "href": "DataSources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Data Gathering\n\n\nData Discription & Source\nIn this section, different datasets are gained to help provide different analysis in later sections. Each dataset serves to answer questions that are defined in introduction section. To be more specifc, there are datasets regarding the household savings, incomes, sale prices, Gini idex, and housing affordability which can help us analysis the impact of the income disparities and housing affordability throughout USA. The detialed description of each dataset is provided.\n\n1. Household Savings\n\nLink: Household Saving Dataset\nDescription: This dataset contains information related to household savings in the United States throughout the years from 1992 to 2021. It includes data information on the saving, which can be an important economic indicator for me to make analysis. The Units for dataset are Billions of Dollars, Not Seasonally Adjusted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Real Median Household Income in the United States\n\nLink: Real Median Household Income Dataset\nDescription: This dataset represents the real median household income dataset. This is an estimation of Median Incomes. The Census Bureau has computed medians using either Pareto interpolation or linear interpolation. Currently, we are using linear interpolation to estimate all medians. Pareto interpolation assumes a decreasing density of population within an income interval, whereas linear interpolation assumes a constant density of population within an income interval. The Census Bureau calculated estimates of median income and associated standard errors for 1979 through 1987 using Pareto interpolation if the estimate was larger than $20,000 for people or $40,000 for families and households. This is because the width of the income interval containing the estimate is greater than $2,500.\n\n\n\n\n\n\n\n\n\n\n\n3. Median Sales Price of Houses Sold for the United States\n\nLink: Median Sales Price of Houses Dataset\nDescription: This dataset contains information on the median sales price of houses sold in the United States. It can be a valuable indicator of the state of the real estate market.\nThis dataset can provide me with a direct view about the sale price of the house in USA. We can see clearly about the price disparities. Then, by comparing with the income and saving, we can know about the patterns regarding the impact of disparities.\n\n\n\n\n\n\n\n\n\n\n\n4. GINI Index for the United States\n\nLink: GINI Index Dataset\nDescription: This dataset shows the Gini index for United States. Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. In addition, a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nData are based on primary household survey data obtained from government statistical agencies and World Bank country departments.\nWorld Bank collection of development indicators, compiled from officially recognized international sources. It presents the most current and accurate global development data available and includes national, regional, and global estimates. The World Bank labels these annual series, but several observations are missing..\n\n Source Indicator: SI.POV.GINI\n\n\n5. Housing Affordability Index\n\nLink: Housing Affordability Index Dataset\nDescription: This dataset contains data on the Housing Affordability Index. The housing affordability index measures the degree to which a typical family can afford the monthly mortgage payments on a typical home. Value of 100 means that a family with the median income has exactly enough income to qualify for a mortgage on a median-priced home. An index above 100 signifies that family earning the median income has more than enough income to qualify for a mortgage loan on a median-priced home, assuming a 20 percent down payment. This index is calculated for fixed mortgages.\nThis dataset can directly tell us if one can easily afford a house. This can help me make analysis the resources disparities and the possible reasons for the differences in housing affordability\n\n\n\n\n\n\n\n\n\n\n\n6. Personal Saving Rate\n\nLink: Personal Saving Rate Dataset\nDescription: This dataset is to provide information on the personal saving rate. The personal saving rate measures the percentage of disposable income that individuals save, which can be indicative of financial health. In here, personal saving as a percentage of disposable personal income (DPI), frequently referred to as “the personal saving rate,” is calculated as the ratio of personal saving to DPI.\nFrom this dataset, I can evaluate the overall changes throughout years regarding the personal savings. Then, by comparing with the income and house prices, we can see if there are any patterns or relationships.\n\n\n\n\n\n\n\n\n\n\n\n7. Gross Domestic Product: Implicit Price Deflator\n\nLink: GDP: Implicit Price Deflator Dataset\nDescription: This dataset represents the Gross Domestic Product (GDP) Implicit Price Deflator for USA, which is a measure of inflation in the economy and is used to adjust GDP for price changes.\nThis dataset aims to cover the overall GDP changes quetsions that can be utilized to make analysis on the impact of the incomes and housing prices.\n\n\n\n\n8. All Home Prices in States in the USA\n\nLink: Zillow Home Prices Dataset\nDescription: This dataset provides information on home prices in various states in the United States. It is sourced from Zillow, a well-known real estate and rental marketplace.\nThese housing datasets (from zillow group) not only provide sufficient information concerning the house values and sales prices, but also include the different regions. I wish to compare different states or regions to find if there are any patterns in house values, sales prices, and rental prices. I want to find correlations among them. In addition, some of the datasets cover a great range of time even from 2000 to 2023. This can provide with a broader view of the dataset when making visualizations across the time. We can see clearly how the housing values and sales prices changed throughout the years and make forecasts for the future.\n\nA Screen shot for the dataset: \n\n\n\nReference:\n\nCodes: Rmd, Python & Qmd\nU.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.\nU.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.\nU.S. Census Bureau and U.S. Department of Housing and Urban Development, Median Sales Price of Houses Sold for the United States [MSPUS], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MSPUS, September 19, 2023.\nWorld Bank, GINI Index for the United States [SIPOVGINIUSA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/SIPOVGINIUSA, September 19, 2023.\nNational Association of Realtors, Housing Affordability Index (Fixed) [FIXHAI], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/FIXHAI, September 19, 2023.\nU.S. Bureau of Economic Analysis, Personal Saving Rate [PSAVERT], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/PSAVERT, September 19, 2023.\nU.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.\nZillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/."
  },
  {
    "objectID": "DataVis.html",
    "href": "DataVis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we will explore the datasets that we created by using visualizations. The goal is to gain insights and ideas before applying models and analysis. The data visualization and storytelling are very crucial for us to understand the impact of the income and houseing prices disparities. In this section, by utilizing different visualizations with different tools, we endeavor to delve into the depths of the datasets, finding potential trends, seasonality, and other features.\n \n\n\nFirst of all, we need to have a basic look at for the household income, household savings, and the sale price of the house. By putting the figures together, we can have a more direct idea about the trending and oscciliation throughout the time. Through interactive tableau visualization, we can see the temporal trends and oscillations that may be important for our analysis.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the above plots, we can see that the household savings throughout the years generally follow an upwarding trend, which means that more and more people are willingly to save moneny. For Median household income, however, we can see that from 1984 to 2021, it has a lot of osccilations. Bascilly, the median household income did not change too much. And then! for sale prices of the houses, we can see that it follows a positive trend as well.\nFrom here, we can have a basic insight: while sale price increases, the median household income still remains the same throughout the years. This give us questions, why? And if this is the case, can people nowadays afford the house? In order to answer these quetsions, we need to dive into other datasets as well.      \n\n\n\nFirst, we need to have a look at of the GDP as a whole. Since GDP represent economic evaluations, which can offer us invaluable insights into the financial health and trajectory of a nation. With the GDP Implicit Price Deflator data, we aim to have insights on the potential reason of the income and sale price of house. The Deflator helps to discern between changes in GDP due to alterations in prices and changes due to alterations in quantities.\n\nBased on the interactive plotly plot, we can see the percent change from preceding period. Generally from 1960 to 1980, it follows an upwarding trend which aligns with the household savings and sale prices. Then from 1980 to 2020, it follows a downwarding trend and appears to have fluctuations from 2020 to now. From 2020, the Covid-19 surely had huge impact of the general economic. However, the decreasing of the percent change from 1980 to 2020 indicates that the percent changes are generally small, which means that for these years the GDP remains roughly the same, which aligns with the incomes from previous figure.\nThis also shows that while the GDP did not change too much, the household median incomes did not change as well. However, the sale prices and savings keep increasing. In addition, as we mentioned from introduction, the other important aspect that we want to explore is about the disparity. From the visulizations, although we see that the income keep increasing, there are still disparities in distribution of the income.      \n\n\n\nTo have a brief recap, Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. A Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nAs we all aware, the disparity between geographic area exists. The Urban areas, especially metropolitan regions, are often have more economic. In this section, we want to find the extents of disparities and correlations between the income and other factors. From exploring the Gini coefficient, we can gain insights with income disparities, housing affordability as well as the trend and seasonality.\n\nBased on the GINI index over time, we can see that it follows an upwarding trend. Which is not a good thing, this means that we are towarding more inequality throughout the time. This means that the distribution of income within an economy deviates is not equal. We can see that from 1975 to 2019, the GINI index has increased about 8. However, from 2020, it seems that we have a decline of the GINI idex. Again, the Covid also had impacts of the generaly distribution of the incomes.\nFrom here, we can see that while the median income remains roughly the same, the distribution of the income did not remain constant. Which means that while many people gaining wealth, there are still more and more proverties. This can also imply that for a part of the people, they have got more resources while the others have troubles with housing affordability. To be more specific, we need to have a look at about the relationship between personal saving rate and the house price.      \n\n\n\nIn this section, we have a focused on understanding how personal savings rates interface with average sales prices of houses in the USA. From here, we aim to identify the trend and pattern such that we can see the direct impact of the relationship between saving rate and the sale price. We can understand how changes in saving rates might reflect or impact shifts in housing market trends.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the average personal saving rate and the average sales price of the houses, we can see that the personal saving rate fluctuates throughout the years while the average sales price follows an upwarding trend. The increasing of the house prices may be attributed to factors such as population growth, urbanization. However, as population grows, the saving rate did not change too much. Again, this could also means that the disparities in geographic locations play an important role.      \n\n\n\nTaking a step further into our exploration, this section presents a bar visualization, providing a categorical breakdown of pivotal economic indicators and metrics. Our goal is to compare, contrast, and evaluate different states and MSA, identifying links and disparities among districts.\nNow, we wish to have a more specific look at about the disparities among regions. Here, we have selected some Meyropolitan Areas with states and MSAs. In here, we can see that which states or regions generally have a higher prices. Which can help us find the disparities.\n\n\nBased on the linked figure, we can see that states for CA, CO, and HI generally have higher sale prices of houses. This means that the Geographic Disparities in Housing Markets did exist throughout the time. From 2018 to 2023, most of the places also have an upwarding trend in sale prices. This can make the housing affordability to change as well.\nIn addition, the real estate landscapes of CA, CO, and HI have been prospered among economists. The increasing in housing prices in these regions indicates several factors and disparities:\nHigh demand in states like CA and HI, may be the cause of increasing prices. In addition, the economic landscapes of these states, often associated with sectors like advanced technology and high tourism that can boost the economy which makes the prices higher. However the average incomes did not change too much. This could bring affordability crisis in the future."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For Exploratory Data Analysis (EDA), we aim to have a deeper understanding of the time series data. This section involves multiple medthods and datasets for us to gain insights and understandings such as decompositions, lag plots, ACF, PACF, ADF tests, detrending, and others. For these methods, we are going to identify correlations, trends, seasonalities, and stationaries. Which can help us to make analysis and apply models in later sections. To be more specific, the lag plots and decomposing methods will allow us to discover dependencies and components. ACF and PACF can help us see the correlations and stationary, and the Augmented Dickey-Fuller Test to empirically probe its stationarity."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The Impact of Income Disparities and Housing Affordability in the United States\n\nProject Backgroung:\nWith the dramatic development of the technology and society, we have witnessed the continuous increasing in economies. However, such developments did not spread through every corner in USA. The disparities of resources still exist. There are housing price disparities, rental sales disparities, and income disparities for different regions and states. These differents play a very crucial role in our society and they bring huge impact and massive pressure to people nowadays.\nThe central focus of this project lies in understanding the relationship between the income disparities between and housing affordability in the United States. I aim to help us understand and gain new insights on how varying income levels within society directly affect the ability of individuals and families to secure housing that is both suitable and affordable. Throughout the analysis, different smaller aspects and factors will be analyzed one by one. \n\n\nThe Big Picture & Literature Review:\nIncome disparity and housing affordability are not just matters of individual but also broader societal concerns that need to be attended to. The influences of such disparities and results are not only the quality of life for countless Americans citizens but also the overall stability of communities and the economic health of the nation. As I help us explore this issue, we must recognize that it’s not simply about monetary figures; it’s about the pursuit of a fundamental human need: shelter and how income inequality, housing price, rental price, GDP, personal saveing and regional difference influence us as a whole in the United States.\nIn order to bring a more comprehensive understanding of this topic, we must delve into the body of research and discourse that precedes us. Existing literature reveals the persistent challenge of housing affordability, marked by the growing gap between incomes and housing costs.(Jajtner et al., 2020). Scholars have examined the historical trends, policy interventions, and socioeconomic factors. In this exploration of the impact of income disparities on housing affordability, we will adopt multiple analytical angles to provide a well-rounded perspective.\n\nMutiple datasets are analyzed thoroughly to provide insights and conclusions. Different methods such as Exploratory Data Analysis, ARMA Models, ARIMAX Models, Spectral Analysis and Filtering,, Financial Time Series Models, and Deep Learning methods are conducted to help me achieve the goal.\n\n\nGuiding Questions that need to be answered:\nTo guide the explorations, we have conducted some questions that would be answered using different analysis and dataset:\n\nHow have income inequality trends in the United States impacted the individuals?\nWhat are the historical trends in housing prices?\nWhat is the definition of housing affordability, and what metrics are commonly used to assess it?\nHow do income disparities vary from one region to another?\nWhat regions or states have greater disparities?\nWhat impact have the economy had on the housing market?\nIs there any trends or seasonal patterns regarding the income and housing price?\nIn what ways does housing affordability intersect with broader economic and social issues, such as workforce mobility and education outcomes?\nWhat innovative solutions and strategies can be employed to improve housing affordability for all Americans?\nWhat are the correlations between the sale price, housing price, and rental price?\nWhat future trends should we anticipate in the context of income disparities and housing affordability?\nHow can addressing income disparities lead to a more equitable and sustainable housing market in the United States?\n\nThese guiding questions will serve as different milestones throughout this exploration. The questions might be changed and increased as the project proceeds."
  },
  {
    "objectID": "SAF.html",
    "href": "SAF.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "work in progress （optional) Might do it later"
  },
  {
    "objectID": "GARCH.html",
    "href": "GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "In this section, the goal is to determine if the GARCH model can help the dataset better. The basic work flow of this section is to compare different models within ARIMA models. Then Based on different analysis to determine whether a GARCH model can help or not. In my case, the goal is to apply the models on the following datasets: Income Stock, Home value, Sale price, Rental price. The corresponding returns are being calculated before fitting into to differnet models through model diagnostics."
  },
  {
    "objectID": "TS.html",
    "href": "TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "The first analysis is the median sale price analysis, which is also utilzied during previous sections. In here, the goal is to utilize three different models: LSTM, RNN, and GRU. Then make comparsion between deep learning methods and ARIMA models.\n\n\nThe corresponding time seires dataset is prepared and cleaned with objective column and then use train test splits for furthering steps in later section.\n\n\nCode\nimport numpy as np\nimport warnings\nimport pandas as pd\nimport os\nimport random\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.layers import SimpleRNN, Dense\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense\nfrom tensorflow.keras import regularizers\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\nwarnings.filterwarnings('ignore')\n# Setting seeds for reproducibility\nrandom.seed(236)\nnp.random.seed(236)\ntf.random.set_seed(236)\ndf = pd.read_csv(\"../Dataset/project/MSPUS.csv\")\ndf.head()\n\n\n\ndf = df.rename(columns={\"MSPUS\": \"y\"}) # The objective\ndf = df[[\"DATE\", \"y\"]]\nX = np.array(df[\"y\"].values.astype(\"float32\")).reshape(df.shape[0], 1)\n\n# Train and Test Split & Normalization\n\ndef train_test_split(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n   \n    split = int(n * split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\ntrain_data, test_data, data = train_test_split(X)\n\nprint(\"train shape:\", train_data.shape)\nprint(\"test shape:\", test_data.shape)\n\n\nC:\\Users\\yzh20\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n\n\n\nWARNING:tensorflow:From C:\\Users\\yzh20\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\nWARNING:tensorflow:From C:\\Users\\yzh20\\AppData\\Local\\Temp\\ipykernel_39636\\124761955.py:15: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n\n\n\ntrain shape: (193,)\ntest shape: (49,)\n\n\n\n\n\nBelow, we can see clearly about the train and test dataset that I splitted. The goal is to have a visulization of the dataset as a whole.\n\n\nCode\nfig, ax = plt.subplots(figsize=(15, 6), dpi=100)  # Set the size and DPI of the figure\nfig.patch.set_facecolor('#E0E0E0')  # Set the background color for the outer figure\nax.set_facecolor('#E0E0E0')  # Set the background color for the axes\n\n# Plot the training data\nax.plot(range(0, len(train_data)), train_data, \"-\", label=\"Training Data\")\n\n# Plot the test data\nax.plot(range(len(train_data), len(train_data) + len(test_data)), test_data, \"-\", label=\"Test Data\")\n\n# Set labels and title\nax.set(xlabel=\"Time (days)\", ylabel=\"Median Sale Price Scaled\", title=\"Median Sale Price Over Time\")\n\n# Add grid with white color for better visibility on the gray background\nax.grid(color='white')\n\n# Add legend to the plot\nax.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nIn order to correctly fit into the corresonding models, the size of x and y must be determined and transformed correctly.\n\n\nCode\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps, plot_data_partition=False):\n    global X_ind, X, Y_ind, Y  # use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    # print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind = [*range(time_steps * rows_x)]\n    del X_ind[::time_steps]  # if time_steps=10 remove every 10th entry\n    X = dat[X_ind]\n\n    # PLOT\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y, \"o\", X_ind, X, \"-\")\n        plt.show()\n\n    # RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps - 1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n# PARTITION DATA\np = 30  #\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\n #USER PARAM\nrecurrent_hidden_units = 3\nepochs = 200\nf_batch = 0.2  # fraction used for batch size\noptimizer = \"RMSprop\"\nvalidation_split = 0.2\n# trainY=trainY.reshape(trainY.shape[0],1)\n# testY=testY.reshape(testY.shape[0],1)\nprint(\"Testing Array Shape:\", testX.shape, testY.shape)\nprint(\"Training Array Shape:\", trainX.shape, trainY.shape)\n\n\nTesting Array Shape: (1, 29, 1) (1,)\nTraining Array Shape: (6, 29, 1) (6,)\n\n\nNow, we are ready for training into the model for analysis.\n\n\n\nFor this LSTM model, we will compare the performance using RMSE for regularization and no regularization.\n\n\n\n\nCode\nmodel = Sequential()\n\nmodel.add(\n    LSTM(\n        \n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n       \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm (LSTM)                 (None, 3)                 60        \n\n\n                                                                 \n\n\n dense (Dense)               (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=epochs,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median Sale Price Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 272ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00324 RMSE = 0.05688\nTest MSE = 0.08436 RMSE = 0.29045\n\n\nFor LSTM with no regulariztaion, we have the test and train RMSE: 0.29045 and 0.05688\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\nWe can see that from this plot based on LSTM no regulariztaion, the performance of the model is pretty good. The targets and prediction values are close.\n\n\n\n\nNow, we compare the model by adding some regularization. The goal of the regularization is to prevent overfitting.\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    LSTM(\n        # model.add(SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_1 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 247ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00049 RMSE = 0.02203\nTest MSE = 0.02428 RMSE = 0.15581\n\n\nFrom here, we can see that the test and train RMSE becomes: 0.15581 and 0.02203.\nIn this case, by adding the regularization the RMSE becomes lower. This means that my model did not overfit. Overall, LSTM performs well.\n\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\n\n\n\nThe result suggests that the model is able to capture the underlying trend in the data quite well. Regularization comparsion proved that the model did not overfit. Also, the smoothness of the prediction curve indicates that the L2 regularization has effectively penalized overly complex models that could have fit the noise in the training data.\n\n\n\nThe predictions seem to align well with the actual data for most of the original dataset. However, there appears to be a slight divergence toward the end. This divergence might indicate the limit of the model’s predictive horizon, which is about 175 days ahead. In this case, we can say that the model can predict with relatively accuracy for 200 days ahead.\n\n\n\n\n\n\nLikewise, we will test the model prediction and errors with and without regularization for each model.\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n     \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_2\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 97ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 19ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00137 RMSE = 0.03706\nTest MSE = 0.05575 RMSE = 0.23612\n\n\nWith no regularization, the RNN performs better than LSTM with lower rmse. However, regularization can help us prevent overfitting. Therefore, we need further using regularization to make thorough analysis.\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\n\n\n\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Ensures a 1D array output\n\n# Check shapes again to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_3\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 116ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.27946 RMSE = 0.52864\nTest MSE = 1.46867 RMSE = 1.21189\n\n\n\n\nCode\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (train_rmse**2.0, train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (test_rmse**2.0, test_rmse))\n\n\n\n\n\nTrain MSE = 0.27946 RMSE = 0.52864\nTest MSE = 1.46867 RMSE = 1.21189\n\n\nNow, there are relatievly large differences when using regularization in RNN model. The results show that the model will overfit if we do not use regularization in RNN. The result for RNN model with regularization performs less than the LSTM model with higher rmse values.\n\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\nThe figure of the predictions and actual values also proves that the RNN model with regulariztaion performs less than the LSTM model. We can see that there are differences for the step points\n\n\n\n\nThe third model is GRU, we will also test the two results for both with regulariztaion and no regularization. Then by combining the results in a table, we can compare them clearly.\n\n\nLikewise, we will test the model prediction and errors with and without regularization for each model.\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n      \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_4\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru (GRU)                   (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_4 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 238ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00003 RMSE = 0.00535\nTest MSE = 0.00281 RMSE = 0.05301\n\n\nWe can see that with no regulariztaion, the model performs very good. However, it could be overfitting.\n\n\nCode\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\nThe predictions and actual values align very well. This could be the reason of overfitting.\n\n\n\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()  # Ensures a 1D array output\ntest_predict = model.predict(testX).flatten()\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_5\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_1 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_5 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 226ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nTrain MSE = 0.01689 RMSE = 0.12998\nTest MSE = 0.15742 RMSE = 0.39676\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)\n\n\n\n\n\nWe can see that if we add regularization, the result of RMSE becomes higher. This shows that with no regularization, the GRU model overfitted.\nIn general, in my case, with regularization, LSTM model performed the best among the three models. RNN model perfomed the least among the three models."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Author: Zonghong Yu"
  },
  {
    "objectID": "dv.html#question-2",
    "href": "dv.html#question-2",
    "title": "Data Vizes in TS",
    "section": "Question 2",
    "text": "Question 2\n\n1. Try to reproduce the Data Vizes similar to in Lab0/1 “Data Viz Examples” but with a different set of stock prices. (use the quantmod package to get stock prices from yahoo finance https://finance.yahoo.com/lookup/)\n\na.\nIn here, I utilize three different set of stock prices: GOOGL, MSFT - Microsoft Corporation, and NFLX - Netflix Inc. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretations: The three stocks as showned in figure above represent:Google, MicroSoft, and Netflix. We can see that in general, Netflix had an overall highiest stock prices throughout the years. And Google has the least over the years. However, all three of them had fluctuations especially in year 2022, which could be the impact of the Covid-19.\n\n\nb. Plot the climate data (climate.csv) using plotly. https://plotly.com/r/\n\n\nInterpretations: By using the plotly on the climate dataset provided, we can see some interesting features. From above interaction figure, we can see the relation between the Date and the max temperature. We can see that during July and August in 2021, the max temperature is higher in general while lower during October and December which is reasonable.\n\n\nc. Get any economic data / macroeconomic indicators and plot using plotly https://fred.stlouisfed.org/ https://www.bea.gov/\nI utilized the GEPUCURRENT dataset, which is Global Economic Policy Uncertainty Index: Current Price Adjusted GDP, from https://fred.stlouisfed.org/series/GEPUCURRENT\n\n\nInterpretations: By using the plotly on the economic dataset from the website givem, I chose the GDP index of some global economic policy. From above interaction figure, we can see the relation between the Date and the index. We can see that the index follows an upwarding trend throughout the years. However, there are many local maximums and fluctuations throughout the years. In addition, the idex reached at its peak on 2020/05/01.\n\n\n\n2. Make only the plots visible in your webpage. (set echo=FALSE in your R code chunck)\nOnly plots visible as required.\n\n\n3. Add interpretations to all the plots in the webpage.\nAdded as required."
  },
  {
    "objectID": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "ABOUT ME",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "Time Series",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "EDA.html#the-gdp-exploratory-data-analysis",
    "href": "EDA.html#the-gdp-exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The GDP Exploratory Data Analysis",
    "text": "The GDP Exploratory Data Analysis\nThen, after making analysis for income, sale price, and houseing affordability. We should have a look at for analyzing the GDP, which can represent the economy as a whole. By analyzing the GDP data, we can have a more generalized view and gain more insights about the patterns, seasonalities, and stationaries.\n\nTime Series Plot of GDP\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1980, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Which is similar to the houseing affordability index. I think we can see that there is a positive correlation between GDP and housing affordability index. When GDP gets higher, people can afford a house more easily. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow a multiplicative pattern. Because it seems that it does not have constant amplitude and frequency. In order to prove these, we need to explore more using different methods.\n\n\nLag for GDP\n\n\n\n\n\nBased on the lag plot, there’s a correlation but not too strong. This shows potential linearity. However, it could also mean that the dataset is not stationary but we can not decide yet.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has almost constant amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the acf decays very quickly, which also means that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$A191RI1Q225SBEA\nDickey-Fuller = -3.8947, Lag order = 6, p-value = 0.01466\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we reject the Null hypothesis. The time series dataset is stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformation\n\n\n\n\n\n\n\n\n\n\nSince, the original dataset is already stationary. Therefore, no further action is needed to make it stationary. The detrend and log transformation is just to explore the dataset.\n\n\nMoving Average\n\n\nThe “Original” line shows the raw GDP changes. The MA(5), MA(20), and MA(50) represent moving averages with windows of 5, 20, and 50 time units, respectively. It’s clear that as the window size increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-mean-sale-price-analysis",
    "href": "EDA.html#the-mean-sale-price-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean Sale Price Analysis",
    "text": "The Mean Sale Price Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822"
  },
  {
    "objectID": "EDA.html#the-mean-house-sale-price-data-analysis",
    "href": "EDA.html#the-mean-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean House Sale Price Data Analysis",
    "text": "The Mean House Sale Price Data Analysis\n\n\n\n\n\n\n\n\nFrom the plot, you can visually inspect:\nTrend: Any consistent upward or downward direction. Seasonality: Any repeating patterns or cycles. Variation: Fluctuations in the data. Periodic fluctuations: Spikes or drops at consistent intervals. Determine if the time series looks multiplicative or additive. An additive time series has constant amplitude and frequency, while a multiplicative one has varying amplitude/frequency.\n\n\n\n\n\nYou’d interpret the lag plot by looking for any structure. If the points cluster along a diagonal line from bottom-left to top-right, there’s a positive correlation. Any other pattern might suggest non-linearity or some pattern not captured by mere linear correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary"
  },
  {
    "objectID": "EDA.html#the-median-house-sale-price-data-analysis",
    "href": "EDA.html#the-median-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nSince our key is to discover the impact of the income and house price, we certainly need to make analysis for the sale price data.\n\nThe Time Series\n\n\n\n\n\n\n\n\nFrom the plot, the results are the same with the previous section. It has an upwarding trend.\nTo be more specific, from the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists but not too much. Periodic fluctuations: The fluctuations are presented in 2010 and 2021. multiplicative or additive： Only based on the timeseries plot, I think this follows additive, because it does not have too much variance.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plots\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. In addition, this aligns with the same conclusion and it has a strong positive correlation. This shows linearity.\n\n\nDecomposition for the data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. In addition, it has roughly constant amplitude/frequency. Therefore, I think this follows additive.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF of the data\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log transformation\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending, the sale prices remains fluctuations espectially from 2000 to 2020 period. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$MSPUS)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Sale Price Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThe graph shows the House Sale Price from 1970 to 2020. The blue line represents the original data. MA(2), MA(8), and MA(16) depict the moving averages over 2, 8, and 16 periods respectively. The smoothed lines show the underlying trend of house sale prices over time. The higher the period of the moving average, the smoother the line, which can help in understanding long-term trends."
  },
  {
    "objectID": "EDA.html#the-housing-affordability-index-data-analysis",
    "href": "EDA.html#the-housing-affordability-index-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Housing Affordability Index Data Analysis",
    "text": "The Housing Affordability Index Data Analysis\nThen, after we have a basic understanding about the income and sale price. We need to dive into the impact. We utilize the housing affordability index to see the impact of the income and sale price. Let us firt explore the houseing affordability index first.\n\nTime Series Plot for Housing Affordability Index\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = PSAVERT)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Time Series Plot of Housing Affordability Index\",\n       x = \"Date\", \n       y = \"The Housing Affordability Index\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1970, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow an additive pattern. Because it seems that it has constant amplitude and frequency although with some fluctuations. In order to prove these, we need to explore more using different methods.\n\n\nLag Plot for Houseing Affordability\n\n\nCode\n# Lag plot\nlagged_data <- data.frame(value = df$PSAVERT[-1],\n                          lagged_value = df$PSAVERT[-length(df$PSAVERT)])\n\n# Enhanced Lag Plot\nggplot(data = lagged_data, aes(x = lagged_value, y = value)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Lag Plot\",\n       x = \"Value at t-1\",\n       y = \"Value at t\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Housing Affordability\n\n\nCode\n#library(ggfortify)\n\n# Decomposition using ggplot2 styling\ndecomposed <- decompose(ts(df$PSAVERT, frequency=12), type = \"additive\")\nautoplot(decomposed) + \n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, mostly it has constant amplitude/frequency. Therefore, I think this follows additive pattern.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF Analysis\n\n\nCode\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$PSAVERT) +\n  labs(title = \"ACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# PACF Plot\nggPacf(df$PSAVERT) +\n  labs(title = \"PACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nThe ACF plot can help determine if the series is stationary. From here, we can see that the dataset has a correlation, and it is decaying slowly which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\nCode\nlibrary(tseries)\nadf.test(df$PSAVERT)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$PSAVERT\nDickey-Fuller = -3.543, Lag order = 9, p-value = 0.03817\nalternative hypothesis: stationary\n\n\nBased on the ADF test, since the p-value is smaller than the threshold value, we should reject the null hypothesis, which means that the dataset is stationary! However, the value is close to 0.05. In addition, the ADF test is not as reliable as the ACF test. Therefore, since the ACF decays slowly, and it showed strong correlation. This means that, the dataset is not stationary.\n\n\nDetrened and Log-transformation\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nCode\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$PSAVERT))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCode\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$PSAVERT))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Housing Affordability Index\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nENsure it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$PSAVERT)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Housing Affordability IndexTime Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThis graph presents the Housing Affordability Index from 1960 to 2020. The blue line is the original data. MA(2), MA(10), and MA(20) represent moving averages over 2, 10, and 20 periods. The plot illustrates how housing affordability has changed over time, and the smoothed lines can help identify broader trends or shifts in the data."
  },
  {
    "objectID": "EDA.html#the-household-income-data-analysis",
    "href": "EDA.html#the-household-income-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nAs we did in previous section, we start with the household income. In here, we have a basic look about the household income time series.\n\n\n\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： Only based on the timeseries plot, I think this follows multiplicative, because it has varying amplitude/frequency.\n \nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\nLAG Plot\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has varying amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$W398RC1A027NBEA)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Household Income Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Household Income Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: spikes all fall within the blue shaded confidence intervals, which represent the region where correlations are not statistically significant.\n\n\nMoving Average\n\n\nThe graph plots the Household Income from 1995 to 2020. The blue line represents the original data. MA(2), MA(4), and MA(8) are the moving averages taken over 2, 4, and 8 periods respectively. As the period of the moving average increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-sale-price-disparity-analysis",
    "href": "EDA.html#the-sale-price-disparity-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Sale Price Disparity Analysis",
    "text": "The Sale Price Disparity Analysis\nLastly, after analyzing the corresponding numeric datasets, we will explore more about the disparitis by using the Mean Sale price datasets which involves with different regions and states. From here, by making analysis on the pattern and seasonality, it can help us provide with more insights and knowledge in determining the impact of the disparities of the income and sale prices.\n\nTime Series Decomposition Plot\n\n\n\n\n\n\n\n\n\n\n\nFrom the decomposition plot, we can visually inspect: Trend: There is no clear trend. Seasonality: It seems that it has seasonal patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： I think this follows multiplicative, because it has varying amplitude/frequency.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plot\n\n\n\n\n\nBased on the lag plot, there’s a strong positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity.\nIn order to explore more, we utilize the acf and pacf to check the accuracy.\n\n\nACF and PACF plots\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation and decays slowly. Howeverm for PACF, it decays dramatically. Therefore, we can say that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF tests\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we can reject the Null hypothesis. The time series dataset is stationary.\nThen, we wish to make analysis for different values to see if they have correlations among each other. A heatmap can also help us to determine the correlations among the sale price, rental price, and homevalue of the dataset.\n\n\nCorrelation heatmap\n\n\n\n\n\n\n\n\n\n\nMoving Average\n\n\n\n\n\n\n\n\nFrom here, we can see that there is a strong positive correlation between the Sale price and home value. The higher of the home value, the higher of the sale price. However, the rental price did not have too much correlations with the other two. The possible reason could be the cause of the geographic difference and this could also affect the housing affordability.\nIn conclusion, in this section, we provide different observations. And these observations underscore the nature of real estate pricing, incomes, GDP changes througout the years as well as the housing affordability. In addition, different patterns and features are found to identify the trend, seasonality, patterns, and correlations. which allow us to have a more understanding about the dataset."
  },
  {
    "objectID": "ARModels.html#the-household-income-data-analysis",
    "href": "ARModels.html#the-household-income-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and transforming, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.934, Lag order = 3, p-value = 0.598\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\n\n\n\nMake it Stationary by using differencing\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n\n\n\n\n\n\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 2)\nDickey-Fuller = -2.9811, Lag order = 3, p-value = 0.1972\nalternative hypothesis: stationary\n\n\nHowever, the second order of differencing still cannot make it stationary. We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n\nThird Differencing\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 3)\nDickey-Fuller = -5.488, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n\nEvaluate the values for p & q\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.340 -0.006  0.060 -0.087 -0.038  0.241 -0.349  0.214 -0.036 -0.050 \n    11     12     13     14 \n 0.007  0.065 -0.115  0.150 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.340 -0.137  0.013 -0.069 -0.100  0.211 -0.232  0.062 -0.004 -0.007 -0.053 \n    12     13     14 \n 0.017  0.015  0.007 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\nFit Model\nAfter we determing our parameters, we can start fitting the models.\n\n\nSeries: diff \nARIMA(1,3,2) \n\nCoefficients:\n          ar1      ma1     ma2\n      -0.7911  -1.8982  0.9996\ns.e.   0.2096   0.4024  0.4155\n\nsigma^2 = 919174:  log likelihood = -202.53\nAIC=413.07   AICc=415.17   BIC=417.78\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set -104.3663 845.5254 428.3458 153.1425 163.6866 0.695587 -0.3066825\n\n\nFrom here, we get a summary about the aic and bic values.\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 2), and (d = 3), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 3, q))\n\nCoefficients:\n          ar1      ma1      ma2\n      -0.4402  -0.2544  -0.0395\ns.e.   0.0377   0.0394   0.0262\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.19,  aic = 28412.39\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n422.0558\n430.2596\n425.8740\n\n\n2\n2\n2\n413.6960\n420.3570\n416.4232\n\n\n2\n3\n2\n405.0099\n411.4891\n407.8670\n\n\n2\n1\n3\n423.9044\n433.4754\n429.2377\n\n\n2\n2\n3\n411.0795\n419.0727\n415.0795\n\n\n2\n3\n3\n407.0239\n414.7989\n411.2239\n\n\n2\n1\n4\n425.8596\n436.7980\n433.0596\n\n\n2\n2\n4\n413.0766\n422.4020\n418.6766\n\n\n3\n1\n2\n423.9406\n433.5117\n429.2739\n\n\n3\n2\n2\n415.6154\n423.6086\n419.6154\n\n\n3\n3\n2\n406.9074\n414.6824\n411.1074\n\n\n3\n1\n3\n426.0532\n436.9916\n433.2532\n\n\n3\n2\n3\n413.0776\n422.4030\n418.6776\n\n\n3\n1\n4\n427.4187\n439.7243\n436.8923\n\n\n4\n1\n2\n425.4315\n436.3699\n432.6315\n\n\n4\n2\n2\n415.1668\n424.4922\n420.7668\n\n\n4\n1\n3\n427.4290\n439.7347\n436.9027\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\nForecasting for the dataset using the best parameters\n\n\n   Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n31       4278.130 3813.470  4742.791 3567.493  4988.767\n32       3458.665 2921.482  3995.847 2637.115  4280.215\n33       5473.540 4496.374  6450.705 3979.094  6967.985\n34       4679.682 3529.482  5829.881 2920.603  6438.760\n35       6742.303 5099.447  8385.159 4229.772  9254.834\n36       5988.600 4080.957  7896.243 3071.112  8906.088\n37       8093.980 5637.246 10550.714 4336.729 11851.230\n38       7382.141 4577.705 10186.577 3093.127 11671.156\n39       9529.693 6118.454 12940.932 4312.654 14746.732\n40       8859.921 5024.086 12695.756 2993.518 14726.325\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 42.947, df = 6, p-value = 1.195e-07\n\nModel df: 0.   Total lags used: 6\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$W398RC1A027NBEA \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.2008  -0.0093\ns.e.   0.1702   0.2238\n\nsigma^2 = 111877:  log likelihood = -208.7\nAIC=423.4   AICc=424.36   BIC=427.5\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE    MAPE      MASE      ACF1\nTraining set 83.97069 317.3154 158.0274 3.851463 17.2448 0.9308784 -0.111543\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 32.73409 312.2898 183.7851 -1.612877 23.50814 1.082607 -0.08540118\n\n\nTime Series:\nStart = 31 \nEnd = 50 \nFrequency = 1 \n [1]  4278.130  3458.665  5473.540  4679.682  6742.303  5988.600  8093.980\n [8]  7382.141  9529.693  8859.921 11049.575 10421.894 12653.642 12068.054\n[15] 14341.896 13798.400 16114.336 15612.934 17970.962 17511.654\n\n\n\n\nAccuracy of the based models\n\n\n                        ME     RMSE      MAE       MPE     MAPE MASE\nTraining set -2.643441e-14 635.2356 470.2565 -51.63239 78.34547    1\n\n\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE       ACF1\nTraining set 66.26655 330.1867 169.7616 2.867066 18.06051    1 -0.2730483\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.350993e-14 323.4687 166.4792 -9.624318 20.73922 0.9806647\n                   ACF1\nTraining set -0.2730483\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending."
  },
  {
    "objectID": "ARModels.html#the-median-house-sale-price-data-analysis",
    "href": "ARModels.html#the-median-house-sale-price-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.7007, Lag order = 6, p-value = 0.7017\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\nMake it Stationary by using differencing\n\n\n\n\n\nAfter the first order of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$MSPUS, differences = 1)\nDickey-Fuller = -6.3462, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.625  0.098  0.079 -0.054  0.021  0.024 -0.095  0.071  0.094 -0.222 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.106  0.112 -0.158  0.057  0.021 -0.059  0.114 -0.106 -0.050  0.225 -0.228 \n    22     23 \n 0.130 -0.083 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.625 -0.481 -0.278 -0.165 -0.069  0.047 -0.070 -0.119  0.136 -0.028 -0.186 \n    12     13     14     15     16     17     18     19     20     21     22 \n 0.022  0.044 -0.013  0.008 -0.042  0.052  0.040 -0.118  0.082 -0.035  0.115 \n    23 \n-0.010 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.\n\n\nFit model\n\n\nSeries: diff \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.0092  -0.9827\ns.e.  0.0663   0.0133\n\nsigma^2 = 46013474:  log likelihood = -2458.55\nAIC=4923.1   AICc=4923.2   BIC=4933.54\n\nTraining set error measures:\n                   ME     RMSE      MAE  MPE MAPE      MASE        ACF1\nTraining set 542.7894 6740.971 3865.506 -Inf  Inf 0.6808965 -0.01153987\n\n\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 1), and (d = 1), we can write out the ARIMA(1,1,1) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^1 X_t = X_t + 0.0092X_{t-1} - 0.9827Y_{t-1} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 1, q))\n\nCoefficients:\n          ma1\n      -0.1934\ns.e.   0.0098\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.07,  aic = 28408.15\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n4938.630\n4952.569\n4938.799\n\n\n1\n2\n1\n4923.100\n4933.542\n4923.202\n\n\n1\n3\n1\n4976.098\n4986.528\n4976.200\n\n\n1\n1\n2\n4937.123\n4954.547\n4937.379\n\n\n1\n2\n2\n4924.722\n4938.645\n4924.892\n\n\n1\n3\n2\n4915.708\n4929.614\n4915.879\n\n\n1\n1\n3\n4921.957\n4942.866\n4922.316\n\n\n1\n2\n3\n4926.078\n4943.482\n4926.335\n\n\n1\n3\n3\n4918.622\n4936.004\n4918.879\n\n\n2\n1\n1\n4938.832\n4956.256\n4939.087\n\n\n2\n2\n1\n4933.117\n4947.039\n4933.287\n\n\n2\n3\n1\n4931.865\n4945.771\n4932.036\n\n\n2\n1\n2\n4918.505\n4939.414\n4918.864\n\n\n2\n2\n2\n4924.928\n4942.331\n4925.185\n\n\n2\n3\n2\n4923.269\n4940.651\n4923.526\n\n\n2\n1\n3\n4918.739\n4943.132\n4919.219\n\n\n2\n2\n3\n4903.774\n4924.657\n4904.134\n\n\n2\n3\n3\n4913.868\n4934.727\n4914.230\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\nSince we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.\n\n\nForecasting for the dataset\n\n\n    Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n243       415627.1 406980.8 424273.4 402403.7 428850.4\n244       415720.2 403091.1 428349.3 396405.7 435034.7\n245       416226.1 400354.8 432097.4 391953.1 440499.2\n246       417033.1 398322.0 435744.1 388416.9 445649.2\n247       418059.5 396783.9 439335.2 385521.2 450597.8\n248       419246.0 395615.0 442877.1 383105.4 455386.6\n249       420549.3 394731.0 446367.6 381063.5 460035.0\n250       421937.7 394071.9 449803.4 379320.7 464554.7\n251       423388.1 393593.8 453182.5 377821.6 468954.7\n252       424883.9 393263.4 456504.3 376524.6 473243.1\n253       426412.6 393055.6 459769.6 375397.5 477427.7\n254       427965.4 392950.6 462980.2 374415.0 481515.9\n255       429535.8 392933.2 466138.3 373557.0 485514.5\n256       431118.9 392991.1 469246.7 372807.5 489430.4\n257       432711.4 393114.6 472308.3 372153.3 493269.6\n258       434310.7 393295.7 475325.8 371583.7 497037.8\n259       435915.0 393528.0 478302.1 371089.6 500740.4\n260       437522.9 393806.0 481239.8 370663.7 504382.1\n261       439133.4 394125.3 484141.5 370299.5 507967.4\n262       440745.9 394482.2 487009.6 369991.7 511500.1\n263       442359.7 394873.3 489846.2 369735.5 514984.0\n264       443974.6 395296.0 492653.3 369527.0 518422.2\n265       445590.3 395747.7 495432.8 369362.6 521817.9\n266       447206.4 396226.4 498186.4 369239.2 525173.6\n267       448823.0 396730.3 500915.7 369154.1 528491.9\n268       450439.9 397257.7 503622.1 369104.7 531775.0\n269       452056.9 397807.1 506306.8 369089.0 535024.9\n270       453674.2 398377.3 508971.1 369104.9 538243.5\n271       455291.5 398967.0 511616.0 369150.6 541432.4\n272       456908.9 399575.2 514242.7 369224.5 544593.3\n273       458526.4 400200.9 516851.9 369325.2 547727.6\n274       460143.9 400843.2 519444.6 369451.3 550836.5\n275       461761.5 401501.3 522021.6 369601.6 553921.4\n276       463379.1 402174.5 524583.6 369774.8 556983.3\n277       464996.6 402862.1 527131.2 369970.0 560023.3\n278       466614.3 403563.4 529665.1 370186.3 563042.3\n279       468231.9 404277.8 532185.9 370422.6 566041.2\n280       469849.5 405004.8 534694.2 370678.1 569020.9\n281       471467.1 405743.9 537190.4 370952.1 571982.2\n282       473084.8 406494.5 539675.0 371243.8 574925.7\n283       474702.4 407256.4 542148.4 371552.6 577852.2\n284       476320.0 408028.9 544611.1 371877.8 580762.3\n285       477937.7 408811.8 547063.6 372218.7 583656.6\n286       479555.3 409604.6 549506.0 372574.9 586535.7\n287       481172.9 410407.0 551938.8 372945.8 589400.0\n288       482790.6 411218.8 554362.4 373330.9 592250.2\n289       484408.2 412039.5 556776.9 373729.8 595086.6\n290       486025.8 412868.9 559182.8 374141.9 597909.8\n291       487643.5 413706.7 561580.3 374566.9 600720.1\n292       489261.1 414552.6 563969.7 375004.3 603518.0\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1986.7, df = 10, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$MSPUS \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n         ma1    sma1\n      0.0210  0.1671\ns.e.  0.0665  0.0793\n\nsigma^2 = 47248340:  log likelihood = -2470.36\nAIC=4946.73   AICc=4946.83   BIC=4957.18\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 1355.307 6831.004 4055.777 1.052941 2.484314 0.9922265 -0.04153127\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 2.720481 6690.746 3914.234 -0.9087146 2.734111 0.9575986\n                    ACF1\nTraining set -0.05135495\n\n\nTime Series:\nStart = 243 \nEnd = 262 \nFrequency = 1 \n [1] 415627.1 415720.2 416226.1 417033.1 418059.5 419246.0 420549.3 421937.7\n [9] 423388.1 424883.9 426412.6 427965.4 429535.8 431118.9 432711.4 434310.7\n[17] 435915.0 437522.9 439133.4 440745.9\n\n\n\n\nAccuracy of the based models\n\n\n                       ME     RMSE      MAE       MPE     MAPE MASE\nTraining set 3.001896e-12 110997.1 92601.82 -122.4252 153.9077    1\n\n\n\n\n                   ME    RMSE      MAE    MPE     MAPE MASE      ACF1\nTraining set 1652.697 6932.02 4087.552 1.2553 2.554247    1 0.0208634\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -4.089888e-12 6732.124 4019.955 -1.162417 2.944873 0.9834627\n                  ACF1\nTraining set 0.0208634\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the trending pattern."
  },
  {
    "objectID": "ARModels.html#the-household-saving-data-analysis",
    "href": "ARModels.html#the-household-saving-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Household Saving Data Analysis",
    "text": "The Household Saving Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and transforming, the household Saving remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.934, Lag order = 3, p-value = 0.598\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\n\n\n\nMake it Stationary by using differencing\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n\n\n\n\n\n\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 2)\nDickey-Fuller = -2.9811, Lag order = 3, p-value = 0.1972\nalternative hypothesis: stationary\n\n\nHowever, the second order of differencing still cannot make it stationary. We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n\nThird Differencing\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 3)\nDickey-Fuller = -5.488, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n\nEvaluate the values for p & q\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.340 -0.006  0.060 -0.087 -0.038  0.241 -0.349  0.214 -0.036 -0.050 \n    11     12     13     14 \n 0.007  0.065 -0.115  0.150 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.340 -0.137  0.013 -0.069 -0.100  0.211 -0.232  0.062 -0.004 -0.007 -0.053 \n    12     13     14 \n 0.017  0.015  0.007 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\nFit Model\nAfter we determing our parameters, we can start fitting the models.\n\n\nSeries: diff \nARIMA(1,3,2) \n\nCoefficients:\n          ar1      ma1     ma2\n      -0.7911  -1.8982  0.9996\ns.e.   0.2096   0.4024  0.4155\n\nsigma^2 = 919174:  log likelihood = -202.53\nAIC=413.07   AICc=415.17   BIC=417.78\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set -104.3663 845.5254 428.3458 153.1425 163.6866 0.695587 -0.3066825\n\n\nFrom here, we get a summary about the aic and bic values.\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 2), and (d = 3), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 3, q))\n\nCoefficients:\n          ar1      ma1      ma2\n      -0.4402  -0.2544  -0.0395\ns.e.   0.0377   0.0394   0.0262\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.19,  aic = 28412.39\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n422.0558\n430.2596\n425.8740\n\n\n2\n2\n2\n413.6960\n420.3570\n416.4232\n\n\n2\n3\n2\n405.0099\n411.4891\n407.8670\n\n\n2\n1\n3\n423.9044\n433.4754\n429.2377\n\n\n2\n2\n3\n411.0795\n419.0727\n415.0795\n\n\n2\n3\n3\n407.0239\n414.7989\n411.2239\n\n\n2\n1\n4\n425.8596\n436.7980\n433.0596\n\n\n2\n2\n4\n413.0766\n422.4020\n418.6766\n\n\n3\n1\n2\n423.9406\n433.5117\n429.2739\n\n\n3\n2\n2\n415.6154\n423.6086\n419.6154\n\n\n3\n3\n2\n406.9074\n414.6824\n411.1074\n\n\n3\n1\n3\n426.0532\n436.9916\n433.2532\n\n\n3\n2\n3\n413.0776\n422.4030\n418.6776\n\n\n3\n1\n4\n427.4187\n439.7243\n436.8923\n\n\n4\n1\n2\n425.4315\n436.3699\n432.6315\n\n\n4\n2\n2\n415.1668\n424.4922\n420.7668\n\n\n4\n1\n3\n427.4290\n439.7347\n436.9027\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\nForecasting for the dataset using the best parameters\n\n\n   Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n31       4278.130 3813.470  4742.791 3567.493  4988.767\n32       3458.665 2921.482  3995.847 2637.115  4280.215\n33       5473.540 4496.374  6450.705 3979.094  6967.985\n34       4679.682 3529.482  5829.881 2920.603  6438.760\n35       6742.303 5099.447  8385.159 4229.772  9254.834\n36       5988.600 4080.957  7896.243 3071.112  8906.088\n37       8093.980 5637.246 10550.714 4336.729 11851.230\n38       7382.141 4577.705 10186.577 3093.127 11671.156\n39       9529.693 6118.454 12940.932 4312.654 14746.732\n40       8859.921 5024.086 12695.756 2993.518 14726.325\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 42.947, df = 6, p-value = 1.195e-07\n\nModel df: 0.   Total lags used: 6\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$W398RC1A027NBEA \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.2008  -0.0093\ns.e.   0.1702   0.2238\n\nsigma^2 = 111877:  log likelihood = -208.7\nAIC=423.4   AICc=424.36   BIC=427.5\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE    MAPE      MASE      ACF1\nTraining set 83.97069 317.3154 158.0274 3.851463 17.2448 0.9308784 -0.111543\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 32.73409 312.2898 183.7851 -1.612877 23.50814 1.082607 -0.08540118\n\n\nTime Series:\nStart = 31 \nEnd = 50 \nFrequency = 1 \n [1]  4278.130  3458.665  5473.540  4679.682  6742.303  5988.600  8093.980\n [8]  7382.141  9529.693  8859.921 11049.575 10421.894 12653.642 12068.054\n[15] 14341.896 13798.400 16114.336 15612.934 17970.962 17511.654\n\n\n\n\nAccuracy of the based models\n\n\n                        ME     RMSE      MAE       MPE     MAPE MASE\nTraining set -2.643441e-14 635.2356 470.2565 -51.63239 78.34547    1\n\n\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE       ACF1\nTraining set 66.26655 330.1867 169.7616 2.867066 18.06051    1 -0.2730483\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.350993e-14 323.4687 166.4792 -9.624318 20.73922 0.9806647\n                   ACF1\nTraining set -0.2730483\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending."
  },
  {
    "objectID": "ARModels.html#gdp-deflator-sarima-analysis",
    "href": "ARModels.html#gdp-deflator-sarima-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "GDP Deflator SARIMA Analysis",
    "text": "GDP Deflator SARIMA Analysis\n\n\n\n\nBefore Covid Period\n\n\nCode\ngdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)\ngdp_df\n\n\n   Qtr1 Qtr2 Qtr3 Qtr4\n1   5.8  6.9 10.2  3.2\n2   3.6  7.6  1.2 -2.1\n3  -3.9 -1.8  0.0 -0.6\n4   1.3  9.0  7.7 15.3\n5   2.7  0.2  4.7 -0.2\n6   0.5  4.6  1.1  0.1\n7   0.8  1.7  0.7  1.3\n8   0.4  0.5  1.1  1.9\n9   1.7  2.8  4.0  4.1\n10  2.4  5.1  1.6  5.6\n11  2.8  2.4  0.3  4.4\n12  1.2  2.5  1.9  0.9\n13  0.6  1.5  1.6  1.6\n14  1.0  1.4  1.2  0.9\n15  0.9  1.0  1.3  2.1\n16  0.7  0.8  0.8  1.8\n17  0.7  0.5  3.3  1.3\n18  0.9  1.6  1.8  2.0\n19  1.8  1.6  2.8  2.6\n20  3.3  3.9  3.4  1.7\n21  2.1  3.9  4.5  4.5\n22  4.3  4.0  5.8  4.2\n23  5.2  5.7  5.3  5.7\n24  5.7  3.3  5.4  6.2\n25  5.4  4.1  3.4  6.2\n26  2.5  3.9  5.2  4.7\n27  6.3  8.0  8.2  7.8\n28  9.8 12.2 12.3  9.4\n29  6.1  7.3  6.9  4.3\n30  4.1  5.3  7.4  6.6\n31  5.8  5.0  8.9  6.0\n32  7.9  7.0  8.4  7.5\n33 10.2  9.0  7.6  8.7\n34  9.9  9.2 10.8 11.0\n35  8.2  7.7  7.1  5.6\n36  5.3  5.8  4.2  3.1\n37  3.0  4.3  3.1  4.1\n38  3.5  3.6  3.0  4.0\n39  2.6  2.4  2.3  2.0\n40  1.5  1.7  2.2  2.6\n41  2.8  3.1  3.2  3.2\n42  4.0  4.9  3.5  4.2\n43  4.3  3.0  2.9  4.4\n44  4.6  3.5  3.0  4.0\n45  3.0  3.2  2.4  1.5\n46  2.4  2.0  2.8  2.3\n47  2.4  2.4  2.2  1.9\n48  1.9  2.3  2.2  2.2\n49  1.9  2.0  1.9  1.9\n50  1.7  1.3  2.2  2.4\n51  0.8  1.7  1.3  0.6\n52  0.9  1.7  1.1  1.3\n53  1.5  1.4  2.2  2.7\n54  2.5  2.4  2.2  2.6\n55  2.4  1.6  1.3  1.3\n56  1.4  1.9  2.3  2.0\n57  1.4  2.3  2.5  2.9\n58  3.3  2.6  3.1  3.2\n59  2.9  3.7  3.3  2.8\n60  3.6  2.8  1.5  3.9\n61  2.7  2.1  1.7  1.4\n62  2.0  3.1  1.0 -0.2\n63 -0.7  0.4  1.3  1.1\n64  2.0  1.2  2.4  2.1\n65  2.7  2.5  0.5  2.4\n66  1.6  2.1  2.0  1.6\n67  1.1  1.9  2.4  1.7\n68  2.3  1.8  0.7 -0.1\n69  2.2  1.2  0.0 -0.3\n70  2.9  1.1  2.1  2.1\n71  1.3  2.0  2.8  2.5\n72  3.5  1.4  1.8  1.6\n73  2.2  1.3  1.5  1.6\n74 -1.5  3.5  2.6     \n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of GDP Deflator\",\n       x = \"Date\", \n       y = \"GDP Deflator\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCheck Decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(gdp_df, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n\n\n\nCheck Lag Plot\n\n\nCode\ngglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))\n\n\n\n\n\n\n\nSeasonal Difference AND ACF & PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.\n\n\nCode\nts_plot <- autoplot(gdp_df) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot :\nThe sharp drop after lag 1 and some lags across the range. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1. PACF Plot:\nThe sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. Order of Differencing:\nYou mentioned that you applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1, d = 1, q = 1,2 Seasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4. Therefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].\nWE can continue analysis\n\n\nModel Diagnostics\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n1353.815\n1357.485\n1353.829\n\n\n0\n1\n0\n0\n1\n1\n1168.495\n1175.834\n1168.536\n\n\n0\n1\n0\n1\n1\n0\n1257.004\n1264.344\n1257.046\n\n\n0\n1\n0\n1\n1\n1\n1168.185\n1179.194\n1168.269\n\n\n0\n1\n1\n0\n1\n0\n1321.946\n1329.286\n1321.988\n\n\n0\n1\n1\n0\n1\n1\n1129.920\n1140.930\n1130.004\n\n\n0\n1\n1\n1\n1\n0\n1232.655\n1243.665\n1232.739\n\n\n0\n1\n1\n1\n1\n1\n1129.375\n1144.054\n1129.515\n\n\n0\n1\n2\n0\n1\n0\n1288.080\n1299.090\n1288.164\n\n\n0\n1\n2\n0\n1\n1\n1131.369\n1146.049\n1131.510\n\n\n0\n1\n2\n1\n1\n0\n1233.896\n1248.575\n1234.036\n\n\n1\n1\n0\n0\n1\n0\n1325.569\n1332.909\n1325.611\n\n\n1\n1\n0\n0\n1\n1\n1138.391\n1149.401\n1138.475\n\n\n1\n1\n0\n1\n1\n0\n1237.785\n1248.794\n1237.868\n\n\n1\n1\n0\n1\n1\n1\n1138.442\n1153.122\n1138.583\n\n\n1\n1\n1\n0\n1\n0\n1275.876\n1286.886\n1275.960\n\n\n1\n1\n1\n0\n1\n1\n1127.620\n1142.300\n1127.760\n\n\n1\n1\n1\n1\n1\n0\n1204.062\n1218.741\n1204.202\n\n\n1\n1\n2\n0\n1\n0\n1271.665\n1286.345\n1271.805\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCompare the results\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q     AIC     BIC     AICc\n6 0 1 1 0 1 1 1129.92 1140.93 1130.004\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:80], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.5197  -0.8775  -0.9535\ns.e.  0.0810   0.0497   0.0279\n\nsigma^2 estimated as 2.671:  log likelihood = -559.81,  aic = 1127.62\n\n$degrees_of_freedom\n[1] 287\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.5197 0.0810   6.4164       0\nma1   -0.8775 0.0497 -17.6718       0\nsma1  -0.9535 0.0279 -34.1752       0\n\n$AIC\n[1] 3.888345\n\n$AICc\n[1] 3.888634\n\n$BIC\n[1] 3.938964\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n[1] 288\n\n$ttable\n     Estimate     SE  t.value p.value\nma1   -0.4017 0.0579  -6.9345       0\nsma1  -0.9579 0.0242 -39.6386       0\n\n$AIC\n[1] 3.896277\n\n$AICc\n[1] 3.896421\n\n$BIC\n[1] 3.934241\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant.\n\n\nFit model & Forecasting\n\n\nCode\nfit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,120)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nCode\nsarima.for(gdp_df, 36, 0,1,1,0,1,1,4)\n\n\n\n\n\n$pred\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            2.263270\n75 2.221820 2.403705 2.324961 2.241627\n76 2.200177 2.382062 2.303318 2.219983\n77 2.178533 2.360419 2.281674 2.198340\n78 2.156890 2.338775 2.260031 2.176696\n79 2.135247 2.317132 2.238388 2.155053\n80 2.113603 2.295488 2.216744 2.133410\n81 2.091960 2.273845 2.195101 2.111766\n82 2.070316 2.252202 2.173457 2.090123\n83 2.048673 2.230558 2.151814         \n\n$se\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            1.650936\n75 1.923859 2.162607 2.377500 2.602077\n76 2.798329 2.981682 3.154396 3.340432\n77 3.507987 3.667883 3.821094 3.987789\n78 4.140176 4.287134 4.429218 4.584383\n79 4.727550 4.866489 5.001569 5.149237\n80 5.286381 5.420036 5.550474 5.693022\n81 5.826077 5.956138 6.083420 6.222379\n82 6.352607 6.480195 6.605319 6.741734\n83 6.870006 6.995903 7.119573         \n\n\n\n\nBenchMark Comparsion\n\n\nCode\nautoplot(gdp_df) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\",PI=FALSE) +\n  autolayer(meanf(gdp_df, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_df, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_df, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_df, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf2 <- snaive(gdp_df, h=36) \n\naccuracy(f2)\n\n\n                      ME     RMSE     MAE  MPE MAPE MASE      ACF1\nTraining set -0.06838488 2.442416 1.45945 -Inf  Inf    1 0.4805284\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\nOur model fitting is much better than benchmark methods\n\n\nCross validation\n\nOne Step ahead\n\n\nCode\nn <- length(gdp_df)\nn \n\n\n[1] 295\n\n\n\n\nCode\nk <- 89 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 206\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_df[k+i] #76th observation as the test set\n\n  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n       MAE1      MAE2      MSE1      MSE2\n1 0.8997526 0.8742994 0.9035127 0.8766414\n\n\nWe can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.\n\n\n4 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}\n\n# Compute cross-validated errors for up to 4 steps ahead\ne <- tsCV(gdp_df, forecastfunction = farima1, h = 4)\n \nlength(e) \n\n\n[1] 1180\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:4, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case."
  },
  {
    "objectID": "ARModels.html#before-covid-period",
    "href": "ARModels.html#before-covid-period",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Before Covid Period",
    "text": "Before Covid Period\n\n\nCode\ngdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)\ngdp_df\n\n\n   Qtr1 Qtr2 Qtr3 Qtr4\n1   5.8  6.9 10.2  3.2\n2   3.6  7.6  1.2 -2.1\n3  -3.9 -1.8  0.0 -0.6\n4   1.3  9.0  7.7 15.3\n5   2.7  0.2  4.7 -0.2\n6   0.5  4.6  1.1  0.1\n7   0.8  1.7  0.7  1.3\n8   0.4  0.5  1.1  1.9\n9   1.7  2.8  4.0  4.1\n10  2.4  5.1  1.6  5.6\n11  2.8  2.4  0.3  4.4\n12  1.2  2.5  1.9  0.9\n13  0.6  1.5  1.6  1.6\n14  1.0  1.4  1.2  0.9\n15  0.9  1.0  1.3  2.1\n16  0.7  0.8  0.8  1.8\n17  0.7  0.5  3.3  1.3\n18  0.9  1.6  1.8  2.0\n19  1.8  1.6  2.8  2.6\n20  3.3  3.9  3.4  1.7\n21  2.1  3.9  4.5  4.5\n22  4.3  4.0  5.8  4.2\n23  5.2  5.7  5.3  5.7\n24  5.7  3.3  5.4  6.2\n25  5.4  4.1  3.4  6.2\n26  2.5  3.9  5.2  4.7\n27  6.3  8.0  8.2  7.8\n28  9.8 12.2 12.3  9.4\n29  6.1  7.3  6.9  4.3\n30  4.1  5.3  7.4  6.6\n31  5.8  5.0  8.9  6.0\n32  7.9  7.0  8.4  7.5\n33 10.2  9.0  7.6  8.7\n34  9.9  9.2 10.8 11.0\n35  8.2  7.7  7.1  5.6\n36  5.3  5.8  4.2  3.1\n37  3.0  4.3  3.1  4.1\n38  3.5  3.6  3.0  4.0\n39  2.6  2.4  2.3  2.0\n40  1.5  1.7  2.2  2.6\n41  2.8  3.1  3.2  3.2\n42  4.0  4.9  3.5  4.2\n43  4.3  3.0  2.9  4.4\n44  4.6  3.5  3.0  4.0\n45  3.0  3.2  2.4  1.5\n46  2.4  2.0  2.8  2.3\n47  2.4  2.4  2.2  1.9\n48  1.9  2.3  2.2  2.2\n49  1.9  2.0  1.9  1.9\n50  1.7  1.3  2.2  2.4\n51  0.8  1.7  1.3  0.6\n52  0.9  1.7  1.1  1.3\n53  1.5  1.4  2.2  2.7\n54  2.5  2.4  2.2  2.6\n55  2.4  1.6  1.3  1.3\n56  1.4  1.9  2.3  2.0\n57  1.4  2.3  2.5  2.9\n58  3.3  2.6  3.1  3.2\n59  2.9  3.7  3.3  2.8\n60  3.6  2.8  1.5  3.9\n61  2.7  2.1  1.7  1.4\n62  2.0  3.1  1.0 -0.2\n63 -0.7  0.4  1.3  1.1\n64  2.0  1.2  2.4  2.1\n65  2.7  2.5  0.5  2.4\n66  1.6  2.1  2.0  1.6\n67  1.1  1.9  2.4  1.7\n68  2.3  1.8  0.7 -0.1\n69  2.2  1.2  0.0 -0.3\n70  2.9  1.1  2.1  2.1\n71  1.3  2.0  2.8  2.5\n72  3.5  1.4  1.8  1.6\n73  2.2  1.3  1.5  1.6\n74 -1.5  3.5  2.6     \n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of GDP Deflator\",\n       x = \"Date\", \n       y = \"GDP Deflator\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))"
  },
  {
    "objectID": "ARModels.html#check-decomposition",
    "href": "ARModels.html#check-decomposition",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Check Decomposition",
    "text": "Check Decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(gdp_df, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")"
  },
  {
    "objectID": "ARModels.html#check-lag-plot",
    "href": "ARModels.html#check-lag-plot",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Check Lag Plot",
    "text": "Check Lag Plot\n\n\nCode\ngglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))"
  },
  {
    "objectID": "ARModels.html#seasonal-difference-and-acf-pacf",
    "href": "ARModels.html#seasonal-difference-and-acf-pacf",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Seasonal Difference AND ACF & PACF",
    "text": "Seasonal Difference AND ACF & PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.\n\n\nCode\nts_plot <- autoplot(gdp_df) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot :\nThe sharp drop after lag 1 and some lags across the range. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1. PACF Plot:\nThe sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. Order of Differencing:\nYou mentioned that you applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1, d = 1, q = 1,2 Seasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4. Therefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].\nWE can continue analysis"
  },
  {
    "objectID": "ARModels.html#model-diagnostics-2",
    "href": "ARModels.html#model-diagnostics-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n1353.815\n1357.485\n1353.829\n\n\n0\n1\n0\n0\n1\n1\n1168.495\n1175.834\n1168.536\n\n\n0\n1\n0\n1\n1\n0\n1257.004\n1264.344\n1257.046\n\n\n0\n1\n0\n1\n1\n1\n1168.185\n1179.194\n1168.269\n\n\n0\n1\n1\n0\n1\n0\n1321.946\n1329.286\n1321.988\n\n\n0\n1\n1\n0\n1\n1\n1129.920\n1140.930\n1130.004\n\n\n0\n1\n1\n1\n1\n0\n1232.655\n1243.665\n1232.739\n\n\n0\n1\n1\n1\n1\n1\n1129.375\n1144.054\n1129.515\n\n\n0\n1\n2\n0\n1\n0\n1288.080\n1299.090\n1288.164\n\n\n0\n1\n2\n0\n1\n1\n1131.369\n1146.049\n1131.510\n\n\n0\n1\n2\n1\n1\n0\n1233.896\n1248.575\n1234.036\n\n\n1\n1\n0\n0\n1\n0\n1325.569\n1332.909\n1325.611\n\n\n1\n1\n0\n0\n1\n1\n1138.391\n1149.401\n1138.475\n\n\n1\n1\n0\n1\n1\n0\n1237.785\n1248.794\n1237.868\n\n\n1\n1\n0\n1\n1\n1\n1138.442\n1153.122\n1138.583\n\n\n1\n1\n1\n0\n1\n0\n1275.876\n1286.886\n1275.960\n\n\n1\n1\n1\n0\n1\n1\n1127.620\n1142.300\n1127.760\n\n\n1\n1\n1\n1\n1\n0\n1204.062\n1218.741\n1204.202\n\n\n1\n1\n2\n0\n1\n0\n1271.665\n1286.345\n1271.805\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "ARModels.html#compare",
    "href": "ARModels.html#compare",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Compare",
    "text": "Compare\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q     AIC     BIC     AICc\n6 0 1 1 0 1 1 1129.92 1140.93 1130.004\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:80], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.5197  -0.8775  -0.9535\ns.e.  0.0810   0.0497   0.0279\n\nsigma^2 estimated as 2.671:  log likelihood = -559.81,  aic = 1127.62\n\n$degrees_of_freedom\n[1] 287\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.5197 0.0810   6.4164       0\nma1   -0.8775 0.0497 -17.6718       0\nsma1  -0.9535 0.0279 -34.1752       0\n\n$AIC\n[1] 3.888345\n\n$AICc\n[1] 3.888634\n\n$BIC\n[1] 3.938964\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n[1] 288\n\n$ttable\n     Estimate     SE  t.value p.value\nma1   -0.4017 0.0579  -6.9345       0\nsma1  -0.9579 0.0242 -39.6386       0\n\n$AIC\n[1] 3.896277\n\n$AICc\n[1] 3.896421\n\n$BIC\n[1] 3.934241\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant."
  },
  {
    "objectID": "ARModels.html#fit-model-2",
    "href": "ARModels.html#fit-model-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Fit model",
    "text": "Fit model\n\n\nCode\nfit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,120)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nCode\nsarima.for(gdp_df, 36, 0,1,1,0,1,1,4)\n\n\n\n\n\n$pred\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            2.263270\n75 2.221820 2.403705 2.324961 2.241627\n76 2.200177 2.382062 2.303318 2.219983\n77 2.178533 2.360419 2.281674 2.198340\n78 2.156890 2.338775 2.260031 2.176696\n79 2.135247 2.317132 2.238388 2.155053\n80 2.113603 2.295488 2.216744 2.133410\n81 2.091960 2.273845 2.195101 2.111766\n82 2.070316 2.252202 2.173457 2.090123\n83 2.048673 2.230558 2.151814         \n\n$se\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            1.650936\n75 1.923859 2.162607 2.377500 2.602077\n76 2.798329 2.981682 3.154396 3.340432\n77 3.507987 3.667883 3.821094 3.987789\n78 4.140176 4.287134 4.429218 4.584383\n79 4.727550 4.866489 5.001569 5.149237\n80 5.286381 5.420036 5.550474 5.693022\n81 5.826077 5.956138 6.083420 6.222379\n82 6.352607 6.480195 6.605319 6.741734\n83 6.870006 6.995903 7.119573"
  },
  {
    "objectID": "ARModels.html#benchmark-comparsion",
    "href": "ARModels.html#benchmark-comparsion",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "BenchMark Comparsion",
    "text": "BenchMark Comparsion\n\n\nCode\nautoplot(gdp_df) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\",PI=FALSE) +\n  autolayer(meanf(gdp_df, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_df, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_df, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_df, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf2 <- snaive(gdp_df, h=36) \n\naccuracy(f2)\n\n\n                      ME     RMSE     MAE  MPE MAPE MASE      ACF1\nTraining set -0.06838488 2.442416 1.45945 -Inf  Inf    1 0.4805284\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\nOur model fitting is much better than benchmark methods"
  },
  {
    "objectID": "ARModels.html#cross-validation",
    "href": "ARModels.html#cross-validation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Cross validation",
    "text": "Cross validation\n\nOne Step ahead\n\n\nCode\nn <- length(gdp_df)\nn \n\n\n[1] 295\n\n\n\n\nCode\nk <- 89 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 206\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_df[k+i] #76th observation as the test set\n\n  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n       MAE1      MAE2      MSE1      MSE2\n1 0.8997526 0.8742994 0.9035127 0.8766414\n\n\nWe can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.\n\n\n4 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}\n\n# Compute cross-validated errors for up to 4 steps ahead\ne <- tsCV(gdp_df, forecastfunction = farima1, h = 4)\n \nlength(e) \n\n\n[1] 1180\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:4, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case."
  },
  {
    "objectID": "ARModels.html#real-median-household-income-in-the-united-states-sarima-analsyis",
    "href": "ARModels.html#real-median-household-income-in-the-united-states-sarima-analsyis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Real Median Household Income in the United States SARIMA Analsyis",
    "text": "Real Median Household Income in the United States SARIMA Analsyis\n\n\n\n\nBefore Covid Period\n\n\nCode\nrmhi <- ts(df$MEHOINUSA672N,frequency = 2)\nrmhi\n\n\nTime Series:\nStart = c(1, 1) \nEnd = c(19, 2) \nFrequency = 2 \n [1] 55828 56871 58920 59624 60115 61153 60370 58607 58153 57843 58515 60348\n[13] 61225 62484 64781 66385 66248 64779 64047 63967 63745 64427 64930 65801\n[25] 63455 63011 61364 60428 60313 62425 61468 64631 66657 67571 68168 72808\n[37] 71186 70784\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = MEHOINUSA672N)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Income\",\n       x = \"Date\", \n       y = \"Household Income\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCheck with decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(rmhi, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n\n\n\nCheck Seasonal Pattern using Lag\n\n\nCode\ngglagplot(rmhi, do.lines=FALSE, set.lags = c(2, 4, 8, 12))\n\n\n\n\n\n\n\nSeasonal Difference and ACF, PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 2 relatively.\n\n\nCode\nts_plot <- autoplot(rmhi) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot (Autocorrelation Function Plot):\nThe sharp drop after lag 2. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 2,5, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 1, which implies Q = 1,2. PACF Plot (Partial Autocorrelation Function Plot):\nThe sharp drop after lag 2 in the PACF plot indicates a possible AR(1) process. This gives us p = 1,2. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. However, the dataset did not have a strong seasonal pattern. We can also consider P to be 0 Order of Differencing:\nWe applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1,2, d = 1, q = 1,2 Seasonal parameters: P = 0,1 D = 1, Q = 1, and the seasonal period (or frequency) is 2. Therefore, the ARIMA model can be represented as ARIMA(2,1,1)(0,1,1)[2].\nWE can continue analysis\n\n\nModel Diagnostic\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=2\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*23),nrow=23)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCompare Results\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=rmhi)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n629.7060\n631.2613\n629.8272\n\n\n0\n1\n0\n0\n1\n1\n618.0458\n621.1565\n618.4208\n\n\n0\n1\n0\n1\n1\n0\n626.3264\n629.4371\n626.7014\n\n\n0\n1\n0\n1\n1\n1\n619.6015\n624.2676\n620.3757\n\n\n0\n1\n1\n0\n1\n0\n622.7853\n625.8960\n623.1603\n\n\n0\n1\n1\n0\n1\n1\n615.4804\n620.1464\n616.2546\n\n\n0\n1\n1\n1\n1\n0\n618.5424\n623.2084\n619.3166\n\n\n0\n1\n1\n1\n1\n1\n617.1456\n623.3670\n618.4789\n\n\n0\n1\n2\n0\n1\n0\n620.1165\n624.7825\n620.8907\n\n\n0\n1\n2\n0\n1\n1\n617.2042\n623.4256\n618.5375\n\n\n0\n1\n2\n1\n1\n0\n619.5969\n625.8183\n620.9302\n\n\n1\n1\n0\n0\n1\n0\n631.0309\n634.1416\n631.4059\n\n\n1\n1\n0\n0\n1\n1\n616.4825\n621.1486\n617.2567\n\n\n1\n1\n0\n1\n1\n0\n623.7348\n628.4008\n624.5090\n\n\n1\n1\n0\n1\n1\n1\n617.5213\n623.7427\n618.8547\n\n\n1\n1\n1\n0\n1\n0\n622.7940\n627.4601\n623.5682\n\n\n1\n1\n1\n0\n1\n1\n617.2456\n623.4670\n618.5790\n\n\n1\n1\n1\n1\n1\n0\n619.8546\n626.0760\n621.1880\n\n\n1\n1\n2\n0\n1\n0\n617.1006\n623.3219\n618.4339\n\n\n2\n1\n0\n0\n1\n0\n627.3197\n631.9857\n628.0938\n\n\n2\n1\n0\n0\n1\n1\n618.3728\n624.5942\n619.7062\n\n\n2\n1\n0\n1\n1\n0\n624.6816\n630.9030\n626.0149\n\n\n2\n1\n1\n0\n1\n0\n616.9212\n623.1426\n618.2545\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(rmhi, 2,1,1,0,1,1,2))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(rmhi, 0,1,1,0,1,1,2))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:67], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n[1] 31\n\n$ttable\n     Estimate     SE t.value p.value\nar1   -0.1325 0.2471 -0.5362  0.5956\nar2   -0.1839 0.3213 -0.5723  0.5712\nma1    0.6255 0.2393  2.6142  0.0137\nsma1  -0.5330 0.3657 -1.4576  0.1550\n\n$AIC\n[1] 17.68248\n\n$AICc\n[1] 17.72058\n\n$BIC\n[1] 17.90468\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n\n$ttable\n     Estimate     SE t.value p.value\nma1    0.4932 0.2136  2.3085  0.0274\nsma1  -0.6924 0.1661 -4.1678  0.0002\n\n$AIC\n[1] 17.58515\n\n$AICc\n[1] 17.59587\n\n$BIC\n[1] 17.71847\n\nNA\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant. The second one is better indeed.\n\n\nFit model & Forecast\n\n\nCode\nfit2=arima(rmhi, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\nsummary(fit2)\n\n\n\nCall:\narima(x = rmhi, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 2))\n\nCoefficients:\n         ma1     sma1\n      0.4932  -0.6924\ns.e.  0.2136   0.1661\n\nsigma^2 estimated as 2022906:  log likelihood = -304.74,  aic = 615.48\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -72.4368 1365.071 1031.092 -0.1227066 1.603226 0.8711332\n                    ACF1\nTraining set -0.03840144\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,36)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nBenchMark Comparsion\n\n\nCode\nsarima.for(rmhi, 36, 0,1,1,0,1,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(20, 1) \nEnd = c(37, 2) \nFrequency = 2 \n [1] 69952.81 71400.41 70921.82 72369.41 71890.82 73338.42 72859.82 74307.42\n [9] 73828.83 75276.43 74797.83 76245.43 75766.84 77214.43 76735.84 78183.44\n[17] 77704.84 79152.44 78673.85 80121.44 79642.85 81090.45 80611.86 82059.45\n[25] 81580.86 83028.46 82549.86 83997.46 83518.87 84966.46 84487.87 85935.47\n[33] 85456.87 86904.47 86425.88 87873.47\n\n$se\nTime Series:\nStart = c(20, 1) \nEnd = c(37, 2) \nFrequency = 2 \n [1]  1422.290  2555.981  3618.397  4561.172  5580.054  6550.064  7606.739\n [8]  8633.488  9745.571 10835.708 12007.705 13161.940 14394.301 15611.416\n[15] 16903.132 18181.267 19530.792 20867.921 22273.543 23667.660 25127.664\n[22] 26576.861 28089.590 29592.080 31155.970 32710.092 34323.673 35927.889\n[29] 37589.789 39242.674 40951.612 42651.843 44406.626 46152.975 47952.485\n[36] 49743.807\n\n\n\n\nCode\nautoplot(rmhi) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\") +\n  autolayer(meanf(rmhi, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(rmhi, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(rmhi, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(rmhi, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf1 <- meanf(rmhi, h=36)\nf2 <- snaive(rmhi, h=36) \nf3 <-naive(rmhi, h=36)\naccuracy(f1)\n\n\n                       ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set 1.914539e-12 3930.345 3192.816 -0.3813871 5.052014 1.549179\n                  ACF1\nTraining set 0.8332796\n\n\nCode\naccuracy(f2)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 813.0833 2378.943 2060.972 1.213902 3.223489    1 0.6130337\n\n\nCode\naccuracy(f3)\n\n\n                   ME     RMSE      MAE      MPE     MAPE      MASE      ACF1\nTraining set 404.2162 1503.922 1183.622 0.614466 1.854535 0.5743026 0.1656062\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = rmhi, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 2))\n\nCoefficients:\n         ma1     sma1\n      0.4932  -0.6924\ns.e.  0.2136   0.1661\n\nsigma^2 estimated as 2022906:  log likelihood = -304.74,  aic = 615.48\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -72.4368 1365.071 1031.092 -0.1227066 1.603226 0.8711332\n                    ACF1\nTraining set -0.03840144\n\n\nOur model fitting is better than benchmark methods with smaller RMSE\n\n\nCross validation\n\nOne Step ahead\n\n\nCode\nn <- length(rmhi)\nn * 0.3\n\n\n[1] 11.4\n\n\n\n\nCode\nk <- 12 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 26\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- rmhi[1:(k-1)+i] \n  xtest <- rmhi[k+i] \n  fit <- arima(xtrain, order = c(2,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n     MAE1     MAE2     MSE1     MSE2\n1 1415.04 1494.363 410152.8 801771.5\n\n\nWe can see that the corresponding results for model 2: (2,1,1)(0,1,1) is slightly better. Which is not the same as the conclusion from above, the reason could be that the data points are relatively small. The train and test split can not capture the datasets effectively, which can cause the different conclusion. The model diagnotics from previsou section indicates that the (0,1,1)(0,1,1) is better with smaller errors.\n\n\n2 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) ),h=h)}\n\n# Compute cross-validated errors for up to 2 steps ahead\ne <- tsCV(rmhi, forecastfunction = farima1, h = 2)\n \nlength(e) \n\n\n[1] 76\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:2, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than two step ahead in my case."
  },
  {
    "objectID": "ASV.html#set-up-the-variables",
    "href": "ASV.html#set-up-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Set Up The Variables",
    "text": "Set Up The Variables\n\n\nCode\ndd1<-data.frame(homevalue,saleprice,rentalprice,df8$date)\n\ncolnames(dd1)<-c(\"homevalue\",\"saleprice\",\"rentalprice\",'date')\n\nknitr::kable(head(dd1))\n\n\n\n\n\nhomevalue\nsaleprice\nrentalprice\ndate\n\n\n\n\n381885.7\n394588\n1489.368\n2018-08-31\n\n\n383510.8\n396351\n1495.171\n2018-09-30\n\n\n384594.7\n391015\n1492.019\n2018-10-31\n\n\n385442.7\n385500\n1483.661\n2018-11-30\n\n\n387043.6\n384032\n1482.177\n2018-12-31\n\n\n389061.8\n385450\n1477.878\n2019-01-31\n\n\n\n\n\n\n\nCode\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"homevalue\"=log(dd1$homevalue),\"saleprice\"=log(dd1$saleprice),\n                                        \"rentalprice\"=log(dd1$rentalprice))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model",
    "href": "ASV.html#auto-fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit The Model",
    "text": "Auto Fit The Model\n\n\nCode\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"homevalue\"], xreg = xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"homevalue\"] \nRegression with ARIMA(2,1,2) errors \n\nCoefficients:\n         ar1     ar2     ma1      ma2  saleprice  rentalprice\n      0.0357  0.4180  -0.463  -0.4912     0.8765       0.1160\ns.e.  0.1418  0.0786   0.147   0.1386     0.0422       0.0354\n\nsigma^2 = 0.007652:  log likelihood = 1100.12\nAIC=-2186.23   AICc=-2186.13   BIC=-2151.35\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.001968005 0.08719397 0.02695833 0.01034377 0.2024629 0.3681705\n                    ACF1\nTraining set 0.002220954"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model",
    "href": "ASV.html#manual-fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The Model",
    "text": "Manual Fit The Model\n\n\nCode\nfit.reg1 <- lm( homevalue ~ saleprice+ rentalprice, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = homevalue ~ saleprice + rentalprice, data = lg.dd.ts1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68312 -0.03521 -0.00042  0.03629  0.27205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49700    0.18542   2.680  0.00747 ** \nsaleprice    0.93209    0.01680  55.472  < 2e-16 ***\nrentalprice  0.05678    0.01422   3.993 6.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1184 on 1077 degrees of freedom\nMultiple R-squared:  0.823, Adjusted R-squared:  0.8227 \nF-statistic:  2504 on 2 and 1077 DF,  p-value: < 2.2e-16\n\n\nWe can see that the variables are pretty significant."
  },
  {
    "objectID": "ASV.html#check-the-residuals",
    "href": "ASV.html#check-the-residuals",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check The Residuals",
    "text": "Check The Residuals\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,1,2) errors\nQ* = 8.9982, df = 4, p-value = 0.06114\n\nModel df: 4.   Total lags used: 8\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#check-acf-and-pacf",
    "href": "ASV.html#check-acf-and-pacf",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check ACF and PACF",
    "text": "Check ACF and PACF\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit1)\n\n\n\n\n\nSince there is no major seasonal pattern. We use differencing directly"
  },
  {
    "objectID": "ASV.html#differencing-acf-and-pacf",
    "href": "ASV.html#differencing-acf-and-pacf",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Differencing ACF and PACF",
    "text": "Differencing ACF and PACF\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(res.fit1, differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(res.fit1, differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is mostly stationary for us to continue From ACF and PACF, it seems that we can consider: p = 1,2,3 q = 1,2,3. We use first order of differencing so d = 1. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics",
    "href": "ASV.html#model-diagnotistics",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n-2185.822\n-2155.919\n-2185.743\n\n\n2\n2\n2\n-2163.551\n-2138.637\n-2163.495\n\n\n2\n3\n2\n-2031.719\n-2006.809\n-2031.663\n\n\n2\n1\n3\n-2183.857\n-2148.971\n-2183.753\n\n\n2\n2\n3\n-2168.936\n-2139.039\n-2168.858\n\n\n2\n3\n3\n-2024.416\n-1994.525\n-2024.338\n\n\n2\n1\n4\n-2194.287\n-2154.417\n-2194.153\n\n\n2\n2\n4\n-2166.447\n-2131.567\n-2166.343\n\n\n3\n1\n2\n-2183.841\n-2148.954\n-2183.736\n\n\n3\n2\n2\n-2164.637\n-2134.740\n-2164.558\n\n\n3\n3\n2\n-2075.699\n-2045.808\n-2075.621\n\n\n3\n1\n3\n-2183.029\n-2143.159\n-2182.895\n\n\n3\n2\n3\n-2167.183\n-2132.303\n-2167.078\n\n\n3\n1\n4\n-2192.632\n-2147.778\n-2192.464\n\n\n4\n1\n2\n-2192.881\n-2153.011\n-2192.747\n\n\n4\n2\n2\n-2171.137\n-2136.257\n-2171.032\n\n\n4\n1\n3\n-2190.880\n-2146.026\n-2190.712\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 2 1 4 -2194.287 -2154.417 -2194.153\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -2185.822 -2155.919 -2185.743\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 2 1 4 -2194.287 -2154.417 -2194.153\n\n\nFrom here, we have two potential ones: (2,1,2) and (2,1,4)"
  },
  {
    "objectID": "ASV.html#compare-the-models",
    "href": "ASV.html#compare-the-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare The models",
    "text": "Compare The models\n\n\nCode\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 2,1,4)) \n\n\n\n\n\nCode\nmodel_output12 <- capture.output(sarima(res.fit1, 2,1,2)) \n\n\n\n\n\n\n\nCode\ncat(model_output11[90:123], model_output11[length(model_output11)], sep = \"\\n\")\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ma1     ma2      ma3     ma4  constant\n      0.8239  0.0553  -1.2526  0.1997  -0.1235  0.1795     1e-04\ns.e.  0.1938  0.1511   0.1916  0.2323   0.0649  0.0393     1e-04\n\nsigma^2 estimated as 0.007533:  log likelihood = 1105.14,  aic = -2194.29\n\n$degrees_of_freedom\n[1] 1072\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.8239 0.1938  4.2522  0.0000\nar2        0.0553 0.1511  0.3662  0.7143\nma1       -1.2526 0.1916 -6.5386  0.0000\nma2        0.1997 0.2323  0.8597  0.3901\nma3       -0.1235 0.0649 -1.9028  0.0573\nma4        0.1795 0.0393  4.5726  0.0000\nconstant   0.0001 0.0001  0.6131  0.5399\n\n$AIC\n[1] -2.03363\n\n$AICc\n[1] -2.033534\n\n$BIC\n[1] -1.996679\n\n\nCode\ncat(model_output12[40:70], model_output12[length(model_output12)], sep = \"\\n\")\n\n\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ma1      ma2  constant\n      0.0580  0.4279  -0.4812  -0.4893     1e-04\ns.e.  0.1428  0.0806   0.1486   0.1408     2e-04\n\nsigma^2 estimated as 0.007625:  log likelihood = 1098.91,  aic = -2185.82\n\n$degrees_of_freedom\n[1] 1074\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.0580 0.1428  0.4063  0.6846\nar2        0.4279 0.0806  5.3077  0.0000\nma1       -0.4812 0.1486 -3.2384  0.0012\nma2       -0.4893 0.1408 -3.4748  0.0005\nconstant   0.0001 0.0002  0.6602  0.5093\n\n$AIC\n[1] -2.025785\n\n$AICc\n[1] -2.025733\n\n$BIC\n[1] -1.998071\n\n\nBased on this, I think the second one (2,1,4) is slightly better with less correlation and smaller AIC value."
  },
  {
    "objectID": "ASV.html#cross-validation",
    "href": "ASV.html#cross-validation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit1)\nn *0.3 \n\n\n[1] 324\n\n\n\n\nCode\nk=324\n \nrmse1 <- matrix(NA, 53,4)\nrmse2 <- matrix(NA,53,4)\nrmse3 <- matrix(NA,53,4)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:53)\n{\n  xtrain <- window(res.fit1, end=st + i-1)\n  xtest <- window(res.fit1, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,4),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:4, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\n\n\nCode\ncolMeans( rmse1,na.rm=TRUE)\n\n\n[1] 0.01282484 0.01701977 0.02336799 0.02397791\n\n\nCode\ncolMeans( rmse2,na.rm=TRUE)\n\n\n[1] 0.01368543 0.02028240 0.02719078 0.02635619\n\n\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (2,1,4) performs better with smaller errors."
  },
  {
    "objectID": "ASV.html#fit-the-model",
    "href": "ASV.html#fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model",
    "text": "Fit the model\n\n\nCode\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"homevalue\"],order=c(2,1,4),xreg=xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"homevalue\"] \nRegression with ARIMA(2,1,4) errors \n\nCoefficients:\n         ar1     ar2      ma1     ma2      ma3     ma4  saleprice  rentalprice\n      0.8151  0.0766  -1.2454  0.1814  -0.1233  0.1903     0.8651       0.1058\ns.e.  0.1952  0.1493   0.1934  0.2331   0.0684  0.0400     0.0433       0.0358\n\nsigma^2 = 0.007576:  log likelihood = 1106.15\nAIC=-2194.31   AICc=-2194.14   BIC=-2149.45\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.002244825 0.08667672 0.02677427 0.01221504 0.2010633 0.3656567\n                     ACF1\nTraining set -0.001078051"
  },
  {
    "objectID": "ASV.html#write-down-the-equation",
    "href": "ASV.html#write-down-the-equation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Write Down The equation:",
    "text": "Write Down The equation:\nGiven that y_t represents the log-transformed homevalue at time t, the ARIMA(2,1,4) can be:\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\nBased on the summary, my equation is\n(1 - 0.8151B - 0.0766B²)(1 - B)yₜ = α + 0.8651 * salepriceₜ + 0.1058 * rentalpriceₜ + (1 - 1.2454B + 0.1814B² - 0.1233B³ + 0.1903B⁴)εₜ"
  },
  {
    "objectID": "ASV.html#forecasting",
    "href": "ASV.html#forecasting",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nCode\nspfit<-auto.arima(lg.dd.ts1[, \"saleprice\"]) \nsummary(spfit) \n\n\nSeries: lg.dd.ts1[, \"saleprice\"] \nARIMA(0,1,0) \n\nsigma^2 = 0.004229:  log likelihood = 1417.77\nAIC=-2833.53   AICc=-2833.53   BIC=-2828.55\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.0005143487 0.06500146 0.02430292 0.002720093 0.1819437 0.3527218\n                   ACF1\nTraining set 0.05157596\n\n\n\n\nCode\nfsp<-forecast(spfit,80) #obtaining forecasts\n\nrpfit<-auto.arima(lg.dd.ts1[, \"rentalprice\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n\n\nSeries: lg.dd.ts1[, \"rentalprice\"] \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.1423\ns.e.   0.0308\n\nsigma^2 = 0.006889:  log likelihood = 1155.01\nAIC=-2306.01   AICc=-2306   BIC=-2296.05\n\nTraining set error measures:\n                     ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.00063182 0.08292248 0.02052085 0.001505318 0.2698055 0.3434587\n                    ACF1\nTraining set 0.001944753\n\n\n\n\nCode\nfrp<-forecast(rpfit,80)\n\nfxreg <- cbind(saleprice = fsp$mean, \n              rentalprice = frp$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")\n\n\n\n\n\nBased on the forecast, we can see that the model did capture the seasonal contents where it predict the downfall of the data."
  },
  {
    "objectID": "ASV.html#creating-ts-objects-for-variables",
    "href": "ASV.html#creating-ts-objects-for-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Creating Ts Objects For Variables",
    "text": "Creating Ts Objects For Variables\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nrate <- ts(df6_sub$PSAVERT, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\n\n\nsaving rate, income, saveing can be interesting to be evaluated"
  },
  {
    "objectID": "ASV.html#combine-the-varaibles",
    "href": "ASV.html#combine-the-varaibles",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Varaibles",
    "text": "Combine the Varaibles\n\n\nCode\ndd2<-data.frame(rate,saving,income,df1_sub$DATE)\n\ncolnames(dd2)<-c(\"saving_rate\",\"household_saving\",\"household_income\",'date')\n\nknitr::kable(head(dd2))\n\n\n\n\n\nsaving_rate\nhousehold_saving\nhousehold_income\ndate\n\n\n\n\n9.4\n431.693\n58153\n1992-01-01\n\n\n9.8\n377.899\n57843\n1993-01-01\n\n\n9.7\n345.198\n58515\n1994-01-01\n\n\n9.9\n366.774\n60348\n1995-01-01\n\n\n9.9\n353.639\n61225\n1996-01-01\n\n\n10.1\n339.566\n62484\n1997-01-01"
  },
  {
    "objectID": "ASV.html#make-a-log-transformation-for-seasonal-pattern",
    "href": "ASV.html#make-a-log-transformation-for-seasonal-pattern",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Make a Log transformation For Seasonal Pattern",
    "text": "Make a Log transformation For Seasonal Pattern\n\n\nCode\nlg.dd2 <- data.frame(\"date\" =dd2$date,\"saving_rate\"=log(dd2$saving_rate),\"household_saving\"=log(dd2$household_saving),\n                                        \"household_income\"=log(dd2$household_income))\n\n#### converting to time series component #########\nlg.dd.ts2<-ts(lg.dd2,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts2[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model-1",
    "href": "ASV.html#auto-fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit The Model",
    "text": "Auto Fit The Model\n\n\nCode\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\nfit2 <- auto.arima(lg.dd.ts2[, \"saving_rate\"], xreg = xreg2)\nsummary(fit2)\n\n\nSeries: lg.dd.ts2[, \"saving_rate\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      household_income  household_saving\n                0.3057           -0.1981\ns.e.            0.0191            0.0328\n\nsigma^2 = 0.01216:  log likelihood = 23.83\nAIC=-41.66   AICc=-40.7   BIC=-37.56\n\nTraining set error measures:\n                       ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set 0.0001058585 0.1063899 0.08808777 -0.2509742 4.206073 0.7528751\n                  ACF1\nTraining set 0.1430342"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model-1",
    "href": "ASV.html#manual-fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The model",
    "text": "Manual Fit The model\n\n\nCode\nfit.reg2 <- lm( saving_rate ~  household_saving + household_income, data=lg.dd.ts2)\nsummary(fit.reg2)\n\n\n\nCall:\nlm(formula = saving_rate ~ household_saving + household_income, \n    data = lg.dd.ts2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20674 -0.06359 -0.01653  0.06697  0.23007 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.24855    4.53045   1.159    0.257    \nhousehold_saving -0.18141    0.03676  -4.934    4e-05 ***\nhousehold_income -0.17843    0.41833  -0.427    0.673    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1096 on 26 degrees of freedom\nMultiple R-squared:  0.5556,    Adjusted R-squared:  0.5214 \nF-statistic: 16.25 on 2 and 26 DF,  p-value: 2.637e-05\n\n\nIt seems that the household income did not play any important role. Therefore, We can consider to remove it."
  },
  {
    "objectID": "ASV.html#manual-fit-again-without-insignificant-variable",
    "href": "ASV.html#manual-fit-again-without-insignificant-variable",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit Again Without Insignificant Variable",
    "text": "Manual Fit Again Without Insignificant Variable\n\n\nCode\nfit.reg2 <- lm( saving_rate ~  household_saving, data=lg.dd.ts2)\nsummary(fit.reg2)\n\n\n\nCall:\nlm(formula = saving_rate ~ household_saving, data = lg.dd.ts2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20039 -0.07323 -0.01942  0.07709  0.22147 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.3183     0.2097  15.828 3.48e-15 ***\nhousehold_saving  -0.1882     0.0326  -5.774 3.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1079 on 27 degrees of freedom\nMultiple R-squared:  0.5525,    Adjusted R-squared:  0.5359 \nF-statistic: 33.33 on 1 and 27 DF,  p-value: 3.85e-06\n\n\nWe can see that the variables are pretty significant."
  },
  {
    "objectID": "ASV.html#check-residuals",
    "href": "ASV.html#check-residuals",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check residuals",
    "text": "Check residuals\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 5.2247, df = 6, p-value = 0.5153\n\nModel df: 0.   Total lags used: 6\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#acf-and-pacf-check",
    "href": "ASV.html#acf-and-pacf-check",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "ACF and PACF Check",
    "text": "ACF and PACF Check\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit2<-ts(residuals(fit.reg2),frequency = 2)\n\n############## Then look at the residuals ############\nggAcf(res.fit2)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit2)\n\n\n\n\n\nSince there is no major seasonal pattern. And it seems that the data is already stationary"
  },
  {
    "objectID": "ASV.html#no-need-for-seasonal-differencing",
    "href": "ASV.html#no-need-for-seasonal-differencing",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "No Need For Seasonal Differencing",
    "text": "No Need For Seasonal Differencing\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit2) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit2) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is mostly stationary for us to continue From ACF and PACF, it seems that we can consider: p = 1,2 q = 1,2. We use first order of differencing so d = 0. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics-1",
    "href": "ASV.html#model-diagnotistics-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit2,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n-35.62908\n-27.635848\n-31.629075\n\n\n2\n2\n2\n-30.44483\n-23.965644\n-27.587686\n\n\n2\n3\n2\n-14.32464\n-8.034157\n-11.324640\n\n\n2\n1\n3\n-34.33747\n-25.012041\n-28.737473\n\n\n2\n2\n3\n-27.95423\n-20.179204\n-23.754225\n\n\n2\n3\n3\n-11.59444\n-4.045857\n-7.173384\n\n\n2\n1\n4\n-32.35833\n-21.700692\n-24.779380\n\n\n2\n2\n4\n-27.40495\n-18.334089\n-21.510210\n\n\n3\n1\n2\n-34.29408\n-24.968649\n-28.694081\n\n\n3\n2\n2\n-29.01339\n-21.238370\n-24.813392\n\n\n3\n3\n2\n-13.51554\n-5.966961\n-9.094488\n\n\n3\n1\n3\n-32.86641\n-22.208777\n-25.287466\n\n\n3\n2\n3\n-27.18329\n-18.112427\n-21.288549\n\n\n3\n1\n4\n-33.13762\n-21.147775\n-23.137616\n\n\n4\n1\n2\n-34.88942\n-24.231784\n-27.310473\n\n\n4\n2\n2\n-27.67647\n-18.605616\n-21.781737\n\n\n4\n1\n3\n-33.00327\n-21.013431\n-23.003272\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\nFrom here, we have two potential ones: (2,1,2) and (0,0,0) from part 1"
  },
  {
    "objectID": "ASV.html#compare-the-models-1",
    "href": "ASV.html#compare-the-models-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare The Models",
    "text": "Compare The Models\n\n\nCode\nset.seed(236)\n\nmodel_output21 <- capture.output(sarima(res.fit2, 2,1,2)) \n\n\n\n\n\nCode\nmodel_output22 <- capture.output(sarima(res.fit2, 0,0,0)) \n\n\n\n\n\n\n\nCode\ncat(model_output21[145:160], model_output21[length(model_output21)], sep = \"\\n\")\n\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.5752 0.3322 -1.7319  0.0967\nar2       -0.1902 0.2359 -0.8064  0.4283\nma1       -0.2555 0.2865 -0.8917  0.3818\nma2       -0.7445 0.2726 -2.7308  0.0119\nconstant  -0.0032 0.0022 -1.4586  0.1582\n\n$AIC\n[1] -1.272467\n\n$AICc\n[1] -1.175064\n\n$BIC\n[1] -0.9869946\n\n\n\n\nCode\ncat(model_output22[20:38], model_output22[length(model_output22)], sep = \"\\n\")\n\n\n\nsigma^2 estimated as 0.01084:  log likelihood = 24.46,  aic = -44.92\n\n$degrees_of_freedom\n[1] 28\n\n$ttable\n      Estimate     SE t.value p.value\nxmean        0 0.0193       0       1\n\n$AIC\n[1] -1.548841\n\n$AICc\n[1] -1.543732\n\n$BIC\n[1] -1.454545\n\n\nBased on this, I think the first one (2,1,2) is slightly better with less correlation and closer to the significant level."
  },
  {
    "objectID": "ASV.html#cross-validation-1",
    "href": "ASV.html#cross-validation-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit2)\nn *0.3 \n\n\n[1] 8.7\n\n\n\n\nCode\nk=9\n \nrmse1 <- matrix(NA, 40,2)\nrmse2 <- matrix(NA,40,2)\nrmse3 <- matrix(NA,40,2)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:12)\n{\n  xtrain <- window(res.fit2, end=st + i-1)\n  xtest <- window(res.fit2, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:2, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:2, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\nHere, although\n\n\nCode\ncolMeans( rmse1,na.rm=TRUE)\n\n\n[1] 0.09762763 0.10927137\n\n\nCode\ncolMeans( rmse2,na.rm=TRUE)\n\n\n[1] 0.09827109 0.07812204\n\n\nBased on the cross validation, overall, the two method is simiarly within expectation. We can see that although model (0,0,0) has smaller errors after, the dataset is relatively biased due to the size of the date overlapping. We should still use the fit1 which is (2,1,2) to perform later analysis."
  },
  {
    "objectID": "ASV.html#fit-the-model-with-the-best-one",
    "href": "ASV.html#fit-the-model-with-the-best-one",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model With The Best One",
    "text": "Fit the model With The Best One\n\n\nCode\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\n\nfit2 <- Arima(lg.dd.ts2[, \"saving_rate\"],order=c(2,1,2),xreg=xreg2)\nsummary(fit2)\n\n\nSeries: lg.dd.ts2[, \"saving_rate\"] \nRegression with ARIMA(2,1,2) errors \n\nCoefficients:\n         ar1      ar2      ma1     ma2  household_income  household_saving\n      0.4885  -0.4602  -1.2187  0.5136            0.1188           -0.1330\ns.e.  0.3344   0.2419   0.3265  0.3180            0.3668            0.0648\n\nsigma^2 = 0.0124:  log likelihood = 24.41\nAIC=-34.81   AICc=-29.21   BIC=-25.48\n\nTraining set error measures:\n                      ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set -0.01566954 0.0969955 0.06932111 -0.8864638 3.345804 0.5924788\n                    ACF1\nTraining set -0.07898355"
  },
  {
    "objectID": "ASV.html#write-doen-the-equation",
    "href": "ASV.html#write-doen-the-equation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Write Doen The equation:",
    "text": "Write Doen The equation:\nGiven that y_t represents the log-transformed homevalue at time t, the ARIMA(2,1,4) can be:\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\nBased on the summary, my equation for this model is\n(1 - 0.4885B - (-0.4602)B^2)(1 - B)y_t = + 0.1188 _t - 0.1330 _t + (1 - 1.2187B + 0.5136B^2)_t"
  },
  {
    "objectID": "ASV.html#forecasting-1",
    "href": "ASV.html#forecasting-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nCode\nhsfit<-auto.arima(lg.dd.ts2[, \"household_saving\"]) \nsummary(hsfit) \n\n\nSeries: lg.dd.ts2[, \"household_saving\"] \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0701\ns.e.  0.0447\n\nsigma^2 = 0.05808:  log likelihood = 0.62\nAIC=2.76   AICc=3.24   BIC=5.42\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.0002068136 0.2325379 0.1666254 -0.1667371 2.642544 0.4511904\n                    ACF1\nTraining set -0.04495977\n\n\n\n\nCode\nfhs<-forecast(hsfit,12) #obtaining forecasts\n\nhifit<-auto.arima(lg.dd.ts2[, \"household_income\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n\n\nSeries: lg.dd.ts1[, \"rentalprice\"] \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.1423\ns.e.   0.0308\n\nsigma^2 = 0.006889:  log likelihood = 1155.01\nAIC=-2306.01   AICc=-2306   BIC=-2296.05\n\nTraining set error measures:\n                     ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.00063182 0.08292248 0.02052085 0.001505318 0.2698055 0.3434587\n                    ACF1\nTraining set 0.001944753\n\n\n\n\nCode\nfhi<-forecast(hifit,12)\n\nfxreg <- cbind(household_income = fhi$mean,household_saving = fhs$mean\n              ) #fimp$mean gives the forecasted values\n\nfcast <- forecast(fit2, xreg=fxreg,12) \nautoplot(fcast, main=\"Forecast of Saving Rate\") + xlab(\"Year\") +\n  ylab(\"Saving Rate\")\n\n\n\n\n\nBased on the forecast, we can see that the model did capture the seasonal patterns where it predict the fluctuations and downfall of the data in the future."
  },
  {
    "objectID": "ASV.html#transform-all-to-time-series",
    "href": "ASV.html#transform-all-to-time-series",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Transform all to time series",
    "text": "Transform all to time series\n\n\nCode\nsaving =ts(df1$W398RC1A027NBEA)\nincome =ts(df2$MEHOINUSA672N)\nsale = ts(df3$MSPUS)\ngini =ts(df4$SIPOVGINIUSA)\nafford =ts(df5$FIXHAI)\nsaverate =ts(df6$PSAVERT)\ngdp =ts(df7$A191RI1Q225SBEA)\n\nsaleprice =ts(df8$Mean.Sale.Price)\nhomevalue =ts(df8$Mean.Home.Value)\nrentalprice =ts(df8$mean)"
  },
  {
    "objectID": "ASV.html#prepare-variables-saving-rate-sale-price-gdp-deflator",
    "href": "ASV.html#prepare-variables-saving-rate-sale-price-gdp-deflator",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Prepare Variables: saving rate, sale price, gdp deflator",
    "text": "Prepare Variables: saving rate, sale price, gdp deflator\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1965-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 5\n\n# Create ts objects\nsale_price <- ts(df3_sub$MSPUS, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nsaving_rate <- ts(df6_sub$PSAVERT, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(1965, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-variables",
    "href": "ASV.html#combine-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine Variables",
    "text": "Combine Variables\n\n\nCode\ndd3<-data.frame(sale_price,saving_rate,gdp,DATE)\n\ncolnames(dd3)<-c(\"sale_price\",\"saving_rate\",\"gdp_deflator\",'date')\n\nknitr::kable(head(dd3))\n\n\n\n\n\nsale_price\nsaving_rate\ngdp_deflator\ndate\n\n\n\n\n20200\n12.1\n2.0\n-1826\n\n\n19800\n10.7\n1.8\n-1736\n\n\n20200\n10.8\n1.6\n-1645\n\n\n20300\n10.2\n2.8\n-1553\n\n\n21000\n11.2\n2.6\n-1461\n\n\n22100\n12.1\n3.3\n-1371"
  },
  {
    "objectID": "ASV.html#plot-the-variables-together",
    "href": "ASV.html#plot-the-variables-together",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Plot the Variables Together",
    "text": "Plot the Variables Together\n\n\nCode\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale_price\"=log(dd3$sale_price),\"saving_rate\"=log(dd3$saving_rate),\n                                        \"gdp_deflator\"=log(dd3$gdp_deflator))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 5)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate in USA\")"
  },
  {
    "objectID": "ASV.html#finding-out-the-best-p",
    "href": "ASV.html#finding-out-the-best-p",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Finding out the best p:",
    "text": "Finding out the best p:\n\n\nCode\nVARselect(dd3[, c(2:4)], lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     3      1      1      3 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.398087e+01 1.398082e+01 1.391871e+01 1.393094e+01 1.395240e+01\nHQ(n)  1.406205e+01 1.411071e+01 1.409731e+01 1.415825e+01 1.422842e+01\nSC(n)  1.418295e+01 1.430414e+01 1.436328e+01 1.449676e+01 1.463946e+01\nFPE(n) 1.179834e+06 1.179822e+06 1.108866e+06 1.122687e+06 1.147305e+06\n                  6            7            8            9           10\nAIC(n) 1.398304e+01 1.403364e+01 1.406570e+01 1.408240e+01 1.412592e+01\nHQ(n)  1.430777e+01 1.440708e+01 1.448785e+01 1.455325e+01 1.464549e+01\nSC(n)  1.479134e+01 1.496320e+01 1.511650e+01 1.525444e+01 1.541922e+01\nFPE(n) 1.183385e+06 1.245368e+06 1.286686e+06 1.309306e+06 1.368790e+06\n\n\nFrom the results, we can see that 3,4 have relatively smaller criterias: We can fit several models with p=1, 3, and 4.=> VAR(1), VAR(3), VAR(4)"
  },
  {
    "objectID": "ASV.html#fitting-a-var-model-with-different-p",
    "href": "ASV.html#fitting-a-var-model-with-different-p",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting a VAR model with different p:",
    "text": "Fitting a VAR model with different p:\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 275 \nLog Likelihood: -3075.874 \nRoots of the characteristic polynomial:\n0.9816 0.8407 0.7771\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 1, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   8.296e-01  3.447e-02  24.070  < 2e-16 ***\ngdp_deflator.l1 -1.619e-02  2.129e-02  -0.761  0.44760    \ndate.l1          9.867e-06  9.057e-06   1.089  0.27692    \nconst            2.241e+00  4.772e-01   4.696 4.23e-06 ***\ntrend           -2.320e-03  7.247e-04  -3.202  0.00153 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7166 on 270 degrees of freedom\nMultiple R-Squared: 0.8099, Adjusted R-squared: 0.807 \nF-statistic: 287.5 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1  -8.763e-02  5.740e-02  -1.527  0.12803    \ngdp_deflator.l1  8.022e-01  3.545e-02  22.625  < 2e-16 ***\ndate.l1         -4.714e-05  1.508e-05  -3.126  0.00197 ** \nconst            2.191e+00  7.946e-01   2.757  0.00623 ** \ntrend           -7.055e-04  1.207e-03  -0.585  0.55934    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.193 on 270 degrees of freedom\nMultiple R-Squared: 0.7827, Adjusted R-squared: 0.7795 \nF-statistic: 243.1 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1    90.19249   59.02459   1.528    0.128    \ngdp_deflator.l1  -16.61672   36.45826  -0.456    0.649    \ndate.l1            0.96759    0.01551  62.390   <2e-16 ***\nconst           -811.20104  817.15553  -0.993    0.322    \ntrend              0.63544    1.24101   0.512    0.609    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1227 on 270 degrees of freedom\nMultiple R-Squared: 0.961,  Adjusted R-squared: 0.9605 \nF-statistic:  1665 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate     0.513560    -0.007928 -8.194e+00\ngdp_deflator   -0.007928     1.424116  3.316e+01\ndate           -8.193763    33.159659  1.506e+06\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator      date\nsaving_rate     1.000000    -0.009271 -0.009317\ngdp_deflator   -0.009271     1.000000  0.022643\ndate           -0.009317     0.022643  1.000000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 273 \nLog Likelihood: -3025.413 \nRoots of the characteristic polynomial:\n0.9806 0.9118 0.8704 0.4113 0.4113 0.3397 0.3397 0.2947 0.2947\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 3, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   6.770e-01  6.060e-02  11.172  < 2e-16 ***\ngdp_deflator.l1 -1.671e-02  3.692e-02  -0.452 0.651324    \ndate.l1         -4.871e-06  3.464e-05  -0.141 0.888274    \nsaving_rate.l2   9.846e-02  7.264e-02   1.356 0.176419    \ngdp_deflator.l2  8.798e-02  4.378e-02   2.010 0.045496 *  \ndate.l2         -2.431e-05  4.825e-05  -0.504 0.614747    \nsaving_rate.l3   9.630e-02  6.066e-02   1.588 0.113581    \ngdp_deflator.l3 -9.371e-02  3.629e-02  -2.582 0.010364 *  \ndate.l3          3.352e-05  3.475e-05   0.965 0.335646    \nconst            1.786e+00  5.123e-01   3.487 0.000573 ***\ntrend           -2.034e-03  7.360e-04  -2.763 0.006128 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6969 on 262 degrees of freedom\nMultiple R-Squared: 0.8253, Adjusted R-squared: 0.8187 \nF-statistic: 123.8 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1  -2.610e-03  1.015e-01  -0.026  0.97949    \ngdp_deflator.l1  6.218e-01  6.182e-02  10.059  < 2e-16 ***\ndate.l1          2.358e-05  5.799e-05   0.407  0.68463    \nsaving_rate.l2  -1.154e-01  1.216e-01  -0.949  0.34333    \ngdp_deflator.l2  4.379e-02  7.330e-02   0.597  0.55073    \ndate.l2         -7.619e-06  8.078e-05  -0.094  0.92493    \nsaving_rate.l3   6.686e-03  1.016e-01   0.066  0.94756    \ngdp_deflator.l3  1.687e-01  6.076e-02   2.776  0.00591 ** \ndate.l3         -5.886e-05  5.817e-05  -1.012  0.31256    \nconst            2.347e+00  8.578e-01   2.736  0.00663 ** \ntrend           -8.907e-04  1.232e-03  -0.723  0.47040    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.167 on 262 degrees of freedom\nMultiple R-Squared: 0.7974, Adjusted R-squared: 0.7897 \nF-statistic: 103.1 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   5.357e+01  1.048e+02   0.511  0.60968    \ngdp_deflator.l1 -7.718e+00  6.386e+01  -0.121  0.90389    \ndate.l1          9.861e-01  5.990e-02  16.464  < 2e-16 ***\nsaving_rate.l2  -5.835e+01  1.256e+02  -0.464  0.64269    \ngdp_deflator.l2 -2.332e+02  7.571e+01  -3.080  0.00229 ** \ndate.l2         -7.233e-03  8.344e-02  -0.087  0.93099    \nsaving_rate.l3   1.095e+02  1.049e+02   1.044  0.29752    \ngdp_deflator.l3  2.502e+02  6.276e+01   3.987 8.69e-05 ***\ndate.l3         -8.317e-03  6.009e-02  -0.138  0.89003    \nconst           -1.124e+03  8.860e+02  -1.269  0.20574    \ntrend            8.568e-01  1.273e+00   0.673  0.50141    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1205 on 262 degrees of freedom\nMultiple R-Squared: 0.963,  Adjusted R-squared: 0.9616 \nF-statistic: 681.9 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate      0.48565      0.01418      20.31\ngdp_deflator     0.01418      1.36133     -14.92\ndate            20.30792    -14.91718 1452449.04\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator     date\nsaving_rate      1.00000      0.01744  0.02418\ngdp_deflator     0.01744      1.00000 -0.01061\ndate             0.02418     -0.01061  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=4, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 272 \nLog Likelihood: -3007.166 \nRoots of the characteristic polynomial:\n0.9868 0.9304 0.8686 0.5835 0.5835 0.5395 0.4834 0.4834 0.416 0.416 0.1904 0.1904\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 4, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   6.421e-01  6.088e-02  10.546  < 2e-16 ***\ngdp_deflator.l1 -1.304e-02  3.695e-02  -0.353  0.72434    \ndate.l1          1.551e-06  3.519e-05   0.044  0.96487    \nsaving_rate.l2   8.610e-02  7.251e-02   1.187  0.23615    \ngdp_deflator.l2  9.791e-02  4.326e-02   2.263  0.02444 *  \ndate.l2         -3.474e-05  4.867e-05  -0.714  0.47604    \nsaving_rate.l3  -4.977e-03  7.196e-02  -0.069  0.94491    \ngdp_deflator.l3 -5.673e-02  4.425e-02  -1.282  0.20102    \ndate.l3          2.894e-05  4.753e-05   0.609  0.54317    \nsaving_rate.l4   1.658e-01  6.035e-02   2.747  0.00645 ** \ngdp_deflator.l4 -5.754e-02  3.783e-02  -1.521  0.12943    \ndate.l4          5.146e-06  3.435e-05   0.150  0.88102    \nconst            1.631e+00  5.249e-01   3.108  0.00210 ** \ntrend           -1.970e-03  7.385e-04  -2.668  0.00811 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6861 on 258 degrees of freedom\nMultiple R-Squared: 0.833,  Adjusted R-squared: 0.8245 \nF-statistic: 98.96 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   1.883e-02  1.041e-01   0.181  0.85656    \ngdp_deflator.l1  6.149e-01  6.315e-02   9.738  < 2e-16 ***\ndate.l1          1.349e-05  6.014e-05   0.224  0.82265    \nsaving_rate.l2  -1.195e-01  1.239e-01  -0.964  0.33594    \ngdp_deflator.l2  3.769e-02  7.393e-02   0.510  0.61066    \ndate.l2          5.842e-06  8.318e-05   0.070  0.94406    \nsaving_rate.l3   5.427e-02  1.230e-01   0.441  0.65940    \ngdp_deflator.l3  1.298e-01  7.563e-02   1.716  0.08737 .  \ndate.l3         -6.911e-05  8.124e-05  -0.851  0.39568    \nsaving_rate.l4  -7.248e-02  1.031e-01  -0.703  0.48287    \ngdp_deflator.l4  5.796e-02  6.465e-02   0.896  0.37084    \ndate.l4          8.775e-06  5.870e-05   0.149  0.88128    \nconst            2.407e+00  8.971e-01   2.683  0.00776 ** \ntrend           -9.482e-04  1.262e-03  -0.751  0.45319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.173 on 258 degrees of freedom\nMultiple R-Squared: 0.7984, Adjusted R-squared: 0.7882 \nF-statistic: 78.58 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   3.404e+01  1.071e+02   0.318  0.75097    \ngdp_deflator.l1  8.281e+00  6.502e+01   0.127  0.89876    \ndate.l1          1.012e+00  6.192e-02  16.340  < 2e-16 ***\nsaving_rate.l2  -3.016e+01  1.276e+02  -0.236  0.81334    \ngdp_deflator.l2 -2.322e+02  7.613e+01  -3.050  0.00253 ** \ndate.l2         -3.837e-02  8.566e-02  -0.448  0.65459    \nsaving_rate.l3   1.588e+02  1.266e+02   1.254  0.21100    \ngdp_deflator.l3  3.224e+02  7.788e+01   4.140  4.7e-05 ***\ndate.l3          2.568e-03  8.365e-02   0.031  0.97553    \nsaving_rate.l4  -7.421e+01  1.062e+02  -0.699  0.48533    \ngdp_deflator.l4 -1.040e+02  6.657e+01  -1.562  0.11947    \ndate.l4         -6.418e-03  6.044e-02  -0.106  0.91552    \nconst           -8.460e+02  9.237e+02  -0.916  0.36063    \ntrend            6.347e-01  1.300e+00   0.488  0.62569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1208 on 258 degrees of freedom\nMultiple R-Squared: 0.9632, Adjusted R-squared: 0.9613 \nF-statistic: 518.9 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate      0.47080      0.02528  1.839e+01\ngdp_deflator     0.02528      1.37521 -9.918e+00\ndate            18.38808     -9.91797  1.458e+06\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator      date\nsaving_rate      1.00000     0.031413  0.022193\ngdp_deflator     0.03141     1.000000 -0.007004\ndate             0.02219    -0.007004  1.000000\n\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one."
  },
  {
    "objectID": "ASV.html#cross-validations",
    "href": "ASV.html#cross-validations",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross Validations",
    "text": "Cross Validations\n\nDefine length\n\n\nCode\nn=length(dd3$gdp_deflator)\nk=85 #19*4\n\nn*0.3\n\n\n[1] 82.8\n\n\n\n\nCode\ndat = ts(dd3[,c(1,2,3)])\n\n\n\n\nCross Validation For gdp deflator\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nNormalize the RMSE For Ploting\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare the errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 3.802198\n\n\nCode\nerror2\n\n\n[1] 3.871752\n\n\nThe first one is better but not too different for GDP Deflator\n\n\n\nCross Validation For Saving rate\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$saving_rate-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$saving_rate-xtest)^2))\n\n}\n\n\n\nNormalize the errors for ploting\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare errors\nLet us reorganize the errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 4.48152\n\n\nCode\nerror2\n\n\n[1] 4.607221\n\n\nThe first one is also better for saving rate\n\n\n\nCross Validation For Sale price\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$sale_price-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$sale_price-xtest)^2))\n\n}\n\n\n\nNormalize The Errors\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare The Errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 125445.5\n\n\nCode\nerror2\n\n\n[1] 125987.7\n\n\nOverall, we can see that in my case, all variables have smaller RMSE with VAR(3)"
  },
  {
    "objectID": "ASV.html#forecasting-2",
    "href": "ASV.html#forecasting-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\nForecast using the best one which is VAR(3)\n\n\nCode\nfit1 <- vars::VAR(dat[,1:3], p=3, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n\n\n         fcst    lower     upper       CI\n[1,] 7.894615 5.607798 10.181432 2.286817\n[2,] 7.282398 4.582903  9.981893 2.699495\n[3,] 7.492970 4.611125 10.374815 2.881845\n[4,] 7.372062 4.294439 10.449686 3.077623\n[5,] 7.295611 4.073932 10.517289 3.221678\n[6,] 7.253375 3.915601 10.591150 3.337774\n[7,] 7.186365 3.748134 10.624597 3.438231\n[8,] 7.131866 3.607845 10.655887 3.524021\n\n\n\nCheck Corresponind Forecasting\n\n\nCode\nfcast1$fcst$sale_price\n\n\n         fcst     lower     upper       CI\n[1,] 52039.84  11740.19  92339.49 40299.65\n[2,] 34492.15 -20395.42  89379.73 54887.58\n[3,] 38522.27 -27352.88 104397.41 65875.14\n[4,] 39360.19 -35082.57 113802.95 74442.76\n[5,] 37722.35 -44022.64 119467.33 81744.99\n[6,] 37462.86 -50890.48 125816.19 88353.33\n[7,] 36986.76 -57304.47 131277.99 94291.23\n[8,] 36584.86 -63106.89 136276.61 99691.75\n\n\n\n\nCode\nfcast1$fcst$saving_rate\n\n\n         fcst    lower    upper       CI\n[1,] 8.674407 7.292309 10.05651 1.382098\n[2,] 9.136267 7.444615 10.82792 1.691652\n[3,] 9.041880 7.142789 10.94097 1.899091\n[4,] 9.091204 7.003047 11.17936 2.088156\n[5,] 9.210690 6.970091 11.45129 2.240599\n[6,] 9.265283 6.897064 11.63350 2.368219\n[7,] 9.332344 6.853394 11.81129 2.478950\n[8,] 9.401929 6.826736 11.97712 2.575193"
  },
  {
    "objectID": "ASV.html#forecast-results",
    "href": "ASV.html#forecast-results",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast Results",
    "text": "Forecast Results\n\n\nCode\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")\n\n\n\n\n\nBased on the forecast, we can see that the model has help us forecast for each variable, for gdp deflator, and sale price, and saving rate, due to the model lacks of seasonal patterns, it did not capture the fluctuations very will. However, the forecast did capture the overall trends correctly."
  },
  {
    "objectID": "ASV.html#prepare-the-variables",
    "href": "ASV.html#prepare-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Prepare the Variables",
    "text": "Prepare the Variables\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf4_sub <- subset(df4, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nsale <- ts(df3_sub$MSPUS, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\ngini <- ts(df4_sub$SIPOVGINIUSA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-the-variables",
    "href": "ASV.html#combine-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Variables",
    "text": "Combine the Variables\n\n\nCode\ndd1<-data.frame(gini,saving,income,sale,df1_sub$DATE)\n\ncolnames(dd1)<-c(\"gini\",\"saving\",\"income\",'sale','date')\ndd1$gini <- ts(as.numeric(dd1$gini))\nknitr::kable(head(dd1))\n\n\n\n\n\ngini\nsaving\nincome\nsale\ndate\n\n\n\n\n38.4\n431.693\n58153\n119500\n1992-01-01\n\n\n40.4\n377.899\n57843\n120000\n1993-01-01\n\n\n40.0\n345.198\n58515\n120000\n1994-01-01\n\n\n39.9\n366.774\n60348\n126000\n1995-01-01\n\n\n40.3\n353.639\n61225\n125000\n1996-01-01\n\n\n40.5\n339.566\n62484\n127000\n1997-01-01\n\n\n\n\n\n\n\nCode\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"gini\"=log(dd1$gini),\"saving\"=log(dd1$saving),\n                                        \"income\"=log(dd1$income),\"sale\"=log(dd1$sale))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model-2",
    "href": "ASV.html#auto-fit-the-model-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit the Model",
    "text": "Auto Fit the Model\n\n\nCode\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              income = lg.dd.ts1[, \"income\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"gini\"], xreg = xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"gini\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n       saving   income    sale\n      -0.0304  -0.0279  0.3559\ns.e.   0.0052   0.0516  0.0503\n\nsigma^2 = 0.0001483:  log likelihood = 88.27\nAIC=-168.54   AICc=-166.87   BIC=-163.07\n\nTraining set error measures:\n                     ME       RMSE         MAE           MPE      MAPE\nTraining set 6.9935e-06 0.01153159 0.009483874 -0.0005959199 0.2563307\n                  MASE       ACF1\nTraining set 0.7926458 -0.2180884"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model-2",
    "href": "ASV.html#manual-fit-the-model-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The Model",
    "text": "Manual Fit The Model\n\n\nCode\nfit.reg1 <- lm( gini ~ saving+ income + sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = gini ~ saving + income + sale, data = lg.dd.ts1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.026995 -0.007527  0.001984  0.005942  0.019001 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.660654   0.696434   0.949 0.351892    \nsaving      -0.024639   0.008162  -3.019 0.005773 ** \nincome      -0.039670   0.055946  -0.709 0.484839    \nsale         0.307851   0.073432   4.192 0.000302 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0122 on 25 degrees of freedom\nMultiple R-squared:  0.5148,    Adjusted R-squared:  0.4566 \nF-statistic: 8.842 on 3 and 25 DF,  p-value: 0.0003612\n\n\nWe can see that the sale and saving are pretty significant. For income, it seems that it does not provide too much impact. We can consider to remove it"
  },
  {
    "objectID": "ASV.html#manual-fit-again",
    "href": "ASV.html#manual-fit-again",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit Again",
    "text": "Manual Fit Again\n\n\nCode\nfit.reg1 <- lm( gini ~ saving+ sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = gini ~ saving + sale, data = lg.dd.ts1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.026489 -0.006684  0.002481  0.005509  0.020068 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.551544   0.672696   0.820  0.41973    \nsaving      -0.022918   0.007718  -2.969  0.00634 ** \nsale         0.279022   0.060561   4.607 9.48e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01208 on 26 degrees of freedom\nMultiple R-squared:  0.5051,    Adjusted R-squared:  0.467 \nF-statistic: 13.27 on 2 and 26 DF,  p-value: 0.000107"
  },
  {
    "objectID": "ASV.html#check-residuals-1",
    "href": "ASV.html#check-residuals-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check Residuals",
    "text": "Check Residuals\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 4.509, df = 6, p-value = 0.6081\n\nModel df: 0.   Total lags used: 6\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#check-acf-and-pacf-1",
    "href": "ASV.html#check-acf-and-pacf-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check ACF and PACF",
    "text": "Check ACF and PACF\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit1)\n\n\n\n\n\nSince there is no major seasonal pattern. And the dataset is stationary, We can actually use it as it is"
  },
  {
    "objectID": "ASV.html#no-need-to-differencing",
    "href": "ASV.html#no-need-to-differencing",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "NO Need To Differencing",
    "text": "NO Need To Differencing\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit1) +\n  labs(title = \"ACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit1) +\n  labs(title = \"PACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is stationary for us to continue\nFrom ACF and PACF, although there are not too many spikes and seasonal patterns, it seems that we can still consider: p = 1,2 q = 1,2. We use d = 0. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics-2",
    "href": "ASV.html#model-diagnotistics-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*70),nrow=70) \n\n\nfor (p in 1:5)# p=1,2,3 : 3\n{\n  for(q in 1:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d-1,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-171.1425\n-167.0406\n-170.1825\n\n\n0\n1\n0\n-145.8315\n-143.1671\n-145.3515\n\n\n0\n2\n0\n-120.2833\n-118.9874\n-120.1233\n\n\n0\n0\n1\n-172.1225\n-166.6533\n-170.4559\n\n\n0\n1\n1\n-160.6843\n-156.6877\n-159.6843\n\n\n0\n2\n1\n-136.1663\n-133.5746\n-135.6663\n\n\n0\n0\n2\n-175.6608\n-168.8243\n-173.0521\n\n\n0\n1\n2\n-160.6057\n-155.2769\n-158.8665\n\n\n0\n2\n2\n-146.7495\n-142.8620\n-145.7060\n\n\n0\n0\n3\n-174.5409\n-166.3371\n-170.7227\n\n\n0\n1\n3\n-161.7076\n-155.0466\n-158.9804\n\n\n0\n2\n3\n-145.5431\n-140.3598\n-143.7250\n\n\n0\n0\n4\n-174.8678\n-165.2967\n-169.5345\n\n\n0\n1\n4\n-158.8306\n-150.8374\n-154.8306\n\n\n0\n2\n4\n-140.6901\n-134.2109\n-137.8329\n\n\n1\n0\n0\n-171.0691\n-165.6000\n-169.4025\n\n\n1\n1\n0\n-149.5256\n-145.5290\n-148.5256\n\n\n1\n2\n0\n-125.4846\n-122.8930\n-124.9846\n\n\n1\n0\n1\n-174.9728\n-168.1363\n-172.3641\n\n\n1\n1\n1\n-160.0436\n-154.7148\n-158.3045\n\n\n1\n2\n1\n-138.8040\n-134.9164\n-137.7605\n\n\n1\n0\n2\n-175.2214\n-167.0176\n-171.4032\n\n\n1\n1\n2\n-161.2007\n-154.5397\n-158.4735\n\n\n1\n2\n2\n-145.3671\n-140.1837\n-143.5489\n\n\n1\n0\n3\n-173.4902\n-163.9191\n-168.1568\n\n\n1\n1\n3\n-160.8444\n-152.8512\n-156.8444\n\n\n1\n2\n3\n-144.4574\n-137.9782\n-141.6003\n\n\n1\n0\n4\n-173.1838\n-162.2455\n-165.9838\n\n\n1\n1\n4\n-157.5276\n-148.2022\n-151.9276\n\n\n1\n2\n4\n-144.0399\n-136.2649\n-139.8399\n\n\n2\n0\n0\n-170.7698\n-163.9333\n-168.1611\n\n\n2\n1\n0\n-156.3436\n-151.0148\n-154.6045\n\n\n2\n2\n0\n-135.5731\n-131.6856\n-134.5296\n\n\n2\n0\n1\n-170.4178\n-162.2140\n-166.5996\n\n\n2\n1\n1\n-159.2853\n-152.6243\n-156.5580\n\n\n2\n2\n1\n-144.3626\n-139.1793\n-142.5444\n\n\n2\n0\n2\n-173.5908\n-164.0197\n-168.2575\n\n\n2\n1\n2\n-159.2471\n-151.2539\n-155.2471\n\n\n2\n2\n2\n-142.8931\n-136.4140\n-140.0360\n\n\n2\n0\n3\n-171.6011\n-160.6627\n-164.4011\n\n\n2\n1\n3\n-157.3864\n-148.0609\n-151.7864\n\n\n2\n2\n3\n-144.3482\n-136.5732\n-140.1482\n\n\n2\n0\n4\n-171.8233\n-159.5177\n-162.3496\n\n\n2\n1\n4\n-155.9225\n-145.2648\n-148.3435\n\n\n3\n0\n0\n-169.1140\n-160.9102\n-165.2958\n\n\n3\n1\n0\n-155.1027\n-148.4417\n-152.3754\n\n\n3\n2\n0\n-137.2481\n-132.0647\n-135.4299\n\n\n3\n0\n1\n-168.7809\n-159.2099\n-163.4476\n\n\n3\n1\n1\n-157.8894\n-149.8962\n-153.8894\n\n\n3\n2\n1\n-142.7900\n-136.3108\n-139.9329\n\n\n3\n0\n2\n-171.6359\n-160.6975\n-164.4359\n\n\n3\n1\n2\n-156.0375\n-146.7120\n-150.4375\n\n\n3\n2\n2\n-143.0555\n-135.2804\n-138.8555\n\n\n3\n0\n3\n-169.6948\n-157.3891\n-160.2211\n\n\n3\n1\n3\n-157.8215\n-147.1639\n-150.2426\n\n\n3\n0\n4\n-177.8730\n-164.2001\n-165.6508\n\n\n4\n0\n0\n-167.5350\n-157.9639\n-162.2017\n\n\n4\n1\n0\n-153.4391\n-145.4459\n-149.4391\n\n\n4\n2\n0\n-139.8623\n-133.3832\n-137.0052\n\n\n4\n0\n1\n-173.9016\n-162.9633\n-166.7016\n\n\n4\n1\n1\n-156.1053\n-146.7798\n-150.5053\n\n\n4\n2\n1\n-140.9387\n-133.1636\n-136.7387\n\n\n4\n0\n2\n-179.3839\n-167.0783\n-169.9102\n\n\n4\n1\n2\n-155.4640\n-144.8063\n-147.8850\n\n\n4\n0\n3\n-175.6302\n-161.9573\n-163.4080\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n63 4 0 2 -179.3839 -167.0783 -169.9102\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 0 0 2 -175.6608 -168.8243 -173.0521\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 0 0 2 -175.6608 -168.8243 -173.0521\n\n\nFrom here, we can see that (4,0,2) and (0,0,2) is better. Therefore, we will compare them"
  },
  {
    "objectID": "ASV.html#compare-with-the-two-models",
    "href": "ASV.html#compare-with-the-two-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare with the two models",
    "text": "Compare with the two models\n\n\nCode\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 4,0,2)) \n\n\n\n\n\nCode\nmodel_output12 <- capture.output(sarima(res.fit1, 0,0,2)) \n\n\n\n\n\n\n\nCode\ncat(model_output11[150:173], model_output11[length(model_output11)], sep = \"\\n\")\n\n\nsigma^2 estimated as 8.309e-05:  log likelihood = 92.98,  aic = -169.95\n\n$degrees_of_freedom\n[1] 22\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.8855 0.3372  2.6261  0.0154\nar2    -0.2522 0.3647 -0.6915  0.4965\nar3     0.3589 0.3174  1.1306  0.2704\nar4    -0.4707 0.2172 -2.1672  0.0413\nma1    -1.4055 0.3923 -3.5824  0.0017\nma2     0.4056 0.3622  1.1198  0.2749\nxmean   0.0002 0.0004  0.6298  0.5353\n\n$AIC\n[1] -5.860371\n\n$AICc\n[1] -5.676463\n\n$BIC\n[1] -5.483186\n\n\nCode\ncat(model_output12[40:67], model_output12[length(model_output12)], sep = \"\\n\")\n\n\n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1      ma2  xmean\n      -0.2594  -0.7406  2e-04\ns.e.   0.3312   0.3122  4e-04\n\nsigma^2 estimated as 0.000101:  log likelihood = 90.7,  aic = -173.4\n\n$degrees_of_freedom\n[1] 26\n\n$ttable\n      Estimate     SE t.value p.value\nma1    -0.2594 0.3312 -0.7831  0.4406\nma2    -0.7406 0.3122 -2.3721  0.0254\nxmean   0.0002 0.0004  0.5482  0.5883\n\n$AIC\n[1] -5.979431\n\n$AICc\n[1] -5.946327\n\n$BIC\n[1] -5.790838\n\nNA\n\n\nBased on this, I think the second one (0,0,2) is slightly better with less correlation and smaller AIC value."
  },
  {
    "objectID": "ASV.html#cross-validation-2",
    "href": "ASV.html#cross-validation-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit1)\nn *0.3 \n\n\n[1] 8.7\n\n\nCode\ndat = ts(dd1[,c(1,2,4)])\n\n\n\n\nCode\n# The number of folds for cross-validation\nn_folds <- 5\nhorizon <- 4  # Forecasting horizon\nwindow_size <- length(dat[, 3]) - (n_folds * horizon)\n\n# Initialize an empty list to store forecasts\nforecasts <- list()\nforecasts2 <- list()\n\n# Initialize vectors to store error metrics for each model\nrmse1 <- numeric(n_folds)\nrmse2 <- numeric(n_folds)\n\nfor (i in 1:n_folds) {\n  # Define the training set for this fold\n  train_set <- window(dat[, 3], end = c(window_size + ((i - 1) * horizon)))\n\n  # Fit the ARIMA models on the training set\n  fit <- Arima(train_set, order = c(4, 0, 2), include.drift = TRUE, method = \"ML\")\n  fit2 <- Arima(train_set, order = c(0, 0, 2), include.drift = TRUE, method = \"ML\")\n\n  # Forecast on the horizon\n  fcast <- forecast(fit, h = horizon)\n  fcast2 <- forecast(fit2, h = horizon)\n\n  # Store forecasts\n  forecasts[[i]] <- fcast\n  forecasts2[[i]] <- fcast2\n\n  # Define the test set for this fold\n  test_set <- window(dat[, 3], start = window_size + ((i - 1) * horizon) + 1, end = window_size + (i * horizon))\n\n  # Calculate and store the RMSE for each model\n  rmse1[i] <- sqrt(mean((fcast$mean - test_set)^2, na.rm = TRUE))\n  rmse2[i] <- sqrt(mean((fcast2$mean - test_set)^2, na.rm = TRUE))\n}\n\n# Calculate the average RMSE for each model\nmean_rmse1 <- mean(rmse1)\nmean_rmse2 <- mean(rmse2)\n\n\n\n# Plot RMSE values for both models\nplot(rmse1, type = \"b\", col = \"blue\", ylim = range(c(rmse1, rmse2)), \n     xlab = \"Fold\", ylab = \"RMSE\", pch = 19, \n     main = \"Cross-Validation RMSE for ARIMA Models\")\nlines(rmse2, type = \"b\", col = \"red\", pch = 18)\npoints(rmse2, type = \"b\", col = \"red\", pch = 18)\n\n# Add a legend to the plot\nlegend(\"topright\", legend = c(\"Model 1 (ARIMA(4,0,2))\", \"Model 2 (ARIMA(0,0,2))\"), \n       col = c(\"blue\", \"red\"), pch = c(19, 18), lty = 1)\n\n\n\n\n\n\n\nCode\n# Output \nmean_rmse1\n\n\n[1] 2804.59\n\n\nCode\nmean_rmse2\n\n\n[1] 2507.994\n\n\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (0,0,2) performs better with smaller errors."
  },
  {
    "objectID": "ASV.html#fit-the-model-1",
    "href": "ASV.html#fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model",
    "text": "Fit the model\n\n\nCode\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"gini\"],order=c(0,0,2),xreg=xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"gini\"] \nRegression with ARIMA(0,0,2) errors \n\nCoefficients:\n          ma1      ma2  intercept   saving    sale\n      -0.4973  -0.5027     0.5375  -0.0204  0.2789\ns.e.   0.2694   0.2576     0.1358   0.0007  0.0119\n\nsigma^2 = 0.0001067:  log likelihood = 92.76\nAIC=-173.52   AICc=-169.7   BIC=-165.31\n\nTraining set error measures:\n                        ME       RMSE         MAE         MPE     MAPE\nTraining set -9.655453e-05 0.00939564 0.007461685 -0.00333216 0.201711\n                  MASE        ACF1\nTraining set 0.6236347 -0.06097811\n\n\nThe equation: Given that y_t represents the log-transformed homevalue at time t, the ARIMA(0,0,2) can be:\n( y_t = c + 1 e{t-1} + 2 e{t-2} + _1 _t + _2 _t + e_t )\nBased on the summary, my equation is\n( y_t = 0.5375 - 0.4973 e_{t-1} - 0.5027 e_{t-2} - 0.0204 _t + 0.2789 _t + e_t )"
  },
  {
    "objectID": "ASV.html#forecast",
    "href": "ASV.html#forecast",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast",
    "text": "Forecast\n\n\nCode\nsfit<-auto.arima(lg.dd.ts1[, \"sale\"]) \nsummary(sfit) \n\n\nSeries: lg.dd.ts1[, \"sale\"] \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1   drift\n      -0.6624  0.0097\ns.e.   0.1365  0.0016\n\nsigma^2 = 0.0002195:  log likelihood = 79.3\nAIC=-152.6   AICc=-151.6   BIC=-148.6\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.0003253088 0.01402717 0.01068364 0.002634758 0.0903859 0.2864612\n                   ACF1\nTraining set -0.1025856\n\n\n\n\nCode\nfs<-forecast(sfit,80) #obtaining forecasts\n\ns2fit<-auto.arima(lg.dd.ts1[, \"saving\"]) #fitting an ARIMA model to the Import variable\nsummary(s2fit)\n\n\nSeries: lg.dd.ts1[, \"saving\"] \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0701\ns.e.  0.0447\n\nsigma^2 = 0.05808:  log likelihood = 0.62\nAIC=2.76   AICc=3.24   BIC=5.42\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.0002068136 0.2325379 0.1666254 -0.1667371 2.642544 0.4511904\n                    ACF1\nTraining set -0.04495977\n\n\n\n\nCode\nfs2<-forecast(s2fit,80)\n\nfxreg <- cbind(\n              sale = fs2$mean,saving = fs$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")"
  },
  {
    "objectID": "ASV.html#time-series-transformation",
    "href": "ASV.html#time-series-transformation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Time Series Transformation",
    "text": "Time Series Transformation"
  },
  {
    "objectID": "ASV.html#select-the-variables-with-common-dates",
    "href": "ASV.html#select-the-variables-with-common-dates",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Select the Variables with common Dates",
    "text": "Select the Variables with common Dates\nHome Value ~ Saving Rate + Gdp Deflator + Sale Price\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"2000-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\ndf9_sub <- subset(df9, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 4\n\n# Create ts objects\nsale <- ts(df3_sub$MSPUS, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nsaving <- ts(df6_sub$PSAVERT, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nhomevalue <- ts(df9_sub$USAUCSFRCONDOSMSAMID, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(2000, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-the-variables-1",
    "href": "ASV.html#combine-the-variables-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Variables",
    "text": "Combine the Variables\n\n\nCode\ndd3<-data.frame(homevalue,saving,gdp,sale,DATE)\n\ncolnames(dd3)<-c(\"homevalue\",\"saving\",\"gdp_deflator\", \"sale\",'date')\n\nknitr::kable(head(dd3))\n\n\n\n\n\nhomevalue\nsaving\ngdp_deflator\nsale\ndate\n\n\n\n\n121428.3\n5.0\n2.7\n165300\n10957\n\n\n121642.0\n4.5\n2.5\n163200\n11048\n\n\n121906.9\n4.3\n2.4\n168800\n11139\n\n\n122475.1\n4.8\n2.2\n172900\n11231\n\n\n123129.1\n4.8\n2.6\n169800\n11323\n\n\n123830.3\n4.8\n2.4\n179000\n11413"
  },
  {
    "objectID": "ASV.html#plot-the-figure-together",
    "href": "ASV.html#plot-the-figure-together",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Plot the Figure Together",
    "text": "Plot the Figure Together\n\n\nCode\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale\"=log(dd3$sale),\"saving\"=log(dd3$saving),\"homevalue\"=log(dd3$homevalue),\n                                        \"gdp_deflator\"=log(dd3$gdp))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate, Home value in USA\")"
  },
  {
    "objectID": "ASV.html#fitting-a-var-model",
    "href": "ASV.html#fitting-a-var-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting a VAR model",
    "text": "Fitting a VAR model\nFinding out the best p:\n\n\nCode\nVARselect(dd3[, c(2:5)], lag.max=14, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    14     14      1     14 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.623230e+01 1.585994e+01 1.597680e+01 1.591038e+01 1.530104e+01\nHQ(n)  1.654480e+01 1.638078e+01 1.670598e+01 1.684788e+01 1.644688e+01\nSC(n)  1.702204e+01 1.717618e+01 1.781953e+01 1.827960e+01 1.819676e+01\nFPE(n) 1.123135e+07 7.794514e+06 8.902039e+06 8.575733e+06 4.886908e+06\n                  6            7            8            9           10\nAIC(n) 1.540760e+01 1.514539e+01 1.518739e+01 1.460313e+01 1.459530e+01\nHQ(n)  1.676178e+01 1.670790e+01 1.695824e+01 1.658231e+01 1.678281e+01\nSC(n)  1.882981e+01 1.909409e+01 1.966259e+01 1.960482e+01 2.012348e+01\nFPE(n) 5.834120e+06 4.967706e+06 5.969293e+06 4.042891e+06 5.231765e+06\n                 11           12           13           14\nAIC(n) 1.364742e+01 1.256891e+01 1.182785e+01      9.56155\nHQ(n)  1.604327e+01 1.517310e+01 1.464037e+01     12.58241\nSC(n)  1.970209e+01 1.915008e+01 1.893551e+01     17.19571\nFPE(n) 2.917799e+06 1.647224e+06 1.628958e+06 519454.19470\n\n\nFrom the results, we can see that 13,14 have relatively smaller criterias:\nWe can fit several models with p=13, 14.=> VAR(13), VAR(14)\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:5)], p=13, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving, gdp_deflator, sale, date \nDeterministic variables: both \nSample size: 68 \nLog Likelihood: -567.72 \nRoots of the characteristic polynomial:\n1.035 1.021 1.021 1.008 1.008 1.004 1.004     1     1 0.9997 0.9997 0.9985 0.9985 0.9915 0.9915 0.9857 0.9834 0.9834 0.9832 0.9832 0.9815 0.9815 0.9749 0.9749 0.9734 0.9734 0.9729 0.9729 0.9669 0.9669 0.9637 0.9637 0.9621 0.9621 0.9476 0.9476 0.9337 0.9337 0.9328 0.9328 0.9302 0.9302 0.9299 0.9299 0.909 0.909 0.8471 0.7737 0.695 0.695 0.4355 0.4355\nCall:\nvars::VAR(y = dd3[, c(2:5)], p = 13, type = \"both\")\n\n\nEstimation results for equation saving: \n======================================= \nsaving = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1         2.908e-01  2.872e-01   1.013   0.3285  \ngdp_deflator.l1   1.711e-01  3.511e-01   0.487   0.6336  \nsale.l1           2.098e-05  3.099e-05   0.677   0.5094  \ndate.l1           1.046e+00  6.562e-01   1.594   0.1333  \nsaving.l2        -1.198e-01  2.610e-01  -0.459   0.6533  \ngdp_deflator.l2   1.933e-02  3.535e-01   0.055   0.9572  \nsale.l2          -2.719e-05  2.607e-05  -1.043   0.3146  \ndate.l2          -3.554e-01  6.401e-01  -0.555   0.5875  \nsaving.l3        -1.422e-01  2.480e-01  -0.574   0.5753  \ngdp_deflator.l3   5.727e-02  3.110e-01   0.184   0.8565  \nsale.l3          -4.226e-06  2.668e-05  -0.158   0.8764  \ndate.l3           5.122e-01  6.753e-01   0.758   0.4608  \nsaving.l4        -2.747e-02  2.182e-01  -0.126   0.9016  \ngdp_deflator.l4   6.959e-01  2.947e-01   2.362   0.0332 *\nsale.l4          -3.909e-05  2.674e-05  -1.462   0.1658  \ndate.l4          -8.648e-01  6.756e-01  -1.280   0.2213  \nsaving.l5         1.842e-01  2.431e-01   0.758   0.4612  \ngdp_deflator.l5   5.206e-02  3.233e-01   0.161   0.8744  \nsale.l5           2.488e-05  2.213e-05   1.124   0.2798  \ndate.l5           3.609e-02  6.480e-01   0.056   0.9564  \nsaving.l6         4.881e-02  2.016e-01   0.242   0.8122  \ngdp_deflator.l6   8.258e-02  2.831e-01   0.292   0.7748  \nsale.l6          -2.184e-05  2.360e-05  -0.925   0.3705  \ndate.l6          -2.468e-01  6.530e-01  -0.378   0.7111  \nsaving.l7        -5.310e-01  2.362e-01  -2.248   0.0412 *\ngdp_deflator.l7   5.376e-02  2.009e-01   0.268   0.7929  \nsale.l7           1.010e-05  2.576e-05   0.392   0.7008  \ndate.l7           1.775e+00  7.591e-01   2.339   0.0347 *\nsaving.l8         3.975e-01  3.213e-01   1.237   0.2363  \ngdp_deflator.l8   3.341e-01  1.765e-01   1.893   0.0792 .\nsale.l8          -6.328e-05  4.354e-05  -1.453   0.1682  \ndate.l8          -1.411e-01  7.180e-01  -0.196   0.8471  \nsaving.l9        -1.211e-01  3.235e-01  -0.374   0.7138  \ngdp_deflator.l9   5.550e-02  1.902e-01   0.292   0.7747  \nsale.l9           3.448e-05  3.337e-05   1.033   0.3190  \ndate.l9           4.678e-01  8.289e-01   0.564   0.5815  \nsaving.l10       -2.254e-01  2.921e-01  -0.772   0.4530  \ngdp_deflator.l10 -1.552e-01  1.714e-01  -0.905   0.3805  \nsale.l10         -2.976e-05  3.162e-05  -0.941   0.3624  \ndate.l10          1.580e-01  9.071e-01   0.174   0.8642  \nsaving.l11       -2.268e-02  2.522e-01  -0.090   0.9296  \ngdp_deflator.l11  1.209e-01  1.778e-01   0.680   0.5075  \nsale.l11          9.539e-05  3.521e-05   2.709   0.0169 *\ndate.l11         -8.571e-01  7.812e-01  -1.097   0.2911  \nsaving.l12        3.879e-01  2.473e-01   1.569   0.1390  \ngdp_deflator.l12  1.344e-01  1.553e-01   0.865   0.4014  \nsale.l12         -2.117e-05  4.142e-05  -0.511   0.6172  \ndate.l12          7.070e-01  6.852e-01   1.032   0.3196  \nsaving.l13        4.278e-02  2.301e-01   0.186   0.8552  \ngdp_deflator.l13  2.689e-01  2.413e-01   1.114   0.2838  \nsale.l13         -1.205e-05  3.414e-05  -0.353   0.7293  \ndate.l13         -2.870e-01  6.661e-01  -0.431   0.6731  \nconst            -2.031e+04  3.379e+04  -0.601   0.5573  \ntrend            -1.780e+02  3.017e+02  -0.590   0.5646  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6358 on 14 degrees of freedom\nMultiple R-Squared: 0.9397, Adjusted R-squared: 0.7116 \nF-statistic: 4.119 on 53 and 14 DF,  p-value: 0.002798 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)   \nsaving.l1        -4.611e-01  2.481e-01  -1.859  0.08420 . \ngdp_deflator.l1   4.021e-01  3.032e-01   1.326  0.20603   \nsale.l1           2.180e-05  2.676e-05   0.815  0.42889   \ndate.l1           4.124e-01  5.667e-01   0.728  0.47874   \nsaving.l2         3.869e-01  2.254e-01   1.716  0.10819   \ngdp_deflator.l2   6.262e-02  3.053e-01   0.205  0.84044   \nsale.l2          -1.508e-06  2.251e-05  -0.067  0.94753   \ndate.l2          -7.359e-01  5.528e-01  -1.331  0.20441   \nsaving.l3        -3.211e-02  2.141e-01  -0.150  0.88296   \ngdp_deflator.l3   2.799e-02  2.686e-01   0.104  0.91848   \nsale.l3           4.193e-07  2.305e-05   0.018  0.98574   \ndate.l3          -2.387e-01  5.832e-01  -0.409  0.68851   \nsaving.l4        -4.318e-01  1.885e-01  -2.291  0.03798 * \ngdp_deflator.l4   3.419e-01  2.545e-01   1.344  0.20043   \nsale.l4           1.016e-05  2.309e-05   0.440  0.66679   \ndate.l4          -1.510e-02  5.835e-01  -0.026  0.97972   \nsaving.l5         4.398e-03  2.099e-01   0.021  0.98358   \ngdp_deflator.l5   4.860e-02  2.792e-01   0.174  0.86430   \nsale.l5          -4.313e-05  1.911e-05  -2.257  0.04050 * \ndate.l5          -6.851e-02  5.597e-01  -0.122  0.90431   \nsaving.l6         4.401e-01  1.741e-01   2.527  0.02415 * \ngdp_deflator.l6   6.141e-02  2.445e-01   0.251  0.80532   \nsale.l6           2.741e-05  2.039e-05   1.344  0.20018   \ndate.l6           9.817e-01  5.640e-01   1.741  0.10366   \nsaving.l7        -6.433e-01  2.040e-01  -3.154  0.00704 **\ngdp_deflator.l7  -9.428e-02  1.735e-01  -0.543  0.59539   \nsale.l7          -5.432e-05  2.225e-05  -2.441  0.02852 * \ndate.l7          -5.684e-01  6.556e-01  -0.867  0.40052   \nsaving.l8         5.461e-03  2.775e-01   0.020  0.98458   \ngdp_deflator.l8  -8.684e-02  1.525e-01  -0.570  0.57800   \nsale.l8           4.609e-05  3.760e-05   1.226  0.24054   \ndate.l8           1.083e+00  6.201e-01   1.747  0.10248   \nsaving.l9         3.778e-01  2.794e-01   1.352  0.19781   \ngdp_deflator.l9   1.485e-01  1.642e-01   0.904  0.38123   \nsale.l9          -3.166e-05  2.882e-05  -1.098  0.29056   \ndate.l9          -1.140e+00  7.159e-01  -1.592  0.13369   \nsaving.l10       -7.754e-02  2.522e-01  -0.307  0.76306   \ngdp_deflator.l10  1.545e-01  1.480e-01   1.043  0.31449   \nsale.l10         -2.887e-05  2.730e-05  -1.057  0.30832   \ndate.l10          3.145e-03  7.834e-01   0.004  0.99685   \nsaving.l11       -6.629e-02  2.178e-01  -0.304  0.76536   \ngdp_deflator.l11 -9.043e-02  1.536e-01  -0.589  0.56541   \nsale.l11          3.491e-05  3.041e-05   1.148  0.27019   \ndate.l11          8.235e-01  6.747e-01   1.221  0.24243   \nsaving.l12       -1.859e-01  2.136e-01  -0.871  0.39861   \ngdp_deflator.l12  2.547e-01  1.342e-01   1.898  0.07846 . \nsale.l12          7.403e-06  3.578e-05   0.207  0.83905   \ndate.l12         -9.841e-01  5.918e-01  -1.663  0.11856   \nsaving.l13        2.576e-01  1.987e-01   1.296  0.21588   \ngdp_deflator.l13 -2.652e-01  2.084e-01  -1.272  0.22397   \nsale.l13          1.495e-05  2.948e-05   0.507  0.61990   \ndate.l13          6.931e-01  5.752e-01   1.205  0.24820   \nconst            -2.276e+03  2.918e+04  -0.078  0.93894   \ntrend            -2.257e+01  2.606e+02  -0.087  0.93221   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5491 on 14 degrees of freedom\nMultiple R-Squared: 0.9365, Adjusted R-squared: 0.696 \nF-statistic: 3.895 on 53 and 14 DF,  p-value: 0.003752 \n\n\nEstimation results for equation sale: \n===================================== \nsale = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -4.084e+03  2.979e+03  -1.371   0.1920  \ngdp_deflator.l1  -9.744e+02  3.642e+03  -0.268   0.7929  \nsale.l1           7.623e-01  3.214e-01   2.372   0.0326 *\ndate.l1           2.490e+03  6.806e+03   0.366   0.7199  \nsaving.l2        -4.804e+02  2.707e+03  -0.177   0.8617  \ngdp_deflator.l2   4.768e+03  3.666e+03   1.300   0.2145  \nsale.l2           5.903e-02  2.703e-01   0.218   0.8303  \ndate.l2          -6.124e+02  6.639e+03  -0.092   0.9278  \nsaving.l3         1.587e+03  2.572e+03   0.617   0.5471  \ngdp_deflator.l3  -5.637e+03  3.226e+03  -1.747   0.1024  \nsale.l3           2.499e-01  2.768e-01   0.903   0.3818  \ndate.l3          -7.087e+02  7.005e+03  -0.101   0.9208  \nsaving.l4        -2.482e+03  2.264e+03  -1.097   0.2913  \ngdp_deflator.l4   3.322e+03  3.056e+03   1.087   0.2955  \nsale.l4          -3.578e-01  2.773e-01  -1.290   0.2179  \ndate.l4           1.388e+04  7.007e+03   1.981   0.0676 .\nsaving.l5        -5.417e+02  2.521e+03  -0.215   0.8330  \ngdp_deflator.l5   1.615e+03  3.353e+03   0.482   0.6376  \nsale.l5          -1.824e-01  2.295e-01  -0.795   0.4400  \ndate.l5          -9.008e+03  6.721e+03  -1.340   0.2015  \nsaving.l6         1.384e+03  2.091e+03   0.662   0.5188  \ngdp_deflator.l6   7.061e+02  2.936e+03   0.240   0.8134  \nsale.l6          -7.230e-02  2.448e-01  -0.295   0.7721  \ndate.l6           9.888e+02  6.773e+03   0.146   0.8860  \nsaving.l7        -2.816e+03  2.450e+03  -1.149   0.2696  \ngdp_deflator.l7  -8.100e+02  2.084e+03  -0.389   0.7033  \nsale.l7           3.045e-01  2.672e-01   1.139   0.2737  \ndate.l7           3.220e+03  7.873e+03   0.409   0.6887  \nsaving.l8        -2.854e+03  3.332e+03  -0.856   0.4062  \ngdp_deflator.l8  -9.438e+02  1.831e+03  -0.515   0.6143  \nsale.l8          -9.160e-02  4.516e-01  -0.203   0.8422  \ndate.l8          -4.669e+03  7.447e+03  -0.627   0.5408  \nsaving.l9         4.690e+03  3.356e+03   1.398   0.1840  \ngdp_deflator.l9   5.201e+03  1.973e+03   2.637   0.0195 *\nsale.l9           1.949e-01  3.461e-01   0.563   0.5823  \ndate.l9           1.035e+04  8.598e+03   1.204   0.2486  \nsaving.l10       -1.195e+03  3.029e+03  -0.394   0.6993  \ngdp_deflator.l10 -6.464e+02  1.778e+03  -0.364   0.7216  \nsale.l10         -6.476e-01  3.279e-01  -1.975   0.0683 .\ndate.l10         -4.637e+03  9.408e+03  -0.493   0.6298  \nsaving.l11       -4.289e+03  2.616e+03  -1.639   0.1234  \ngdp_deflator.l11 -1.225e+03  1.845e+03  -0.664   0.5173  \nsale.l11          5.896e-01  3.652e-01   1.615   0.1287  \ndate.l11          7.608e+03  8.103e+03   0.939   0.3637  \nsaving.l12        1.989e+03  2.565e+03   0.776   0.4509  \ngdp_deflator.l12 -9.021e+02  1.611e+03  -0.560   0.5844  \nsale.l12         -1.698e-02  4.296e-01  -0.040   0.9690  \ndate.l12         -1.176e+04  7.107e+03  -1.654   0.1203  \nsaving.l13        1.331e+03  2.387e+03   0.558   0.5859  \ngdp_deflator.l13  2.295e+03  2.503e+03   0.917   0.3747  \nsale.l13         -2.559e-01  3.541e-01  -0.723   0.4817  \ndate.l13          9.671e+03  6.909e+03   1.400   0.1833  \nconst            -1.721e+08  3.504e+08  -0.491   0.6310  \ntrend            -1.535e+06  3.130e+06  -0.490   0.6314  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6595 on 14 degrees of freedom\nMultiple R-Squared: 0.9948, Adjusted R-squared: 0.9751 \nF-statistic: 50.47 on 53 and 14 DF,  p-value: 3.505e-10 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)    \nsaving.l1         9.351e-02  8.226e-02   1.137  0.27471    \ngdp_deflator.l1   1.711e-02  1.005e-01   0.170  0.86728    \nsale.l1          -1.142e-05  8.874e-06  -1.287  0.21899    \ndate.l1           1.523e-02  1.879e-01   0.081  0.93656    \nsaving.l2        -9.768e-02  7.475e-02  -1.307  0.21237    \ngdp_deflator.l2  -1.439e-01  1.012e-01  -1.422  0.17696    \nsale.l2           1.466e-05  7.464e-06   1.964  0.06976 .  \ndate.l2          -4.554e-01  1.833e-01  -2.485  0.02624 *  \nsaving.l3        -5.419e-02  7.100e-02  -0.763  0.45800    \ngdp_deflator.l3   1.575e-01  8.905e-02   1.769  0.09868 .  \nsale.l3           4.743e-06  7.641e-06   0.621  0.54474    \ndate.l3          -2.674e-01  1.934e-01  -1.383  0.18847    \nsaving.l4         1.874e-01  6.249e-02   2.998  0.00959 ** \ngdp_deflator.l4  -1.150e-01  8.438e-02  -1.363  0.19454    \nsale.l4          -5.368e-06  7.656e-06  -0.701  0.49469    \ndate.l4          -2.522e-01  1.935e-01  -1.304  0.21338    \nsaving.l5        -9.269e-02  6.961e-02  -1.332  0.20428    \ngdp_deflator.l5   3.426e-02  9.257e-02   0.370  0.71686    \nsale.l5          -1.229e-06  6.336e-06  -0.194  0.84902    \ndate.l5          -1.950e-01  1.856e-01  -1.051  0.31127    \nsaving.l6        -4.992e-03  5.774e-02  -0.086  0.93233    \ngdp_deflator.l6   3.084e-02  8.107e-02   0.380  0.70932    \nsale.l6          -3.542e-06  6.760e-06  -0.524  0.60848    \ndate.l6          -6.598e-01  1.870e-01  -3.528  0.00334 ** \nsaving.l7        -1.778e-02  6.763e-02  -0.263  0.79650    \ngdp_deflator.l7  -1.476e-02  5.753e-02  -0.257  0.80119    \nsale.l7           5.976e-07  7.378e-06   0.081  0.93659    \ndate.l7          -2.933e-01  2.174e-01  -1.349  0.19861    \nsaving.l8         5.116e-02  9.201e-02   0.556  0.58692    \ngdp_deflator.l8  -7.410e-02  5.055e-02  -1.466  0.16480    \nsale.l8           1.489e-05  1.247e-05   1.194  0.25226    \ndate.l8           4.650e-03  2.056e-01   0.023  0.98228    \nsaving.l9        -1.112e-01  9.265e-02  -1.200  0.25004    \ngdp_deflator.l9  -6.423e-02  5.446e-02  -1.179  0.25794    \nsale.l9          -6.880e-06  9.556e-06  -0.720  0.48339    \ndate.l9          -6.280e-01  2.374e-01  -2.646  0.01920 *  \nsaving.l10        1.487e-01  8.364e-02   1.777  0.09724 .  \ngdp_deflator.l10  1.083e-01  4.909e-02   2.206  0.04455 *  \nsale.l10         -1.814e-06  9.053e-06  -0.200  0.84405    \ndate.l10          2.086e-02  2.598e-01   0.080  0.93714    \nsaving.l11        1.941e-02  7.223e-02   0.269  0.79208    \ngdp_deflator.l11  4.475e-02  5.093e-02   0.879  0.39444    \nsale.l11         -1.522e-05  1.008e-05  -1.509  0.15343    \ndate.l11         -8.110e-01  2.237e-01  -3.625  0.00276 ** \nsaving.l12       -1.298e-01  7.081e-02  -1.833  0.08822 .  \ngdp_deflator.l12 -9.417e-02  4.448e-02  -2.117  0.05267 .  \nsale.l12          1.914e-05  1.186e-05   1.614  0.12886    \ndate.l12         -3.757e-02  1.962e-01  -0.191  0.85092    \nsaving.l13        6.755e-02  6.589e-02   1.025  0.32267    \ngdp_deflator.l13 -3.158e-02  6.910e-02  -0.457  0.65469    \nsale.l13         -5.844e-06  9.775e-06  -0.598  0.55952    \ndate.l13         -6.054e-01  1.907e-01  -3.174  0.00676 ** \nconst             5.316e+04  9.675e+03   5.494 7.90e-05 ***\ntrend             4.716e+02  8.641e+01   5.457 8.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1821 on 14 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.243e+08 on 53 and 14 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                 saving gdp_deflator       sale       date\nsaving          0.40430     -0.02814     -128.6    0.03227\ngdp_deflator   -0.02814      0.30155     1527.7    0.02139\nsale         -128.63360   1527.68477 43494825.8 -140.74083\ndate            0.03227      0.02139     -140.7    0.03315\n\nCorrelation matrix of residuals:\n               saving gdp_deflator     sale    date\nsaving        1.00000     -0.08061 -0.03067  0.2787\ngdp_deflator -0.08061      1.00000  0.42182  0.2139\nsale         -0.03067      0.42182  1.00000 -0.1172\ndate          0.27873      0.21394 -0.11720  1.0000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:5)], p=14, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving, gdp_deflator, sale, date \nDeterministic variables: both \nSample size: 67 \nLog Likelihood: -468.587 \nRoots of the characteristic polynomial:\n1.016 1.016 1.009 1.007 1.007 1.005 1.005 1.001 1.001     1     1 0.9992 0.9992 0.9966 0.9966 0.9939 0.9939 0.9908 0.9908 0.9863 0.9863 0.9859 0.9859 0.9855 0.9855 0.9851 0.9851 0.9789 0.9762 0.9762 0.9755 0.9755 0.9669 0.9669 0.9593 0.9593 0.9561 0.9561 0.9555 0.9555 0.9545 0.9545 0.9493 0.9489 0.9489 0.9425 0.9425 0.9253 0.9253 0.9249 0.9249 0.8538 0.8383 0.8383 0.7951 0.5564\nCall:\nvars::VAR(y = dd3[, c(2:5)], p = 14, type = \"both\")\n\n\nEstimation results for equation saving: \n======================================= \nsaving = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1         1.463e-01  2.654e-01   0.551   0.5949  \ngdp_deflator.l1   4.820e-01  3.451e-01   1.397   0.1960  \nsale.l1           1.220e-05  3.068e-05   0.398   0.7001  \ndate.l1           7.393e-01  9.500e-01   0.778   0.4564  \nsaving.l2        -9.046e-02  3.074e-01  -0.294   0.7752  \ngdp_deflator.l2  -5.264e-01  3.864e-01  -1.362   0.2062  \nsale.l2           8.937e-06  3.454e-05   0.259   0.8017  \ndate.l2          -2.666e-01  6.697e-01  -0.398   0.6999  \nsaving.l3        -5.992e-01  3.002e-01  -1.996   0.0771 .\ngdp_deflator.l3   2.768e-01  3.652e-01   0.758   0.4678  \nsale.l3          -1.071e-05  2.917e-05  -0.367   0.7218  \ndate.l3           6.780e-01  7.349e-01   0.923   0.3803  \nsaving.l4         1.815e-01  2.328e-01   0.780   0.4557  \ngdp_deflator.l4   5.535e-01  3.862e-01   1.433   0.1855  \nsale.l4          -3.200e-05  2.680e-05  -1.194   0.2631  \ndate.l4          -1.277e+00  6.798e-01  -1.878   0.0931 .\nsaving.l5         1.830e-01  3.068e-01   0.597   0.5655  \ngdp_deflator.l5   3.652e-01  3.758e-01   0.972   0.3566  \nsale.l5          -1.319e-05  2.858e-05  -0.461   0.6555  \ndate.l5          -3.563e-01  7.888e-01  -0.452   0.6622  \nsaving.l6        -1.334e-01  2.455e-01  -0.543   0.6000  \ngdp_deflator.l6   3.820e-01  3.002e-01   1.273   0.2351  \nsale.l6           2.371e-07  2.365e-05   0.010   0.9922  \ndate.l6          -8.171e-01  6.998e-01  -1.168   0.2729  \nsaving.l7        -6.584e-01  2.305e-01  -2.856   0.0189 *\ngdp_deflator.l7   2.827e-01  2.916e-01   0.969   0.3578  \nsale.l7          -1.994e-05  2.543e-05  -0.784   0.4531  \ndate.l7           1.395e+00  9.481e-01   1.472   0.1752  \nsaving.l8         7.202e-01  3.210e-01   2.244   0.0515 .\ngdp_deflator.l8   3.057e-01  1.972e-01   1.550   0.1556  \nsale.l8          -3.316e-05  4.185e-05  -0.792   0.4486  \ndate.l8           5.727e-01  8.882e-01   0.645   0.5352  \nsaving.l9        -2.317e-01  3.587e-01  -0.646   0.5344  \ngdp_deflator.l9   2.613e-01  2.002e-01   1.305   0.2242  \nsale.l9          -5.261e-05  4.653e-05  -1.131   0.2874  \ndate.l9          -1.794e-01  8.084e-01  -0.222   0.8293  \nsaving.l10       -2.787e-01  3.470e-01  -0.803   0.4425  \ngdp_deflator.l10 -1.458e-01  2.135e-01  -0.683   0.5117  \nsale.l10          6.185e-06  3.344e-05   0.185   0.8574  \ndate.l10          1.604e+00  1.151e+00   1.394   0.1968  \nsaving.l11       -2.069e-02  3.097e-01  -0.067   0.9482  \ngdp_deflator.l11  9.249e-03  1.897e-01   0.049   0.9622  \nsale.l11          7.680e-05  3.803e-05   2.020   0.0742 .\ndate.l11         -1.994e+00  8.221e-01  -2.425   0.0383 *\nsaving.l12        3.251e-01  2.492e-01   1.305   0.2244  \ngdp_deflator.l12  2.376e-01  1.676e-01   1.418   0.1900  \nsale.l12          2.661e-06  4.714e-05   0.056   0.9562  \ndate.l12          4.892e-01  1.058e+00   0.463   0.6547  \nsaving.l13        2.713e-01  2.801e-01   0.969   0.3581  \ngdp_deflator.l13  1.783e-01  2.297e-01   0.776   0.4575  \nsale.l13         -1.524e-05  4.259e-05  -0.358   0.7287  \ndate.l13          9.146e-02  7.736e-01   0.118   0.9085  \nsaving.l14       -2.111e-01  2.352e-01  -0.897   0.3929  \ngdp_deflator.l14  5.930e-01  2.528e-01   2.346   0.0436 *\nsale.l14          2.412e-05  3.334e-05   0.723   0.4878  \ndate.l14         -6.787e-01  8.703e-01  -0.780   0.4555  \nconst            -4.824e+02  5.852e+04  -0.008   0.9936  \ntrend            -8.263e-02  5.245e+02   0.000   0.9999  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5664 on 9 degrees of freedom\nMultiple R-Squared: 0.9692, Adjusted R-squared: 0.7742 \nF-statistic: 4.971 on 57 and 9 DF,  p-value: 0.00691 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -5.310e-01  2.657e-01  -1.998   0.0768 .\ngdp_deflator.l1   5.984e-01  3.456e-01   1.731   0.1174  \nsale.l1           2.347e-06  3.072e-05   0.076   0.9408  \ndate.l1           7.860e-03  9.513e-01   0.008   0.9936  \nsaving.l2         2.042e-01  3.078e-01   0.663   0.5238  \ngdp_deflator.l2  -2.973e-01  3.870e-01  -0.768   0.4621  \nsale.l2           2.167e-05  3.459e-05   0.627   0.5465  \ndate.l2          -2.858e-01  6.707e-01  -0.426   0.6800  \nsaving.l3        -2.568e-01  3.007e-01  -0.854   0.4152  \ngdp_deflator.l3   2.612e-01  3.657e-01   0.714   0.4932  \nsale.l3           1.394e-05  2.921e-05   0.477   0.6444  \ndate.l3          -2.505e-02  7.359e-01  -0.034   0.9736  \nsaving.l4        -2.762e-01  2.331e-01  -1.185   0.2665  \ngdp_deflator.l4   2.561e-01  3.867e-01   0.662   0.5244  \nsale.l4           4.054e-08  2.684e-05   0.002   0.9988  \ndate.l4          -2.489e-01  6.808e-01  -0.366   0.7231  \nsaving.l5         3.847e-02  3.072e-01   0.125   0.9031  \ngdp_deflator.l5   1.682e-01  3.764e-01   0.447   0.6655  \nsale.l5          -7.211e-05  2.862e-05  -2.520   0.0328 *\ndate.l5           1.852e-01  7.899e-01   0.234   0.8199  \nsaving.l6         1.954e-01  2.458e-01   0.795   0.4471  \ngdp_deflator.l6   3.326e-01  3.006e-01   1.107   0.2972  \nsale.l6           3.402e-05  2.369e-05   1.436   0.1847  \ndate.l6           2.362e-01  7.008e-01   0.337   0.7438  \nsaving.l7        -5.999e-01  2.309e-01  -2.598   0.0288 *\ngdp_deflator.l7   1.030e-01  2.920e-01   0.353   0.7325  \nsale.l7          -6.390e-05  2.547e-05  -2.509   0.0334 *\ndate.l7          -8.538e-01  9.494e-01  -0.899   0.3919  \nsaving.l8         1.751e-01  3.214e-01   0.545   0.5991  \ngdp_deflator.l8  -1.805e-01  1.975e-01  -0.914   0.3846  \nsale.l8           7.564e-05  4.191e-05   1.805   0.1046  \ndate.l8           1.581e+00  8.895e-01   1.777   0.1093  \nsaving.l9         6.421e-02  3.592e-01   0.179   0.8621  \ngdp_deflator.l9   2.489e-01  2.004e-01   1.242   0.2457  \nsale.l9          -7.093e-05  4.660e-05  -1.522   0.1623  \ndate.l9          -1.109e+00  8.095e-01  -1.370   0.2038  \nsaving.l10        2.516e-02  3.475e-01   0.072   0.9439  \ngdp_deflator.l10  3.055e-01  2.138e-01   1.429   0.1868  \nsale.l10         -2.801e-05  3.349e-05  -0.836   0.4246  \ndate.l10          6.123e-01  1.152e+00   0.531   0.6081  \nsaving.l11       -2.733e-02  3.101e-01  -0.088   0.9317  \ngdp_deflator.l11 -8.317e-02  1.900e-01  -0.438   0.6719  \nsale.l11          1.825e-05  3.808e-05   0.479   0.6432  \ndate.l11          5.851e-01  8.233e-01   0.711   0.4953  \nsaving.l12       -3.538e-01  2.495e-01  -1.418   0.1899  \ngdp_deflator.l12  1.778e-01  1.678e-01   1.059   0.3171  \nsale.l12          1.953e-05  4.720e-05   0.414   0.6888  \ndate.l12         -1.210e+00  1.059e+00  -1.142   0.2828  \nsaving.l13        2.470e-01  2.805e-01   0.880   0.4015  \ngdp_deflator.l13 -3.622e-01  2.300e-01  -1.575   0.1497  \nsale.l13          4.599e-05  4.265e-05   1.078   0.3090  \ndate.l13          6.696e-01  7.747e-01   0.864   0.4099  \nsaving.l14        2.484e-01  2.356e-01   1.054   0.3192  \ngdp_deflator.l14  4.051e-01  2.531e-01   1.600   0.1440  \nsale.l14         -1.083e-05  3.339e-05  -0.324   0.7531  \ndate.l14         -9.017e-02  8.716e-01  -0.103   0.9199  \nconst            -3.195e+02  5.860e+04  -0.005   0.9958  \ntrend            -4.929e+00  5.253e+02  -0.009   0.9927  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5672 on 9 degrees of freedom\nMultiple R-Squared: 0.9563, Adjusted R-squared: 0.6792 \nF-statistic: 3.452 on 57 and 9 DF,  p-value: 0.02509 \n\n\nEstimation results for equation sale: \n===================================== \nsale = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -5.784e+03  2.978e+03  -1.942   0.0840 .\ngdp_deflator.l1   1.594e+02  3.873e+03   0.041   0.9681  \nsale.l1           6.880e-01  3.443e-01   1.998   0.0767 .\ndate.l1           1.287e+04  1.066e+04   1.208   0.2580  \nsaving.l2        -2.261e+03  3.450e+03  -0.656   0.5285  \ngdp_deflator.l2   1.876e+03  4.337e+03   0.433   0.6755  \nsale.l2           6.351e-01  3.876e-01   1.638   0.1357  \ndate.l2           3.733e+03  7.516e+03   0.497   0.6313  \nsaving.l3         1.248e+03  3.369e+03   0.370   0.7196  \ngdp_deflator.l3  -1.625e+03  4.098e+03  -0.396   0.7010  \nsale.l3          -4.350e-02  3.273e-01  -0.133   0.8972  \ndate.l3           7.415e+03  8.247e+03   0.899   0.3920  \nsaving.l4        -1.349e+02  2.612e+03  -0.052   0.9599  \ngdp_deflator.l4   9.301e+02  4.334e+03   0.215   0.8348  \nsale.l4          -4.541e-01  3.008e-01  -1.510   0.1654  \ndate.l4           1.650e+04  7.629e+03   2.163   0.0588 .\nsaving.l5        -2.476e+03  3.443e+03  -0.719   0.4902  \ngdp_deflator.l5   6.603e+03  4.218e+03   1.565   0.1519  \nsale.l5          -5.115e-01  3.207e-01  -1.595   0.1452  \ndate.l5          -6.944e+03  8.852e+03  -0.784   0.4530  \nsaving.l6         1.271e+03  2.755e+03   0.461   0.6555  \ngdp_deflator.l6   2.518e+03  3.369e+03   0.747   0.4739  \nsale.l6           1.281e-01  2.654e-01   0.483   0.6409  \ndate.l6          -4.265e+02  7.853e+03  -0.054   0.9579  \nsaving.l7        -3.353e+03  2.587e+03  -1.296   0.2272  \ngdp_deflator.l7  -1.840e+03  3.273e+03  -0.562   0.5877  \nsale.l7           1.813e-01  2.854e-01   0.635   0.5411  \ndate.l7           1.409e+04  1.064e+04   1.324   0.2180  \nsaving.l8        -1.446e+03  3.602e+03  -0.401   0.6975  \ngdp_deflator.l8  -1.376e+03  2.214e+03  -0.621   0.5497  \nsale.l8           8.313e-02  4.697e-01   0.177   0.8634  \ndate.l8           9.431e+03  9.968e+03   0.946   0.3688  \nsaving.l9         4.737e+03  4.025e+03   1.177   0.2695  \ngdp_deflator.l9   6.886e+03  2.246e+03   3.066   0.0134 *\nsale.l9          -5.729e-01  5.222e-01  -1.097   0.3011  \ndate.l9           8.875e+03  9.072e+03   0.978   0.3535  \nsaving.l10        1.197e+03  3.894e+03   0.308   0.7654  \ngdp_deflator.l10  1.482e+03  2.396e+03   0.618   0.5516  \nsale.l10         -3.825e-01  3.753e-01  -1.019   0.3346  \ndate.l10          9.854e+03  1.292e+04   0.763   0.4650  \nsaving.l11       -7.255e+03  3.475e+03  -2.088   0.0664 .\ngdp_deflator.l11 -3.763e+03  2.129e+03  -1.767   0.1110  \nsale.l11          6.217e-01  4.267e-01   1.457   0.1791  \ndate.l11          2.048e+02  9.226e+03   0.022   0.9828  \nsaving.l12        7.267e+02  2.796e+03   0.260   0.8008  \ngdp_deflator.l12 -8.956e+02  1.881e+03  -0.476   0.6453  \nsale.l12          5.491e-01  5.290e-01   1.038   0.3264  \ndate.l12         -2.756e+03  1.187e+04  -0.232   0.8216  \nsaving.l13        5.873e+03  3.144e+03   1.868   0.0946 .\ngdp_deflator.l13  2.631e+03  2.577e+03   1.021   0.3339  \nsale.l13         -4.564e-01  4.780e-01  -0.955   0.3646  \ndate.l13          1.527e+04  8.682e+03   1.759   0.1125  \nsaving.l14       -3.648e+03  2.640e+03  -1.382   0.2003  \ngdp_deflator.l14  4.575e+03  2.837e+03   1.613   0.1413  \nsale.l14         -2.910e-02  3.742e-01  -0.078   0.9397  \ndate.l14          1.203e+04  9.767e+03   1.232   0.2493  \nconst            -1.019e+09  6.567e+08  -1.551   0.1553  \ntrend            -9.144e+06  5.886e+06  -1.553   0.1547  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6356 on 9 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9762 \nF-statistic: 48.56 on 57 and 9 DF,  p-value: 5.12e-07 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)    \nsaving.l1         8.299e-02  7.324e-02   1.133 0.286439    \ngdp_deflator.l1   1.194e-01  9.525e-02   1.254 0.241473    \nsale.l1          -1.590e-05  8.467e-06  -1.878 0.093170 .  \ndate.l1          -4.492e-01  2.622e-01  -1.713 0.120843    \nsaving.l2        -7.984e-02  8.484e-02  -0.941 0.371238    \ngdp_deflator.l2  -2.103e-01  1.067e-01  -1.972 0.080143 .  \nsale.l2           9.861e-06  9.533e-06   1.034 0.327939    \ndate.l2          -4.547e-01  1.848e-01  -2.460 0.036172 *  \nsaving.l3        -1.341e-01  8.286e-02  -1.619 0.139954    \ngdp_deflator.l3   1.625e-01  1.008e-01   1.612 0.141445    \nsale.l3           1.319e-05  8.049e-06   1.638 0.135816    \ndate.l3          -3.961e-01  2.028e-01  -1.953 0.082614 .  \nsaving.l4         1.795e-01  6.425e-02   2.794 0.020915 *  \ngdp_deflator.l4  -9.927e-02  1.066e-01  -0.931 0.375966    \nsale.l4          -5.617e-06  7.398e-06  -0.759 0.467135    \ndate.l4          -5.027e-01  1.876e-01  -2.679 0.025241 *  \nsaving.l5        -1.752e-02  8.467e-02  -0.207 0.840662    \ngdp_deflator.l5  -4.003e-02  1.037e-01  -0.386 0.708537    \nsale.l5          -5.790e-06  7.888e-06  -0.734 0.481610    \ndate.l5          -2.319e-01  2.177e-01  -1.065 0.314529    \nsaving.l6        -7.085e-02  6.775e-02  -1.046 0.322971    \ngdp_deflator.l6   1.248e-01  8.285e-02   1.506 0.166408    \nsale.l6          -2.178e-06  6.528e-06  -0.334 0.746331    \ndate.l6          -8.145e-01  1.931e-01  -4.217 0.002248 ** \nsaving.l7        -1.446e-02  6.363e-02  -0.227 0.825273    \ngdp_deflator.l7  -8.114e-03  8.049e-02  -0.101 0.921911    \nsale.l7          -2.832e-06  7.019e-06  -0.404 0.695986    \ndate.l7          -7.434e-01  2.617e-01  -2.841 0.019371 *  \nsaving.l8         1.013e-01  8.858e-02   1.144 0.282109    \ngdp_deflator.l8  -4.556e-02  5.444e-02  -0.837 0.424296    \nsale.l8           2.081e-05  1.155e-05   1.801 0.105199    \ndate.l8          -1.276e-01  2.451e-01  -0.521 0.615182    \nsaving.l9        -1.781e-01  9.900e-02  -1.799 0.105569    \ngdp_deflator.l9  -5.781e-02  5.524e-02  -1.047 0.322605    \nsale.l9          -1.051e-05  1.284e-05  -0.818 0.434331    \ndate.l9          -7.510e-01  2.231e-01  -3.366 0.008311 ** \nsaving.l10        1.108e-01  9.577e-02   1.157 0.276891    \ngdp_deflator.l10  8.634e-02  5.892e-02   1.465 0.176890    \nsale.l10          1.325e-06  9.229e-06   0.144 0.888979    \ndate.l10         -6.388e-02  3.176e-01  -0.201 0.845087    \nsaving.l11        7.038e-02  8.547e-02   0.823 0.431554    \ngdp_deflator.l11  6.357e-02  5.237e-02   1.214 0.255697    \nsale.l11         -1.509e-05  1.050e-05  -1.438 0.184362    \ndate.l11         -8.454e-01  2.269e-01  -3.726 0.004727 ** \nsaving.l12       -1.632e-01  6.878e-02  -2.372 0.041738 *  \ngdp_deflator.l12 -8.413e-02  4.625e-02  -1.819 0.102280    \nsale.l12          7.241e-06  1.301e-05   0.557 0.591356    \ndate.l12         -4.219e-01  2.919e-01  -1.445 0.182287    \nsaving.l13        4.446e-02  7.732e-02   0.575 0.579376    \ngdp_deflator.l13 -8.406e-02  6.339e-02  -1.326 0.217429    \nsale.l13          1.197e-05  1.176e-05   1.019 0.335032    \ndate.l13         -6.007e-01  2.135e-01  -2.813 0.020262 *  \nsaving.l14        5.157e-02  6.493e-02   0.794 0.447430    \ngdp_deflator.l14  8.936e-02  6.977e-02   1.281 0.232283    \nsale.l14         -1.014e-05  9.203e-06  -1.101 0.299312    \ndate.l14         -5.418e-01  2.402e-01  -2.255 0.050550 .  \nconst             8.139e+04  1.615e+04   5.040 0.000700 ***\ntrend             7.255e+02  1.448e+02   5.011 0.000728 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1563 on 9 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.5e+08 on 57 and 9 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                 saving gdp_deflator       sale      date\nsaving        3.208e-01     -0.16908 -1.498e+03  0.002709\ngdp_deflator -1.691e-01      0.32167  2.226e+03 -0.011232\nsale         -1.498e+03   2225.66391  4.040e+07 44.867011\ndate          2.709e-03     -0.01123  4.487e+01  0.024434\n\nCorrelation matrix of residuals:\n              saving gdp_deflator     sale     date\nsaving        1.0000      -0.5264 -0.41625  0.03060\ngdp_deflator -0.5264       1.0000  0.61741 -0.12670\nsale         -0.4162       0.6174  1.00000  0.04516\ndate          0.0306      -0.1267  0.04516  1.00000\n\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one."
  },
  {
    "objectID": "ASV.html#cross-validations-1",
    "href": "ASV.html#cross-validations-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross Validations",
    "text": "Cross Validations\n\n\nCode\nn=length(dd3$gdp_deflator)\nn*0.3\n\n\n[1] 24.3\n\n\n\n\nCode\nk=25 #19*4\nn-k\n\n\n[1] 56\n\n\n\n\nCode\ndd3\n\n\n   homevalue saving gdp_deflator   sale  date\n1   121428.3    5.0          2.7 165300 10957\n2   121642.0    4.5          2.5 163200 11048\n3   121906.9    4.3          2.4 168800 11139\n4   122475.1    4.8          2.2 172900 11231\n5   123129.1    4.8          2.6 169800 11323\n6   123830.3    4.8          2.4 179000 11413\n7   124572.9    5.1          1.6 172500 11504\n8   125374.5    5.2          1.3 171100 11596\n9   126208.3    4.5          1.3 188700 11688\n10  127045.3    4.8          1.4 187200 11778\n11  127878.3    4.7          1.9 178100 11869\n12  128712.4    4.4          2.3 190100 11961\n13  129478.1    4.9          2.0 186000 12053\n14  130156.2    5.0          1.4 191800 12143\n15  130810.2    5.3          2.3 191900 12234\n16  131517.4    5.1          2.5 198800 12326\n17  132280.5    4.5          2.9 212700 12418\n18  133063.4    4.4          3.3 217600 12509\n19  133869.0    5.6          2.6 213500 12600\n20  134710.7    6.6          3.1 228800 12692\n21  135559.8    7.0          3.2 232500 12784\n22  136405.3    3.1          2.9 233700 12874\n23  137193.3    3.5          3.7 236400 12965\n24  137932.1    3.8          3.3 243600 13057\n25  138585.5    5.7          2.8 247700 13149\n26  139210.5    5.5          3.6 246300 13239\n27  139847.6    5.6          2.8 235600 13330\n28  140540.8    5.5          1.5 245400 13422\n29  141306.4    6.2          3.9 257400 13514\n30  142135.7    6.2          2.7 242200 13604\n31  143044.9    5.4          2.1 241800 13695\n32  144015.6    5.4          1.7 238400 13787\n33  144999.2    5.9          1.4 233900 13879\n34  145981.3    5.8          2.0 235300 13970\n35  146916.7    5.8          3.1 226500 14061\n36  147827.4    5.4          1.0 222500 14153\n37  148651.4    5.4         -0.2 208400 14245\n38  149440.0    5.3         -0.7 220900 14335\n39  150228.1    5.0          0.4 214300 14426\n40  151086.1    5.1          1.3 219000 14518\n41  152030.1    5.5          1.1 222900 14610\n42  153009.4    5.3          2.0 219500 14700\n43  154040.0    6.1          1.2 224100 14791\n44  155134.6    5.9          2.4 224300 14883\n45  156292.9    5.1          2.1 226900 14975\n46  157461.6    5.3          2.7 228100 15065\n47  158548.5    5.3          2.5 223500 15156\n48  159570.5    5.2          0.5 221100 15248\n49  160546.3    4.9          2.4 238400 15340\n50  161588.7    4.9          1.6 238700 15431\n51  162738.7    4.7          2.1 248800 15522\n52  164024.4    5.1          2.0 251700 15614\n53  165472.1    5.1          1.6 258400 15706\n54  167048.8    5.6          1.1 268100 15796\n55  168747.7    5.0          1.9 264800 15887\n56  170471.2    5.0          2.4 273600 15979\n57  172172.7    4.4          1.7 275200 16071\n58  173803.4    4.3          2.3 288000 16161\n59  175333.8    3.8          1.8 281000 16252\n60  176841.6    6.6          0.7 298900 16344\n61  178300.5    3.2         -0.1 289200 16436\n62  179788.9    2.9          2.2 289100 16526\n63  181324.4    3.1          1.2 295800 16617\n64  183054.1    2.7          0.0 302500 16709\n65  184891.6    3.2         -0.3 299800 16801\n66  186816.3    2.6          2.9 306000 16892\n67  188755.2    2.1          1.1 303800 16983\n68  190716.5    2.6          2.1 310900 17075\n69  192645.2    2.7          2.1 313100 17167\n70  194471.9    2.9          1.3 318200 17257\n71  196139.6    3.2          2.0 320500 17348\n72  197646.4    3.1          2.8 337900 17440\n73  198936.7    3.7          2.5 331800 17532\n74  200135.0    3.9          3.5 315600 17622\n75  201342.0    4.1          1.4 330900 17713\n76  202638.9    3.8          1.8 322800 17805\n77  203918.9    3.6          1.6 313000 17897\n78  205047.8    3.7          2.2 322500 17987\n79  205932.3    3.1          1.3 318400 18078\n80  206612.8    3.3          1.5 327100 18170\n81  207058.4    3.2          1.6 329000 18262\n\n\n\n\nCode\ndat = ts(dd3[,c(1,2,3,4)])\n\n\n\nCross Validation For Home Value\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plot\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Home Value\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRSME Comparesions\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 170107.3\n\n\nCode\nerror2\n\n\n[1] 170105.8\n\n\n\n\n\nCross Validation For Saving Rate\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE PLOT\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 2.784849\n\n\nCode\nerror2\n\n\n[1] 4.109366\n\n\n\n\n\nCross Validation For GDP Deflator\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plots\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 3.594482\n\n\nCode\nerror2\n\n\n[1] 5.038616\n\n\n\n\n\nCross Validation For Sale Price\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(4)][1:(k-1)+i] \n  xtest <- dat[,c(4)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plots\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Values Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 268955.8\n\n\nCode\nerror2\n\n\n[1] 268954.3\n\n\nThe second one VAR(14) is better for Home Value and Sale Price, However, overall, not too different The first one VAR(13) is better for GDP Deflator and Saving Rate Therefore, VAR(13) is relatively better than VAR(14)"
  },
  {
    "objectID": "ASV.html#forecasts",
    "href": "ASV.html#forecasts",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasts",
    "text": "Forecasts\n\n\nCode\nfit1 <- vars::VAR(dat[,1:4], p=13, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n\n\n         fcst     lower    upper       CI\n[1,] 3.718453 2.6102444 4.826661 1.108208\n[2,] 5.602610 4.4620373 6.743183 1.140573\n[3,] 7.872473 6.4972147 9.247731 1.375258\n[4,] 4.592520 3.0702843 6.114757 1.522236\n[5,] 5.953920 4.3310094 7.576831 1.622911\n[6,] 2.901753 0.8499155 4.953590 2.051837\n[7,] 4.187176 2.0589012 6.315450 2.128275\n[8,] 2.717952 0.3920197 5.043885 2.325933\n\n\n\n\nCode\nfcast1$fcst$sale\n\n\n         fcst    lower    upper       CI\n[1,] 348102.2 336133.1 360071.2 11969.06\n[2,] 353458.9 339669.3 367248.4 13789.55\n[3,] 350327.6 336109.4 364545.8 14218.21\n[4,] 321335.8 304967.9 337703.7 16367.93\n[5,] 309302.3 291867.5 326737.0 17434.72\n[6,] 286105.4 268004.7 304206.1 18100.70\n[7,] 271981.5 253762.0 290200.9 18219.43\n[8,] 248976.4 229541.9 268410.9 19434.55\n\n\n\n\nCode\nfcast1$fcst$saving\n\n\n          fcst     lower     upper       CI\n[1,]  3.774885  2.665978  4.883793 1.108907\n[2,]  4.885199  3.206902  6.563496 1.678297\n[3,]  9.928629  8.142069 11.715188 1.786559\n[4,] 10.166946  8.366419 11.967473 1.800527\n[5,] 12.688735 10.877602 14.499869 1.811134\n[6,] 10.564305  8.658945 12.469664 1.905360\n[7,]  9.590800  7.567340 11.614261 2.023461\n[8,]  8.682248  6.444347 10.920149 2.237901\n\n\n\n\nCode\nfcast1$fcst$homevalue\n\n\n         fcst    lower    upper         CI\n[1,] 207370.9 207313.3 207428.5   57.61206\n[2,] 207451.6 207280.8 207622.4  170.81360\n[3,] 207354.1 206979.7 207728.4  374.33551\n[4,] 207004.8 206375.3 207634.3  629.46283\n[5,] 206534.7 205611.8 207457.5  922.84057\n[6,] 206003.4 204767.3 207239.5 1236.10298\n[7,] 205538.6 203971.4 207105.8 1567.18724\n[8,] 205002.0 203098.4 206905.6 1903.59212\n\n\n\n\nCode\nfcast1$fcst$sale\n\n\n         fcst    lower    upper       CI\n[1,] 348102.2 336133.1 360071.2 11969.06\n[2,] 353458.9 339669.3 367248.4 13789.55\n[3,] 350327.6 336109.4 364545.8 14218.21\n[4,] 321335.8 304967.9 337703.7 16367.93\n[5,] 309302.3 291867.5 326737.0 17434.72\n[6,] 286105.4 268004.7 304206.1 18100.70\n[7,] 271981.5 253762.0 290200.9 18219.43\n[8,] 248976.4 229541.9 268410.9 19434.55\n\n\n\n\nCode\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")"
  },
  {
    "objectID": "GARCH.html#prepare-the-dataset",
    "href": "GARCH.html#prepare-the-dataset",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Prepare the Dataset",
    "text": "Prepare the Dataset\n\n\n\n\n\n\nIncome Stock Analsyis with Verizon\nLiteruature Review: Since my big picture and goal is to evaluate the correlation between Home values, prices and income to see the impact among them. Therefore, an income stock would be effective for me to get my result.\nWireless subscribers of Verizon provide a reliable base of revenue and cash flow. Verizon generated an impressive $12.4 billion of free cash flow through the first nine months of 2022, giving it the funds to reward its shareholders with $8.1 billion in dividends.\nVerizon’s shares offer a hefty dividend yield (it was more than 6% in late 2022). The telecom giant has also increased its dividend for 16 straight years, the longest current streak in the U.S. telecom sector. That attractive and growing income stream makes Verizon a great stock for earning passive income.\nUsing this dataset, I can represent and make analysis on the income to see if the modeling can capture the pattern effectively."
  },
  {
    "objectID": "GARCH.html#calculate-the-returns",
    "href": "GARCH.html#calculate-the-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Calculate the Returns",
    "text": "Calculate the Returns\n\n\n[1] \"VZ\"\n\n\n\n\n\n\n\n           Income Stock Returns       date\n2016-01-05          0.013641027 2016-01-05\n2016-01-06         -0.021300513 2016-01-06\n2016-01-07         -0.005507228 2016-01-07\n2016-01-08         -0.009766973 2016-01-08\n2016-01-11          0.005782897 2016-01-11\n2016-01-12         -0.003554766 2016-01-12\n\n\nHere, we also have the new column for returns\n\n\nCode\nstock$date<-as.Date(stock$date,\"%Y-%m-%d\")\nfig <- plot_ly(stock, x = ~date, y = ~`Income Stock Returns`, name = 'Income Stock Returns From 2016', type = 'scatter', mode = 'lines') %>%\n  \n  layout(\n    title = \"Income Stock Returns  From 2016\",\n    paper_bgcolor = '#E0E0E0', # Set the background color \n    plot_bgcolor = '#E0E0E0', # Set the background color \n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Stock returns\")\n  )"
  },
  {
    "objectID": "GARCH.html#stationarity",
    "href": "GARCH.html#stationarity",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stationarity",
    "text": "Stationarity\nThe plot shows no particular trend. This indicates that the series is likely stationary because the mean of the series could be constant or not changing too much over time."
  },
  {
    "objectID": "GARCH.html#volatility",
    "href": "GARCH.html#volatility",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volatility",
    "text": "Volatility\nThe series seems to exhibit periods of different volatility levels within expectations. However, overall, it is pretty steady throughout the time. There are some fluctuations, but the variations appear relatively stable and small.\nVolatility in financial time series is often clustered; periods of high volatility are followed by periods of high volatility, and periods of low volatility are followed by periods of low volatility. This plot suggests such clustering might be present."
  },
  {
    "objectID": "GARCH.html#prepare-to-fit-the-model-and-check-acf-and-pacf",
    "href": "GARCH.html#prepare-to-fit-the-model-and-check-acf-and-pacf",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Prepare to fit the model and check acf and pacf",
    "text": "Prepare to fit the model and check acf and pacf\n\n\nCode\nreturns = ts(stock$`Income Stock Returns`)\nacf_plot <- ggAcf(returns) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(returns) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nWe can see that the plots for the returns are weakly stationary."
  },
  {
    "objectID": "GARCH.html#acf-of-absolute-values-of-the-returns",
    "href": "GARCH.html#acf-of-absolute-values-of-the-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF of absolute values of the returns",
    "text": "ACF of absolute values of the returns\n\n\nCode\nacf_plot <- ggAcf(abs(returns)) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(abs(returns)) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)"
  },
  {
    "objectID": "GARCH.html#acf-for-squared-values",
    "href": "GARCH.html#acf-for-squared-values",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF for squared values",
    "text": "ACF for squared values\n\n\nCode\nacf_plot <- ggAcf(returns^2) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(returns^2) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nBased on the acf and pacf, we can see that the returns are weakly stationary.\n\n\nCode\nlibrary(tseries)\nadf.test(returns)\n\n\nWarning in adf.test(returns): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  returns\nDickey-Fuller = -12.686, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that there are little correlations left. The returns are stationary."
  },
  {
    "objectID": "GARCH.html#fit-an-appropriate-ararcharmagarch-or-arima-archgarch",
    "href": "GARCH.html#fit-an-appropriate-ararcharmagarch-or-arima-archgarch",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH",
    "text": "Fit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH\n\nFirst, determine the ARIMA model using model diagnostic\n\n\nCode\n# Reference from the lab 6 part 1 demo:\ntemp.ts = returns\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*71),nrow=71)\n\nfor (p in 1:5)\n{\n  for(q in 1:5)\n  {\n    for(d in 0:2)#\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(temp.ts,order=c(p-1,d,q-1),include.drift=FALSE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-11745.382\n-11734.197\n-11745.376\n\n\n0\n1\n0\n-10278.151\n-10272.559\n-10278.149\n\n\n0\n2\n0\n-8045.864\n-8040.273\n-8045.862\n\n\n0\n0\n1\n-11747.304\n-11730.527\n-11747.292\n\n\n0\n1\n1\n-11730.871\n-11719.688\n-11730.865\n\n\n0\n2\n1\n-10262.373\n-10251.190\n-10262.367\n\n\n0\n0\n2\n-11747.827\n-11725.458\n-11747.807\n\n\n0\n1\n2\n-11732.784\n-11716.009\n-11732.772\n\n\n0\n2\n2\n-11681.966\n-11665.192\n-11681.954\n\n\n0\n0\n3\n-11748.308\n-11720.346\n-11748.277\n\n\n0\n1\n3\n-11733.337\n-11710.970\n-11733.317\n\n\n0\n2\n3\n-11663.255\n-11640.889\n-11663.235\n\n\n0\n0\n4\n-11748.368\n-11714.814\n-11748.326\n\n\n0\n1\n4\n-11733.762\n-11705.803\n-11733.732\n\n\n0\n2\n4\n-11675.645\n-11647.688\n-11675.615\n\n\n1\n0\n0\n-11747.576\n-11730.799\n-11747.564\n\n\n1\n1\n0\n-10957.920\n-10946.736\n-10957.914\n\n\n1\n2\n0\n-9368.017\n-9356.834\n-9368.011\n\n\n1\n0\n1\n-11747.493\n-11725.123\n-11747.472\n\n\n1\n1\n1\n-11733.042\n-11716.267\n-11733.030\n\n\n1\n2\n1\n-10940.935\n-10924.161\n-10940.923\n\n\n1\n0\n2\n-11746.590\n-11718.628\n-11746.560\n\n\n1\n1\n2\n-11732.974\n-11710.607\n-11732.954\n\n\n1\n2\n2\n-11666.519\n-11644.153\n-11666.499\n\n\n1\n0\n3\n-11747.175\n-11713.620\n-11747.132\n\n\n1\n1\n3\n-11734.648\n-11706.689\n-11734.618\n\n\n1\n2\n3\n-11652.780\n-11624.823\n-11652.749\n\n\n1\n0\n4\n-11758.279\n-11719.133\n-11758.223\n\n\n1\n1\n4\n-11737.956\n-11704.405\n-11737.913\n\n\n1\n2\n4\n-11698.618\n-11665.070\n-11698.575\n\n\n2\n0\n0\n-11748.013\n-11725.644\n-11747.993\n\n\n2\n1\n0\n-11155.537\n-11138.762\n-11155.525\n\n\n2\n2\n0\n-9939.557\n-9922.783\n-9939.545\n\n\n2\n0\n1\n-11746.403\n-11718.441\n-11746.373\n\n\n2\n1\n1\n-11733.515\n-11711.147\n-11733.495\n\n\n2\n2\n1\n-11137.912\n-11115.547\n-11137.892\n\n\n2\n0\n2\n-11752.330\n-11718.776\n-11752.288\n\n\n2\n1\n2\n-11729.981\n-11702.022\n-11729.950\n\n\n2\n2\n2\n-11691.835\n-11663.878\n-11691.805\n\n\n2\n0\n3\n-11746.133\n-11706.987\n-11746.076\n\n\n2\n1\n3\n-11730.281\n-11696.730\n-11730.239\n\n\n2\n2\n3\n-11691.169\n-11657.621\n-11691.127\n\n\n2\n0\n4\n-11756.256\n-11711.517\n-11756.183\n\n\n2\n1\n4\n-11731.122\n-11691.979\n-11731.065\n\n\n2\n2\n4\n-11690.040\n-11650.901\n-11689.983\n\n\n3\n0\n0\n-11747.448\n-11719.486\n-11747.417\n\n\n3\n1\n0\n-11257.341\n-11234.974\n-11257.321\n\n\n3\n2\n0\n-10215.582\n-10193.216\n-10215.562\n\n\n3\n0\n1\n-11746.623\n-11713.069\n-11746.581\n\n\n3\n1\n1\n-11732.916\n-11704.957\n-11732.886\n\n\n3\n2\n1\n-11239.257\n-11211.301\n-11239.227\n\n\n3\n0\n2\n-11746.236\n-11707.089\n-11746.179\n\n\n3\n1\n2\n-11732.070\n-11698.519\n-11732.028\n\n\n3\n2\n2\n-11684.516\n-11650.968\n-11684.473\n\n\n3\n0\n3\n-11744.189\n-11699.451\n-11744.117\n\n\n3\n1\n3\n-11724.890\n-11685.747\n-11724.833\n\n\n3\n2\n3\n-11693.643\n-11654.503\n-11693.586\n\n\n3\n0\n4\n-11755.169\n-11704.838\n-11755.078\n\n\n3\n1\n4\n-11719.422\n-11674.687\n-11719.349\n\n\n4\n0\n0\n-11748.903\n-11715.349\n-11748.860\n\n\n4\n1\n0\n-11370.038\n-11342.078\n-11370.007\n\n\n4\n2\n0\n-10473.688\n-10445.731\n-10473.658\n\n\n4\n0\n1\n-11759.226\n-11720.079\n-11759.169\n\n\n4\n1\n1\n-11734.363\n-11700.811\n-11734.320\n\n\n4\n2\n1\n-11351.471\n-11317.923\n-11351.429\n\n\n4\n0\n2\n-11757.224\n-11712.485\n-11757.151\n\n\n4\n1\n2\n-11744.694\n-11705.551\n-11744.637\n\n\n4\n2\n2\n-11692.959\n-11653.820\n-11692.903\n\n\n4\n0\n3\n-11755.526\n-11705.194\n-11755.435\n\n\n4\n1\n3\n-11741.229\n-11696.494\n-11741.156\n\n\n4\n0\n4\n-11759.351\n-11703.428\n-11759.240\n\n\n\n\n\n\n\nEvaluations using AIC, BIC, and AICc\n\n\n   p d q       AIC       BIC      AICc\n71 4 0 4 -11759.35 -11703.43 -11759.24\n\n\n  p d q       AIC      BIC      AICc\n1 0 0 0 -11745.38 -11734.2 -11745.38\n\n\n   p d q       AIC       BIC      AICc\n71 4 0 4 -11759.35 -11703.43 -11759.24\n\n\nWe can see that for (4,0,4) has the lowest AIC and AICc. For (0,0,0), it has the lowest BIC. Therefore we should compare them."
  },
  {
    "objectID": "GARCH.html#compare-arima-models-for-404-and-arima-000",
    "href": "GARCH.html#compare-arima-models-for-404-and-arima-000",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Compare ARIMA Models for (4,0,4) and ARIMA (0,0,0)",
    "text": "Compare ARIMA Models for (4,0,4) and ARIMA (0,0,0)\n\n\n\n\n\n\n\n\n\n\nfinal  value -4.389022 \nstopped after 100 iterations\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ar3      ar4     ma1      ma2     ma3     ma4   xmean\n      -0.259  0.8386  -0.2731  -0.8354  0.2154  -0.8339  0.2782  0.7779  -1e-04\ns.e.     NaN  0.0636      NaN   0.0753     NaN   0.0720     NaN  0.0891   3e-04\n\nsigma^2 estimated as 0.0001541:  log likelihood = 5889.68,  aic = -11759.35\n\n$degrees_of_freedom\n[1] 1974\n\n$ttable\n      Estimate     SE  t.value p.value\nar1    -0.2590    NaN      NaN     NaN\nar2     0.8386 0.0636  13.1812  0.0000\nar3    -0.2731    NaN      NaN     NaN\nar4    -0.8354 0.0753 -11.0975  0.0000\nma1     0.2154    NaN      NaN     NaN\nma2    -0.8339 0.0720 -11.5739  0.0000\nma3     0.2782    NaN      NaN     NaN\nma4     0.7779 0.0891   8.7279  0.0000\nxmean  -0.0001 0.0003  -0.4742  0.6354\n\n$AIC\n[1] -5.930081\n\n$AICc\n[1] -5.930035\n\n$BIC\n[1] -5.90188\n\nNA\n\n\n\n\n\nsigma^2 estimated as 0.0001564:  log likelihood = 5874.69,  aic = -11745.38\n\n$degrees_of_freedom\n[1] 1982\n\n$ttable\n      Estimate    SE t.value p.value\nxmean   -1e-04 3e-04 -0.4223  0.6729\n\n$AIC\n[1] -5.923037\n\n$AICc\n[1] -5.923036\n\n$BIC\n[1] -5.917396\n\n\nBased on the model comparsion and diagnostic, I think that ARIMA Models for (4,0,4) is better with smaller evaluation matrices and it is more suitable and proper based on acf and pacf plots. In addition, the acf of residule is also more stationary."
  },
  {
    "objectID": "GARCH.html#fit-the-best-one-arima404",
    "href": "GARCH.html#fit-the-best-one-arima404",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fit the best one ARIMA(4,0,4)",
    "text": "Fit the best one ARIMA(4,0,4)\n\n\nCode\nfit2 <- arima(temp.ts, order = c(4,0,4))\nsummary(fit2)\n\n\n\nCall:\narima(x = temp.ts, order = c(4, 0, 4))\n\nCoefficients:\n         ar1     ar2      ar3      ar4     ma1      ma2     ma3     ma4\n      -0.259  0.8386  -0.2731  -0.8354  0.2154  -0.8339  0.2782  0.7779\ns.e.     NaN  0.0636      NaN   0.0753     NaN   0.0720     NaN  0.0891\n      intercept\n         -1e-04\ns.e.      3e-04\n\nsigma^2 estimated as 0.0001541:  log likelihood = 5889.68,  aic = -11759.35\n\nTraining set error measures:\n                      ME       RMSE         MAE MPE MAPE      MASE        ACF1\nTraining set 5.06567e-06 0.01241243 0.008686146 NaN  Inf 0.6859388 0.003727206"
  },
  {
    "objectID": "GARCH.html#get-the-residuals-of-the-arima-model",
    "href": "GARCH.html#get-the-residuals-of-the-arima-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "get the residuals of the arima model",
    "text": "get the residuals of the arima model\n\n\n\n\n\nWe can see that mostly, the residual is stationary however the residuals of ARIMA model indicates some flutucations. In this case, I think that we can try to fit an ARCH/GARCH model.\nTherefore, I think we can further making analysis by adding GARCH models to see if we should use it."
  },
  {
    "objectID": "GARCH.html#model-diagnostics-for-archgarch-model",
    "href": "GARCH.html#model-diagnostics-for-archgarch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Diagnostics For ARCH/GARCH model",
    "text": "Model Diagnostics For ARCH/GARCH model\n\nAcf and pacf for squared residuals\n\n\n\n\n\nWe can see that it is not yet stationary for squared residuals. GARCH model can be applied.\nHowever, no matter what, I still think that I should make a further diagnostic and compare the results.\n\n\nCode\nmean_res <- mean(arima.res, na.rm = TRUE)\nsd_res <- sd(arima.res, na.rm = TRUE)\n\n# Normalize the residuals\narima.res <- (arima.res - mean_res) / sd_res\n\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:7) {\n  for (q in 1:7) {\n  \nmodel[[cc]] <- garch(arima.res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\n\n\n\n\n[1] 35\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         a3         a4         a5         b1  \n3.258e-01  1.697e-01  5.803e-02  4.661e-02  8.038e-03  1.044e-01  3.080e-03  \n       b2         b3         b4         b5         b6         b7  \n1.078e-14  6.203e-02  2.340e-02  1.280e-02  3.958e-02  1.607e-01  \n\n\nFrom here, I think that garch(5,6) is a good choice\n\n\nNOTE: Packages 'fBasics', 'timeDate', and 'timeSeries' are no longer\nattached to the search() path when 'fGarch' is attached.\n\nIf needed attach them yourself in your R script by e.g.,\n        require(\"timeSeries\")\n\n\n\nAttaching package: 'fGarch'\n\n\nThe following object is masked from 'package:TTR':\n\n    volatility\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(5, 6), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(5, 6)\n<environment: 0x000001e53c78d300>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1       alpha2       alpha3       alpha4  \n-1.6874e-16   2.2543e-01   1.3768e-01   5.5858e-02   2.9213e-02   1.0000e-08  \n     alpha5        beta1        beta2        beta3        beta4        beta5  \n 7.5133e-02   1.0000e-08   1.0000e-08   3.1469e-01   1.6925e-01   1.0000e-08  \n      beta6  \n 1.0000e-08  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -1.687e-16   2.052e-02    0.000 1.000000    \nomega   2.254e-01   2.805e-01    0.804 0.421614    \nalpha1  1.377e-01   3.905e-02    3.525 0.000423 ***\nalpha2  5.586e-02   9.879e-02    0.565 0.571769    \nalpha3  2.921e-02   1.250e-01    0.234 0.815173    \nalpha4  1.000e-08   1.262e-01    0.000 1.000000    \nalpha5  7.513e-02   6.684e-02    1.124 0.260997    \nbeta1   1.000e-08   7.142e-01    0.000 1.000000    \nbeta2   1.000e-08   3.978e-01    0.000 1.000000    \nbeta3   3.147e-01   1.955e-01    1.610 0.107500    \nbeta4   1.692e-01   2.506e-01    0.675 0.499448    \nbeta5   1.000e-08   4.406e-01    0.000 1.000000    \nbeta6   1.000e-08         NaN      NaN      NaN    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -2704.735    normalized:  -1.363961 \n\nDescription:\n Mon Nov 20 22:34:29 2023 by user: yzh20 \n\n\nStandardised Residuals Tests:\n                                   Statistic    p-Value\n Jarque-Bera Test   R    Chi^2  1567.0838773 0.00000000\n Shapiro-Wilk Test  R    W         0.9555035 0.00000000\n Ljung-Box Test     R    Q(10)     7.4317759 0.68415000\n Ljung-Box Test     R    Q(15)    12.8665717 0.61260318\n Ljung-Box Test     R    Q(20)    28.7236060 0.09334599\n Ljung-Box Test     R^2  Q(10)     2.5905318 0.98948975\n Ljung-Box Test     R^2  Q(15)     5.0826087 0.99140358\n Ljung-Box Test     R^2  Q(20)     6.2308460 0.99854770\n LM Arch Test       R    TR^2      3.5306982 0.99049639\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n2.741034 2.777696 2.740949 2.754502 \n\n\nThe results shows that (5,6) is not an optimal fit. The coefficients are not significant.\nTherefore, let us try GARCH(1,2) and GARCH(1,1)\n\n\nTry to compare with Garch(1,1),Garch(1,2)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x000001e53b4e6680>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-1.6874e-16   9.3778e-02   1.0948e-01   7.9863e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -1.687e-16   2.030e-02    0.000  1.00000    \nomega   9.378e-02   2.893e-02    3.241  0.00119 ** \nalpha1  1.095e-01   2.170e-02    5.045 4.53e-07 ***\nbeta1   7.986e-01   4.545e-02   17.570  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -2713.414    normalized:  -1.368338 \n\nDescription:\n Mon Nov 20 22:34:29 2023 by user: yzh20 \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  1842.376562 0.0000000\n Shapiro-Wilk Test  R    W         0.952537 0.0000000\n Ljung-Box Test     R    Q(10)     7.407214 0.6865228\n Ljung-Box Test     R    Q(15)    12.974946 0.6042332\n Ljung-Box Test     R    Q(20)    28.397143 0.1003264\n Ljung-Box Test     R^2  Q(10)     3.461572 0.9683871\n Ljung-Box Test     R^2  Q(15)     6.479211 0.9705262\n Ljung-Box Test     R^2  Q(20)     7.857064 0.9927719\n LM Arch Test       R    TR^2      4.561534 0.9710469\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n2.740710 2.751991 2.740702 2.744854 \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x000001e53c5493c0>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2  \n-1.6874e-16   1.2838e-01   1.5585e-01   2.8817e-01   4.3124e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -1.687e-16   2.027e-02    0.000 1.000000    \nomega   1.284e-01   4.092e-02    3.137 0.001706 ** \nalpha1  1.559e-01   3.167e-02    4.922 8.58e-07 ***\nbeta1   2.882e-01   1.383e-01    2.083 0.037258 *  \nbeta2   4.312e-01   1.199e-01    3.598 0.000321 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -2711.124    normalized:  -1.367183 \n\nDescription:\n Mon Nov 20 22:34:30 2023 by user: yzh20 \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  1736.0814311 0.0000000\n Shapiro-Wilk Test  R    W         0.9536721 0.0000000\n Ljung-Box Test     R    Q(10)     7.0492350 0.7207888\n Ljung-Box Test     R    Q(15)    12.5374314 0.6379784\n Ljung-Box Test     R    Q(20)    27.7833434 0.1146281\n Ljung-Box Test     R^2  Q(10)     4.7226574 0.9089183\n Ljung-Box Test     R^2  Q(15)     7.8011003 0.9315089\n Ljung-Box Test     R^2  Q(20)     9.0154593 0.9827275\n LM Arch Test       R    TR^2      6.1485452 0.9084094\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n2.739409 2.753510 2.739396 2.744589 \n\n\nNow, it seems that the GARCH(1,2) is better with more siginificant components."
  },
  {
    "objectID": "GARCH.html#final-model-arima-404-garch12",
    "href": "GARCH.html#final-model-arima-404-garch12",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Final Model: ARIMA (4,0,4) + GARCH(1,2)",
    "text": "Final Model: ARIMA (4,0,4) + GARCH(1,2)\nThe final model consist of the ARIMA modeling with no differencing and the GARCH model. This model combined the two different models to evaluate and capture the returns. The final one is better than the separate ones.\n\nForecast: ARIMA (4,0,4) + GARCH(1,2)\n\n\nCode\nfinal.fit <- garchFit(~garch(1,2), arima.res,trace = F)\npredict(final.fit, n.ahead = 365, plot=TRUE)\n\n\n\n\n\n     meanForecast meanError standardDeviation lowerInterval upperInterval\n1   -1.687411e-16 0.8793496         0.8793496     -1.723494      1.723494\n2   -1.687411e-16 0.9211444         0.9211444     -1.805410      1.805410\n3   -1.687411e-16 0.9157470         0.9157470     -1.794831      1.794831\n4   -1.687411e-16 0.9309361         0.9309361     -1.824601      1.824601\n5   -1.687411e-16 0.9353186         0.9353186     -1.833191      1.833191\n6   -1.687411e-16 0.9436888         0.9436888     -1.849596      1.849596\n7   -1.687411e-16 0.9492413         0.9492413     -1.860479      1.860479\n8   -1.687411e-16 0.9552530         0.9552530     -1.872261      1.872261\n9   -1.687411e-16 0.9602731         0.9602731     -1.882101      1.882101\n10  -1.687411e-16 0.9650553         0.9650553     -1.891474      1.891474\n11  -1.687411e-16 0.9693126         0.9693126     -1.899818      1.899818\n12  -1.687411e-16 0.9732389         0.9732389     -1.907513      1.907513\n13  -1.687411e-16 0.9767967         0.9767967     -1.914486      1.914486\n14  -1.687411e-16 0.9800518         0.9800518     -1.920866      1.920866\n15  -1.687411e-16 0.9830166         0.9830166     -1.926677      1.926677\n16  -1.687411e-16 0.9857245         0.9857245     -1.931984      1.931984\n17  -1.687411e-16 0.9881952         0.9881952     -1.936827      1.936827\n18  -1.687411e-16 0.9904515         0.9904515     -1.941249      1.941249\n19  -1.687411e-16 0.9925118         0.9925118     -1.945287      1.945287\n20  -1.687411e-16 0.9943938         0.9943938     -1.948976      1.948976\n21  -1.687411e-16 0.9961130         0.9961130     -1.952346      1.952346\n22  -1.687411e-16 0.9976839         0.9976839     -1.955425      1.955425\n23  -1.687411e-16 0.9991195         0.9991195     -1.958238      1.958238\n24  -1.687411e-16 1.0004315         1.0004315     -1.960810      1.960810\n25  -1.687411e-16 1.0016307         1.0016307     -1.963160      1.963160\n26  -1.687411e-16 1.0027270         1.0027270     -1.965309      1.965309\n27  -1.687411e-16 1.0037294         1.0037294     -1.967273      1.967273\n28  -1.687411e-16 1.0046458         1.0046458     -1.969070      1.969070\n29  -1.687411e-16 1.0054838         1.0054838     -1.970712      1.970712\n30  -1.687411e-16 1.0062502         1.0062502     -1.972214      1.972214\n31  -1.687411e-16 1.0069511         1.0069511     -1.973588      1.973588\n32  -1.687411e-16 1.0075921         1.0075921     -1.974844      1.974844\n33  -1.687411e-16 1.0081784         1.0081784     -1.975993      1.975993\n34  -1.687411e-16 1.0087147         1.0087147     -1.977044      1.977044\n35  -1.687411e-16 1.0092053         1.0092053     -1.978006      1.978006\n36  -1.687411e-16 1.0096540         1.0096540     -1.978886      1.978886\n37  -1.687411e-16 1.0100646         1.0100646     -1.979690      1.979690\n38  -1.687411e-16 1.0104402         1.0104402     -1.980426      1.980426\n39  -1.687411e-16 1.0107838         1.0107838     -1.981100      1.981100\n40  -1.687411e-16 1.0110982         1.0110982     -1.981716      1.981716\n41  -1.687411e-16 1.0113858         1.0113858     -1.982280      1.982280\n42  -1.687411e-16 1.0116490         1.0116490     -1.982796      1.982796\n43  -1.687411e-16 1.0118898         1.0118898     -1.983268      1.983268\n44  -1.687411e-16 1.0121101         1.0121101     -1.983699      1.983699\n45  -1.687411e-16 1.0123118         1.0123118     -1.984095      1.984095\n46  -1.687411e-16 1.0124962         1.0124962     -1.984456      1.984456\n47  -1.687411e-16 1.0126651         1.0126651     -1.984787      1.984787\n48  -1.687411e-16 1.0128195         1.0128195     -1.985090      1.985090\n49  -1.687411e-16 1.0129609         1.0129609     -1.985367      1.985367\n50  -1.687411e-16 1.0130902         1.0130902     -1.985620      1.985620\n51  -1.687411e-16 1.0132086         1.0132086     -1.985852      1.985852\n52  -1.687411e-16 1.0133169         1.0133169     -1.986065      1.986065\n53  -1.687411e-16 1.0134161         1.0134161     -1.986259      1.986259\n54  -1.687411e-16 1.0135068         1.0135068     -1.986437      1.986437\n55  -1.687411e-16 1.0135898         1.0135898     -1.986600      1.986600\n56  -1.687411e-16 1.0136658         1.0136658     -1.986748      1.986748\n57  -1.687411e-16 1.0137353         1.0137353     -1.986885      1.986885\n58  -1.687411e-16 1.0137989         1.0137989     -1.987009      1.987009\n59  -1.687411e-16 1.0138572         1.0138572     -1.987124      1.987124\n60  -1.687411e-16 1.0139105         1.0139105     -1.987228      1.987228\n61  -1.687411e-16 1.0139592         1.0139592     -1.987324      1.987324\n62  -1.687411e-16 1.0140039         1.0140039     -1.987411      1.987411\n63  -1.687411e-16 1.0140447         1.0140447     -1.987491      1.987491\n64  -1.687411e-16 1.0140821         1.0140821     -1.987564      1.987564\n65  -1.687411e-16 1.0141163         1.0141163     -1.987631      1.987631\n66  -1.687411e-16 1.0141476         1.0141476     -1.987693      1.987693\n67  -1.687411e-16 1.0141763         1.0141763     -1.987749      1.987749\n68  -1.687411e-16 1.0142025         1.0142025     -1.987800      1.987800\n69  -1.687411e-16 1.0142265         1.0142265     -1.987847      1.987847\n70  -1.687411e-16 1.0142484         1.0142484     -1.987890      1.987890\n71  -1.687411e-16 1.0142685         1.0142685     -1.987930      1.987930\n72  -1.687411e-16 1.0142869         1.0142869     -1.987966      1.987966\n73  -1.687411e-16 1.0143038         1.0143038     -1.987999      1.987999\n74  -1.687411e-16 1.0143192         1.0143192     -1.988029      1.988029\n75  -1.687411e-16 1.0143333         1.0143333     -1.988057      1.988057\n76  -1.687411e-16 1.0143462         1.0143462     -1.988082      1.988082\n77  -1.687411e-16 1.0143580         1.0143580     -1.988105      1.988105\n78  -1.687411e-16 1.0143688         1.0143688     -1.988126      1.988126\n79  -1.687411e-16 1.0143787         1.0143787     -1.988146      1.988146\n80  -1.687411e-16 1.0143878         1.0143878     -1.988163      1.988163\n81  -1.687411e-16 1.0143960         1.0143960     -1.988180      1.988180\n82  -1.687411e-16 1.0144036         1.0144036     -1.988195      1.988195\n83  -1.687411e-16 1.0144106         1.0144106     -1.988208      1.988208\n84  -1.687411e-16 1.0144169         1.0144169     -1.988221      1.988221\n85  -1.687411e-16 1.0144227         1.0144227     -1.988232      1.988232\n86  -1.687411e-16 1.0144281         1.0144281     -1.988242      1.988242\n87  -1.687411e-16 1.0144329         1.0144329     -1.988252      1.988252\n88  -1.687411e-16 1.0144374         1.0144374     -1.988261      1.988261\n89  -1.687411e-16 1.0144415         1.0144415     -1.988269      1.988269\n90  -1.687411e-16 1.0144452         1.0144452     -1.988276      1.988276\n91  -1.687411e-16 1.0144486         1.0144486     -1.988283      1.988283\n92  -1.687411e-16 1.0144517         1.0144517     -1.988289      1.988289\n93  -1.687411e-16 1.0144546         1.0144546     -1.988294      1.988294\n94  -1.687411e-16 1.0144572         1.0144572     -1.988300      1.988300\n95  -1.687411e-16 1.0144596         1.0144596     -1.988304      1.988304\n96  -1.687411e-16 1.0144618         1.0144618     -1.988309      1.988309\n97  -1.687411e-16 1.0144638         1.0144638     -1.988313      1.988313\n98  -1.687411e-16 1.0144656         1.0144656     -1.988316      1.988316\n99  -1.687411e-16 1.0144673         1.0144673     -1.988319      1.988319\n100 -1.687411e-16 1.0144689         1.0144689     -1.988322      1.988322\n101 -1.687411e-16 1.0144703         1.0144703     -1.988325      1.988325\n102 -1.687411e-16 1.0144716         1.0144716     -1.988328      1.988328\n103 -1.687411e-16 1.0144727         1.0144727     -1.988330      1.988330\n104 -1.687411e-16 1.0144738         1.0144738     -1.988332      1.988332\n105 -1.687411e-16 1.0144748         1.0144748     -1.988334      1.988334\n106 -1.687411e-16 1.0144757         1.0144757     -1.988336      1.988336\n107 -1.687411e-16 1.0144765         1.0144765     -1.988337      1.988337\n108 -1.687411e-16 1.0144773         1.0144773     -1.988339      1.988339\n109 -1.687411e-16 1.0144780         1.0144780     -1.988340      1.988340\n110 -1.687411e-16 1.0144786         1.0144786     -1.988342      1.988342\n111 -1.687411e-16 1.0144792         1.0144792     -1.988343      1.988343\n112 -1.687411e-16 1.0144797         1.0144797     -1.988344      1.988344\n113 -1.687411e-16 1.0144802         1.0144802     -1.988345      1.988345\n114 -1.687411e-16 1.0144807         1.0144807     -1.988346      1.988346\n115 -1.687411e-16 1.0144811         1.0144811     -1.988346      1.988346\n116 -1.687411e-16 1.0144815         1.0144815     -1.988347      1.988347\n117 -1.687411e-16 1.0144818         1.0144818     -1.988348      1.988348\n118 -1.687411e-16 1.0144821         1.0144821     -1.988348      1.988348\n119 -1.687411e-16 1.0144824         1.0144824     -1.988349      1.988349\n120 -1.687411e-16 1.0144827         1.0144827     -1.988349      1.988349\n121 -1.687411e-16 1.0144829         1.0144829     -1.988350      1.988350\n122 -1.687411e-16 1.0144831         1.0144831     -1.988350      1.988350\n123 -1.687411e-16 1.0144833         1.0144833     -1.988351      1.988351\n124 -1.687411e-16 1.0144835         1.0144835     -1.988351      1.988351\n125 -1.687411e-16 1.0144837         1.0144837     -1.988351      1.988351\n126 -1.687411e-16 1.0144838         1.0144838     -1.988352      1.988352\n127 -1.687411e-16 1.0144840         1.0144840     -1.988352      1.988352\n128 -1.687411e-16 1.0144841         1.0144841     -1.988352      1.988352\n129 -1.687411e-16 1.0144842         1.0144842     -1.988353      1.988353\n130 -1.687411e-16 1.0144843         1.0144843     -1.988353      1.988353\n131 -1.687411e-16 1.0144844         1.0144844     -1.988353      1.988353\n132 -1.687411e-16 1.0144845         1.0144845     -1.988353      1.988353\n133 -1.687411e-16 1.0144846         1.0144846     -1.988353      1.988353\n134 -1.687411e-16 1.0144847         1.0144847     -1.988353      1.988353\n135 -1.687411e-16 1.0144847         1.0144847     -1.988354      1.988354\n136 -1.687411e-16 1.0144848         1.0144848     -1.988354      1.988354\n137 -1.687411e-16 1.0144849         1.0144849     -1.988354      1.988354\n138 -1.687411e-16 1.0144849         1.0144849     -1.988354      1.988354\n139 -1.687411e-16 1.0144850         1.0144850     -1.988354      1.988354\n140 -1.687411e-16 1.0144850         1.0144850     -1.988354      1.988354\n141 -1.687411e-16 1.0144850         1.0144850     -1.988354      1.988354\n142 -1.687411e-16 1.0144851         1.0144851     -1.988354      1.988354\n143 -1.687411e-16 1.0144851         1.0144851     -1.988354      1.988354\n144 -1.687411e-16 1.0144851         1.0144851     -1.988354      1.988354\n145 -1.687411e-16 1.0144852         1.0144852     -1.988354      1.988354\n146 -1.687411e-16 1.0144852         1.0144852     -1.988354      1.988354\n147 -1.687411e-16 1.0144852         1.0144852     -1.988355      1.988355\n148 -1.687411e-16 1.0144852         1.0144852     -1.988355      1.988355\n149 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n150 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n151 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n152 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n153 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n154 -1.687411e-16 1.0144853         1.0144853     -1.988355      1.988355\n155 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n156 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n157 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n158 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n159 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n160 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n161 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n162 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n163 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n164 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n165 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n166 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n167 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n168 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n169 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n170 -1.687411e-16 1.0144854         1.0144854     -1.988355      1.988355\n171 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n172 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n173 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n174 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n175 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n176 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n177 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n178 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n179 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n180 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n181 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n182 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n183 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n184 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n185 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n186 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n187 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n188 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n189 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n190 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n191 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n192 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n193 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n194 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n195 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n196 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n197 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n198 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n199 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n200 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n201 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n202 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n203 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n204 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n205 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n206 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n207 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n208 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n209 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n210 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n211 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n212 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n213 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n214 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n215 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n216 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n217 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n218 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n219 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n220 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n221 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n222 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n223 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n224 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n225 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n226 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n227 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n228 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n229 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n230 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n231 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n232 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n233 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n234 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n235 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n236 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n237 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n238 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n239 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n240 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n241 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n242 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n243 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n244 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n245 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n246 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n247 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n248 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n249 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n250 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n251 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n252 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n253 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n254 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n255 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n256 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n257 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n258 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n259 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n260 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n261 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n262 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n263 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n264 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n265 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n266 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n267 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n268 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n269 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n270 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n271 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n272 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n273 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n274 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n275 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n276 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n277 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n278 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n279 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n280 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n281 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n282 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n283 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n284 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n285 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n286 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n287 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n288 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n289 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n290 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n291 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n292 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n293 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n294 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n295 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n296 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n297 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n298 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n299 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n300 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n301 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n302 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n303 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n304 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n305 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n306 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n307 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n308 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n309 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n310 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n311 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n312 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n313 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n314 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n315 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n316 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n317 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n318 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n319 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n320 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n321 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n322 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n323 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n324 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n325 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n326 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n327 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n328 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n329 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n330 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n331 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n332 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n333 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n334 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n335 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n336 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n337 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n338 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n339 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n340 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n341 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n342 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n343 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n344 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n345 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n346 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n347 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n348 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n349 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n350 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n351 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n352 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n353 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n354 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n355 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n356 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n357 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n358 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n359 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n360 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n361 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n362 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n363 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n364 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n365 -1.687411e-16 1.0144855         1.0144855     -1.988355      1.988355\n\n\nWe can see that the model captures the relatively constant variation with the confidence intervals fitted in the range. This means that the model combination is effective to predict the future values. The overall interpretation and prediction on the historical dataset is reasonable.\n\n\nBox Ljung Test\n\n\nCode\nbox_ljung_test <- Box.test(arima.res, lag = 10, type = \"Ljung-Box\")\n\n# Display the test results\nbox_ljung_test\n\n\n\n    Box-Ljung test\n\ndata:  arima.res\nX-squared = 5.2087, df = 10, p-value = 0.8768\n\n\nThe Box Ljung yields the simiarl results of my model choosing. The p-value is above a significance level 0.05, therefore, I do not reject the null hypothesis. This suggests that the residuals do not exhibit autocorrelation and that the model is adequate. This conclusion alignes that my models choosing capture the dataset and make predictions well.\n\n\nThe Equation\nThe ARIMA (0,2,3) + Garch(1,0) model is defined as: The combined ARIMA(0,2,3) + GARCH(1,0) model is defined as:\nThe ARIMA(0,2,3) + GARCH(1,0) model is defined as:\n[ \\[\\begin{align*}\n(1 - B)^2 X_t &= \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\theta_3 \\varepsilon_{t-3} + \\varepsilon_t, \\\\\n\\text{where } \\varepsilon_t &\\sim N(0, \\sigma_t^2), \\\\\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2.\n\\end{align*}\\] ]\nHere: - ( (1 - B)^2 X_t ) represents the second difference of the time series ( X_t ). - ( _1, _2, _3 ) are the parameters of the moving average (MA) component. - ( _t ) is the white noise error term at time ( t ). - ( _t^2 ) is the conditional variance at time ( t ). - ( _0, _1 ) are the coefficients of the GARCH model’s variance equation. - ( _1 ) is the coefficient for the lagged conditional variance."
  },
  {
    "objectID": "GARCH.html#calculate-the-returns-1",
    "href": "GARCH.html#calculate-the-returns-1",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Calculate the Returns",
    "text": "Calculate the Returns\n\n\n\n\n\n        date Mean.Sale.Price        state Mean.Home.Value     mean\n1 2018-08-31         1138521 San Jose, CA         1158784 2932.893\n2 2018-09-30         1121256 San Jose, CA         1169495 2929.350\n3 2018-10-31         1133715 San Jose, CA         1177582 2918.253\n4 2018-11-30         1104818 San Jose, CA         1180853 2900.606\n5 2018-12-31         1081306 San Jose, CA         1179078 2886.816\n6 2019-01-31         1060308 San Jose, CA         1169053 2889.411\n  Metropolitan.Area Home_Value_Return Rental_Price_Return Sale_Price_Return\n1      San Jose, CA        38.6990930         -0.57535478         45.463315\n2      San Jose, CA         0.9243363         -0.12078553         -1.516441\n3      San Jose, CA         0.6914401         -0.37882682          1.111165\n4      San Jose, CA         0.2778182         -0.60471865         -2.548877\n5      San Jose, CA        -0.1503518         -0.47541551         -2.128133\n6      San Jose, CA        -0.8501966          0.08988467         -1.941911"
  },
  {
    "objectID": "GARCH.html#plot-for-rental-sale-home-values-for-san-jose-ca",
    "href": "GARCH.html#plot-for-rental-sale-home-values-for-san-jose-ca",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Plot For Rental, Sale, Home Values For San Jose, CA",
    "text": "Plot For Rental, Sale, Home Values For San Jose, CA\n\n\nCode\nlibrary(plotly)\n\n# Your existing plot_ly code\np <- plot_ly(data = san_jose_data, x = ~date) %>%\n     add_trace(y = ~Rental_Price_Return, name = 'Mean Rental Price', mode = 'lines') \n\n# Adding background color and title\np <- p %>% layout(\n    title = \"Mean Rental Price Over Time\",  # Add your title here\n    paper_bgcolor = '#E0E0E0',  # Set the background color of the plotting area\n    plot_bgcolor = '#E0E0E0'  # Set the background color of the graph\n)\n\n\n\n\n\n\nCode\np <- plot_ly(data = san_jose_data, x = ~date) %>%\n     add_trace(y = ~Mean.Sale.Price, name = 'Mean Sale Price', mode = 'lines') %>%\n     add_trace(y = ~Mean.Home.Value, name = 'Mean Home Value', mode = 'lines') \n# Adding background color and title\np <- p %>% layout(\n    title = \"Mean Sale Price and Home Value Over Time\",  # Add your title here\n    paper_bgcolor = '#E0E0E0',  # Set the background color of the plotting area\n    plot_bgcolor = '#E0E0E0'  # Set the background color of the graph\n)"
  },
  {
    "objectID": "GARCH.html#return-plots",
    "href": "GARCH.html#return-plots",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Return Plots",
    "text": "Return Plots\n\n\nCode\nlibrary(ggplot2)\n\np <- ggplot(san_jose_data, aes(x = date)) +\n  geom_line(aes(y = Sale_Price_Return), color = \"blue\") +\n  geom_line(aes(y = Home_Value_Return), color = \"red\") +\n  geom_line(aes(y = Rental_Price_Return), color = \"green\") +\n  labs(title = \"Time Series Plot Of Returns\", x = \"Date\", y = \"Value\") +\n  theme_minimal()\n\n# Modify the background color\np <- p + theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = NA), # Background of the entire plot\n    panel.background = element_rect(fill = \"#E0E0E0\", color = NA)  # Background of the plotting area\n)\n\n# Display the plot\np"
  },
  {
    "objectID": "GARCH.html#stationarity-1",
    "href": "GARCH.html#stationarity-1",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stationarity",
    "text": "Stationarity\nThe plot shows a clear upward trend in both Mean Sale Price and Mean Home Value over time until what appears to be a sharp drop in the most recent data point. This trend indicates that the series is likely non-stationary because the mean of the series is changing over time. The presence of a trend is a strong indication that at least the mean is not constant. The drop at the end could be due to the incident of Covid-19, an extreme value, or a real market crash. ## Volatility The series seems to exhibit periods of different volatility levels. Initially, there is some fluctuation, but the variations appear relatively stable and small. However, the spike at the end of the series suggests a sudden increase in volatility. Volatility in financial time series is often clustered; periods of high volatility are followed by periods of high volatility, and periods of low volatility are followed by periods of low volatility. This plot suggests such clustering might be present, although the spike at the end may skew this perception."
  },
  {
    "objectID": "GARCH.html#first-fit-a-linear-model",
    "href": "GARCH.html#first-fit-a-linear-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "First Fit a Linear Model",
    "text": "First Fit a Linear Model\nI think that the home value can be consisted of the sale price and rental price\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\n\n\nCall:\nlm(formula = Mean.Home.Value ~ Mean.Sale.Price + mean, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1077736   -23052     3736    58944   139684 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      4.130e+05  4.781e+05   0.864    0.392    \nMean.Sale.Price  9.340e-01  1.973e-01   4.733  1.8e-05 ***\nmean            -1.092e+02  1.869e+02  -0.585    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162500 on 51 degrees of freedom\nMultiple R-squared:  0.347, Adjusted R-squared:  0.3214 \nF-statistic: 13.55 on 2 and 51 DF,  p-value: 1.904e-05\n\n\nThe coefficients are significant\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCode\nvif(model)\n\n\nMean.Sale.Price            mean \n       1.362895        1.362895 \n\n\nWe can see that the Mean sale price and rental price have much relatively low VIF scores. Therefore, we can keep them in modeling:"
  },
  {
    "objectID": "GARCH.html#evaluations-of-the-model",
    "href": "GARCH.html#evaluations-of-the-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Evaluations of the model",
    "text": "Evaluations of the model\n\n\nCode\npredictions <- predict(model, newdata = test)\nSSE <- sum((predictions - test$Mean.Home.Value)^2)\nSST <- sum((test$Mean.Home.Value - mean(train$Mean.Home.Value))^2)\nrsquared_test <- 1 - SSE/SST\nrsquared_test \n\n\n[1] 0.8123188\n\n\nThe rsquared is relatively small, we can now keep the model for later analysis."
  },
  {
    "objectID": "GARCH.html#prepare-to-fit-the-model-and-get-residuals",
    "href": "GARCH.html#prepare-to-fit-the-model-and-get-residuals",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Prepare to fit the model and get residuals",
    "text": "Prepare to fit the model and get residuals\n\n\nCode\nlm.residuals <- residuals(model)\nplot(lm.residuals, ylab = \"Residuals\", main = \"Residuals \")\n\n\n\n\n\n\ncheck correlation in these residuals using an ACF plot\n\n\nCode\nacf_plot <- ggAcf(lm.residuals) +\n  labs(title = \"ACF for Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(lm.residuals) +\n  labs(title = \"PACF for Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nBased on the acf and pacf, we can choose p = 0,1,2,3 q = 0,1,2\n\n\nCode\nlibrary(tseries)\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -4.5916, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWE can see that there are little correlations left. The lm residuals are stationary now."
  },
  {
    "objectID": "GARCH.html#fit-an-appropriate-ararcharmagarch-or-arima-archgarch-1",
    "href": "GARCH.html#fit-an-appropriate-ararcharmagarch-or-arima-archgarch-1",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH",
    "text": "Fit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH\n\nFirst, determine the ARIMA model using model diagnostic\n\n\nCode\n# Reference from the lab 6 part 1 demo:\ntemp.ts = log(diff(lm.residuals))\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*71),nrow=71)\n\nfor (p in 1:5)\n{\n  for(q in 1:5)\n  {\n    for(d in 0:2)#\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(temp.ts,order=c(p-1,d,q-1),include.drift=FALSE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n97.45315\n100.18774\n97.91469\n\n\n0\n1\n0\n107.36990\n108.70210\n107.52375\n\n\n0\n2\n0\n136.14317\n137.43900\n136.30317\n\n\n0\n0\n1\n97.72055\n101.82243\n98.68055\n\n\n0\n1\n1\n97.19728\n99.86169\n97.67728\n\n\n0\n2\n1\n109.48434\n112.07602\n109.98434\n\n\n0\n0\n2\n99.71110\n105.18029\n101.37777\n\n\n0\n1\n2\n98.08008\n102.07670\n99.08008\n\n\n0\n2\n2\n100.84798\n104.73549\n101.89146\n\n\n0\n0\n3\n101.63650\n108.47298\n104.24520\n\n\n0\n1\n3\n99.15448\n104.48330\n100.89361\n\n\n0\n2\n3\n100.96699\n106.15033\n102.78517\n\n\n0\n0\n4\n98.32414\n106.52792\n102.14232\n\n\n0\n1\n4\n96.49156\n103.15259\n99.21884\n\n\n0\n2\n4\n101.82668\n108.30587\n104.68383\n\n\n1\n0\n0\n98.48268\n102.58457\n99.44268\n\n\n1\n1\n0\n107.31463\n109.97904\n107.79463\n\n\n1\n2\n0\n130.00856\n132.60023\n130.50856\n\n\n1\n0\n1\n99.71315\n105.18233\n101.37982\n\n\n1\n1\n1\n99.01782\n103.01443\n100.01782\n\n\n1\n2\n1\n109.92294\n113.81045\n110.96642\n\n\n1\n0\n2\n101.50948\n108.34596\n104.11817\n\n\n1\n1\n2\n99.91203\n105.24085\n101.65116\n\n\n1\n2\n2\n102.39524\n107.57859\n104.21342\n\n\n1\n0\n3\n102.90624\n111.11002\n106.72443\n\n\n1\n1\n3\n100.00410\n106.66512\n102.73137\n\n\n1\n2\n3\n102.64853\n109.12772\n105.50568\n\n\n1\n0\n4\n100.32335\n109.89442\n105.65668\n\n\n1\n1\n4\n98.17100\n106.16423\n102.17100\n\n\n1\n2\n4\n101.91516\n109.69018\n106.11516\n\n\n2\n0\n0\n100.19843\n105.66761\n101.86509\n\n\n2\n1\n0\n97.91843\n101.91504\n98.91843\n\n\n2\n2\n0\n120.71913\n124.60664\n121.76261\n\n\n2\n0\n1\n101.67495\n108.51143\n104.28364\n\n\n2\n1\n1\n97.80713\n103.13595\n99.54626\n\n\n2\n2\n1\n101.76404\n106.94739\n103.58222\n\n\n2\n0\n2\n103.40488\n111.60866\n107.22307\n\n\n2\n1\n2\n99.68591\n106.34694\n102.41319\n\n\n2\n2\n2\n100.81063\n107.28982\n103.66778\n\n\n2\n0\n3\n101.43536\n111.00643\n106.76869\n\n\n2\n1\n3\n98.94989\n106.94312\n102.94989\n\n\n2\n2\n3\n102.04989\n109.82491\n106.24989\n\n\n2\n0\n4\n99.25180\n110.19016\n106.45180\n\n\n2\n1\n4\n100.21015\n109.53558\n105.81015\n\n\n2\n2\n4\n100.95327\n110.02412\n106.84800\n\n\n3\n0\n0\n99.33167\n106.16815\n101.94036\n\n\n3\n1\n0\n97.99772\n103.32654\n99.73685\n\n\n3\n2\n0\n109.96631\n115.14965\n111.78449\n\n\n3\n0\n1\n101.26710\n109.47087\n105.08528\n\n\n3\n1\n1\n99.55553\n106.21655\n102.28280\n\n\n3\n2\n1\n102.21620\n108.69538\n105.07334\n\n\n3\n0\n2\n99.34029\n108.91136\n104.67363\n\n\n3\n1\n2\n99.42824\n107.42147\n103.42824\n\n\n3\n2\n2\n102.50291\n110.27793\n106.70291\n\n\n3\n0\n3\n101.19914\n112.13751\n108.39914\n\n\n3\n1\n3\n99.33271\n108.65814\n104.93271\n\n\n3\n2\n3\n103.01403\n112.08489\n108.90877\n\n\n3\n0\n4\n104.20091\n116.50657\n113.67459\n\n\n3\n1\n4\n101.16082\n111.81846\n108.73977\n\n\n4\n0\n0\n101.16119\n109.36496\n104.97937\n\n\n4\n1\n0\n99.85388\n106.51490\n102.58115\n\n\n4\n2\n0\n111.92853\n118.40772\n114.78568\n\n\n4\n0\n1\n101.61498\n111.18605\n106.94831\n\n\n4\n1\n1\n101.11461\n109.10784\n105.11461\n\n\n4\n2\n1\n104.02255\n111.79757\n108.22255\n\n\n4\n0\n2\n100.73599\n111.67436\n107.93599\n\n\n4\n1\n2\n101.42404\n110.74947\n107.02404\n\n\n4\n2\n2\n104.34488\n113.41574\n110.23962\n\n\n4\n0\n3\n102.73352\n115.03919\n112.20721\n\n\n4\n1\n3\n100.78264\n111.44028\n108.36159\n\n\n4\n0\n4\n101.54131\n115.21426\n113.76353\n\n\n\n\n\n\n\nEvaluations using AIC, BIC, and AICc\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n14 0 1 4 96.49156 103.1526 99.21884\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n5 0 1 1 97.19728 99.86169 97.67728\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n5 0 1 1 97.19728 99.86169 97.67728\n\n\nWe can see that for (0,1,1) and (0,1,4), the AIC, BIC, and AICc are different. We should choose from them.\n\n\nCompare ARIMA Models for (4,0,4) and ARIMA (0,0,0)\n\n\nCode\nset.seed(236)\n\nmodel_output21 <- capture.output(sarima(lm.residuals, 0,1,1)) \n\n\n\n\n\nCode\nmodel_output22 <- capture.output(sarima(lm.residuals, 0,1,4)) \n\n\n\n\n\n\n\nCode\ncat(model_output21[120:147], model_output21[length(model_output21)], sep = \"\\n\")\n\n\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1   constant\n      -1.0000  -575.8749\ns.e.   0.0519  1390.1147\n\nsigma^2 estimated as 2.532e+10:  log likelihood = -712,  aic = 1430.01\n\n$degrees_of_freedom\n[1] 51\n\n$ttable\n          Estimate        SE  t.value p.value\nma1        -1.0000    0.0519 -19.2505  0.0000\nconstant -575.8749 1390.1147  -0.4143  0.6804\n\n$AIC\n[1] 26.98128\n\n$AICc\n[1] 26.98581\n\n$BIC\n[1] 27.09281\n\n\n\n\nCode\ncat(model_output22[50:80], model_output22[length(model_output22)], sep = \"\\n\")\n\n\n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2     ma3     ma4   constant\n      -1.1837  0.0274  0.0310  0.1253  -744.0292\ns.e.   0.1572  0.2249  0.2357  0.1388   784.9040\n\nsigma^2 estimated as 2.335e+10:  log likelihood = -710.54,  aic = 1433.07\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n          Estimate       SE t.value p.value\nma1        -1.1837   0.1572 -7.5293  0.0000\nma2         0.0274   0.2249  0.1216  0.9037\nma3         0.0310   0.2357  0.1315  0.8959\nma4         0.1253   0.1388  0.9032  0.3709\nconstant -744.0292 784.9040 -0.9479  0.3479\n\n$AIC\n[1] 27.03906\n\n$AICc\n[1] 27.06315\n\n$BIC\n[1] 27.26212\n\nNA\n\n\nBased on the model comparsion and diagnostic, I think that ARIMA Models for (0,1,1) is better with smaller evaluation matrices and it is more suitable and proper based on acf and pacf plots. In addition, the acf of residule is also more stationary."
  },
  {
    "objectID": "GARCH.html#fit-the-best-one-arima011",
    "href": "GARCH.html#fit-the-best-one-arima011",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fit the best one ARIMA(0,1,1)",
    "text": "Fit the best one ARIMA(0,1,1)\n\n\nCode\nmean_value <- mean(temp.ts, na.rm = TRUE)\n\n# Replace NaN values with the mean\ntemp.ts[is.na(temp.ts)] <- mean_value\n\n\n\n\nCode\nfit2 <- arima(temp.ts, order = c(0,1,1))\nsummary(fit2)\n\n\n\nCall:\narima(x = temp.ts, order = c(0, 1, 1))\n\nCoefficients:\n          ma1\n      -0.8543\ns.e.   0.0857\n\nsigma^2 estimated as 0.8381:  log likelihood = -69.85,  aic = 143.7\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set -0.006216912 0.9068076 0.5400164 -0.8352652 5.733552 0.7704727\n                    ACF1\nTraining set -0.03946064\n\n\n\nget the residuals of the arima model\n\n\nCode\narima.res <- residuals(fit2)\n# Plot the residuals\n\n\nacf_plot <- ggAcf(arima.res) +\n  labs(title = \"ACF for Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(arima.res) +\n  labs(title = \"PACF for Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nWe can see that mostly, the residual is stationary however the residuals of ARIMA model indicates some flutucations. In this case, I think that we can try to fit an ARCH/GARCH model.\nTherefore, I think we can further making analysis by adding GARCH models to see if we should use it."
  },
  {
    "objectID": "GARCH.html#model-diagnostics-for-archgarch-model-no-need-in-my-case-just-to-verify",
    "href": "GARCH.html#model-diagnostics-for-archgarch-model-no-need-in-my-case-just-to-verify",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Diagnostics For ARCH/GARCH model (No need in my case, Just to Verify)",
    "text": "Model Diagnostics For ARCH/GARCH model (No need in my case, Just to Verify)\n\n\nCode\nacf_plot <- ggAcf(arima.res^2) +\n  labs(title = \"ACF for Squared Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(arima.res^2) +\n  labs(title = \"PACF for Squared Model Residuals\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nWe can see that it acutally already sufficient to just use ARIMA model. GARCH model is not required since it is already pretty good in removing correlations. The residual and squared residual are stationary.\nHowever, I still think that I can at least try to make a further diagnostic and compare the results to verify that I do not need to fit GARCH model.\n\n\n\n\n\n\n\n\n[1] 29\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         a3         a4         a5         b1  \n6.293e-01  2.421e-15  1.233e-02  9.166e-02  1.292e-01  3.065e-02  7.709e-02  \n\n\nFrom here, I think that garch(5,1) is a good choice\n\n\nCode\nlibrary(fGarch)\nsummary(garchFit(~garch(5,1), arima.res,trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(5, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(5, 1)\n<environment: 0x000001e52aff0358>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1       alpha2       alpha3       alpha4  \n-6.2169e-02   8.3807e-07   1.0000e-08   1.0000e-08   1.0000e-08   9.2063e-02  \n     alpha5        beta1  \n 1.0279e-01   9.2259e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -6.217e-02   1.295e-01   -0.480    0.631    \nomega   8.381e-07   4.668e-02    0.000    1.000    \nalpha1  1.000e-08   1.437e-01    0.000    1.000    \nalpha2  1.000e-08   2.182e-01    0.000    1.000    \nalpha3  1.000e-08         NaN      NaN      NaN    \nalpha4  9.206e-02         NaN      NaN      NaN    \nalpha5  1.028e-01   2.194e-01    0.468    0.639    \nbeta1   9.226e-01   1.042e-01    8.856   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -65.04509    normalized:  -1.227266 \n\nDescription:\n Mon Nov 20 22:34:34 2023 by user: yzh20 \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  44.4248571 2.255615e-10\n Shapiro-Wilk Test  R    W       0.8789801 6.678698e-05\n Ljung-Box Test     R    Q(10)   8.6460018 5.659961e-01\n Ljung-Box Test     R    Q(15)  10.7960500 7.669135e-01\n Ljung-Box Test     R    Q(20)  14.7059080 7.929810e-01\n Ljung-Box Test     R^2  Q(10)   1.0064352 9.998227e-01\n Ljung-Box Test     R^2  Q(15)   2.8524750 9.997063e-01\n Ljung-Box Test     R^2  Q(20)  13.9148641 8.347861e-01\n LM Arch Test       R    TR^2    3.3655148 9.923671e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n2.756418 3.053821 2.718346 2.870785 \n\n\nThe results shows that (5,1) is not an optimal fit. The coefficients are not significant.\nTherefore, let us try (1,0)\n\nTry to compare with Garch(1,0)\n\n\nCode\nsummary(garchFit(~garch(1,0), arima.res,trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x000001e5332b85a8>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1  \n-0.00621650   0.82226165   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -6.216e-03   1.283e-01   -0.048    0.961    \nomega   8.223e-01   1.655e-01    4.969 6.73e-07 ***\nalpha1  1.000e-08   9.213e-02    0.000    1.000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -70.01777    normalized:  -1.32109 \n\nDescription:\n Mon Nov 20 22:34:34 2023 by user: yzh20 \n\n\nStandardised Residuals Tests:\n                                  Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  243.8613995 0.000000e+00\n Shapiro-Wilk Test  R    W        0.8077994 7.558550e-07\n Ljung-Box Test     R    Q(10)   11.1170806 3.484694e-01\n Ljung-Box Test     R    Q(15)   14.1037596 5.176748e-01\n Ljung-Box Test     R    Q(20)   17.1059480 6.460837e-01\n Ljung-Box Test     R^2  Q(10)    1.2640754 9.995016e-01\n Ljung-Box Test     R^2  Q(15)    1.6742729 9.999910e-01\n Ljung-Box Test     R^2  Q(20)    3.3919710 9.999883e-01\n LM Arch Test       R    TR^2    12.3534533 4.177270e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n2.755388 2.866914 2.749426 2.798275 \n\n\nNow, it seems that the alpha1 is still not significant. Therefore, in this case, we can stop with arima model. No additional Garch is needed.\nHowever, we still can compare the prediction fits."
  },
  {
    "objectID": "GARCH.html#forecast",
    "href": "GARCH.html#forecast",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Forecast",
    "text": "Forecast\n\nARIMA (0,1,1)\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,10)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\nBy comparing the fits, I think that the ARIMA model alone is better. We do not need the GArch in my case"
  },
  {
    "objectID": "GARCH.html#final-model-arima-011-and-equation",
    "href": "GARCH.html#final-model-arima-011-and-equation",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Final model: ARIMA (0,1,1) and equation",
    "text": "Final model: ARIMA (0,1,1) and equation\n\nBox Ljung Test\n\n\nCode\nbox_ljung_test <- Box.test(arima.res, lag = 10, type = \"Ljung-Box\")\n\n# Display the test results\nbox_ljung_test\n\n\n\n    Box-Ljung test\n\ndata:  arima.res\nX-squared = 11.117, df = 10, p-value = 0.3485\n\n\nthe p-value is above a significance level 0.05, therefore, I do not reject the null hypothesis. This suggests that the residuals do not exhibit autocorrelation and that the model is adequate. This conclusion alignes with my model diagnostics and forecasting comparesion. The ARIMA(0,2,3) alone is enough in my case.\n\n\nEquation\nThe ARIMA(0,1,1) model is defined as:\n\\[\n(1 - B) X_t = (1 + \\theta_1B)a_t\n\\]\nwhere: - ( B ) is the backshift operator, - ( X_t ) is the time series observation at time t, - ( (1 - B) ) denotes first-order differencing, - ( _1 ) is the parameter of the moving average part of the model, - ( a_t ) is the error term at time t.\nThis model is a simplification of higher-order ARIMA models and is used when the data exhibits a level of temporal dependency that can be explained with a single differencing and a single lag in the moving average component.\nNo Garch components since in my case the ARIMA is better and sufficient."
  },
  {
    "objectID": "TS.html#no-regulariztaion",
    "href": "TS.html#no-regulariztaion",
    "title": "Deep Learning for TS",
    "section": "No Regulariztaion",
    "text": "No Regulariztaion\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n     \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_8\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_2 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_8 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\n# TRAIN MODEL\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median Sale Price Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 94ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00132 RMSE = 0.03638\nTest MSE = 0.05922 RMSE = 0.24334\n(174,) (6,)\n\n\n\n\n\n\n\nCode\nrandom.seed(236)\n\n\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_9\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_3 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_9 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Ensures a 1D array output\n\n# Check shapes again to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 131ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 24ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.16279 RMSE = 0.40347\nTest MSE = 0.98611 RMSE = 0.99303\n\n\n\n\nCode\nprint(type(f_batch), f_batch)\nprint(type(epochs), epochs)\nprint(type(validation_split), validation_split)\n\n\n<class 'float'> 0.2\n<class 'range'> range(1, 201)\n<class 'float'> 0.2\n\n\n\n\nCode\nepochs\n\n\nrange(1, 201)\n\n\n\n\nCode\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n\n\n\n\n\nCode\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median Sale Price Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n\n\n\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (train_rmse**2.0, train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (test_rmse**2.0, test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nTrain MSE = 0.16279 RMSE = 0.40347\nTest MSE = 0.98611 RMSE = 0.99303\n(174,) (6,)\n\n\n\n\n\nGRU Model\nNO\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n      \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_10\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_2 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_10 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\n# TRAIN MODEL\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median Sale Price Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 438ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 28ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 22ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 24ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00021 RMSE = 0.01453\nTest MSE = 0.01487 RMSE = 0.12193\n(174,) (6,)\n\n\n\n\n\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n\nModel: \"sequential_11\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_3 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_11 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n\n\n\n\n\nCode\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()  # Ensures a 1D array output\ntest_predict = model.predict(testX).flatten()\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n\n# PLOT THE RESULT\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median Sale Price Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 243ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\nTrain MSE = 0.00411 RMSE = 0.06409\nTest MSE = 0.04240 RMSE = 0.20592\n(174,) (6,)\n\n\n\n\n\n. Effect of Including Regularization: Regularization, like the L2 regularization you’ve applied (regularizers.L2(1e-2)), is used to prevent overfitting by penalizing large weights in the model. Including regularization generally has the following effects:\nPrevents Overfitting: Regularization can help the model perform better on unseen data by preventing it from fitting too closely to the training data. Generalization: It encourages the model to learn more generalized patterns, which might be more robust to noise and variations in the data. Training Impact: The model might take longer to train or require more data to converge, as regularization adds a complexity penalty. To empirically assess the effect of regularization, you could compare the performance (in terms of RMSE or another metric) of models with and without regularization on both the training and validation/test sets.\n\nPredictive Horizon of the Deep Learning Model: The predictive horizon, or how far into the future the model can accurately predict, depends on several factors:\n\nNature of the Data: If the median sale price data has strong seasonal patterns or trends, the model might be able to predict further out with reasonable accuracy. Model Complexity: More complex models (more layers, neurons) might capture deeper patterns but risk overfitting. Training Data Span: The amount and variability of historical data used for training can impact how well the model generalizes to future predictions. To determine the predictive horizon, you could systematically increase the prediction length and observe how the prediction error (like RMSE) changes. A significant increase in error might indicate the limit of the model’s reliable predictive horizon.\n\nComparison with ARMA/ARIMA Models: Comparing the LSTM model with traditional ARMA/ARIMA models involves several considerations:\n\nPerformance Metrics: Compare the RMSE or other relevant metrics between the LSTM and ARMA/ARIMA models. Lower values indicate better predictive accuracy. Data Patterns: LSTM models are generally better at capturing complex, non-linear relationships and interactions in the data, while ARMA/ARIMA models are suited for linear relationships with well-defined autocorrelation structures. Computational Complexity: LSTM models can be more computationally intensive and require more data for training compared to ARMA/ARIMA models. Interpretability: ARMA/ARIMA models are often more interpretable in terms of understanding the influence of past values on future predictions. In summary, to answer your questions, you would need to conduct experiments that compare models with and without regularization, assess prediction accuracy over increasing future horizons, and compare the LSTM results with those obtained from ARMA/ARIMA models in previous homework. Each of these comparisons will provide insights into the strengths and limitations of the LSTM approach for your specific dataset and prediction goals.\n\nComparison with ARMA/ARIMA Models: The performance of the LSTM model should be compared to the ARMA/ARIMA models using the same dataset. If the LSTM model provides a better fit, as the graph suggests, it could be due to its ability to capture non-linear patterns and dependencies over multiple time steps that linear models like ARMA/ARIMA may miss. However, if the ARMA/ARIMA models perform comparably, they might be preferred due to their simplicity and lower computational cost.\n\nIn summary, the LSTM model seems to perform well on the Median Sale Price dataset, capturing the trend effectively with the help of regularization. The actual predictive horizon should be quantified with further analysis, and a direct comparison with ARMA/ARIMA models’ results would be needed to draw a concrete conclusion about relative performance."
  },
  {
    "objectID": "TS.html#no-regularization-1",
    "href": "TS.html#no-regularization-1",
    "title": "Deep Learning for TS",
    "section": "No Regularization",
    "text": "No Regularization\nLikewise, we will test the model prediction and errors with and without regularization for each model.\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n     \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_2\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 98ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 18ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00137 RMSE = 0.03706\nTest MSE = 0.05575 RMSE = 0.23612\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)"
  },
  {
    "objectID": "TS.html#with-regularization-1",
    "href": "TS.html#with-regularization-1",
    "title": "Deep Learning for TS",
    "section": "With Regularization",
    "text": "With Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Ensures a 1D array output\n\n# Check shapes again to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_3\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 180ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 30ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.27946 RMSE = 0.52864\nTest MSE = 1.46867 RMSE = 1.21189\n\n\n\n\nCode\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (train_rmse**2.0, train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (test_rmse**2.0, test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nTrain MSE = 0.27946 RMSE = 0.52864\nTest MSE = 1.46867 RMSE = 1.21189\n(174,) (6,)"
  },
  {
    "objectID": "TS.html#no-regularization-2",
    "href": "TS.html#no-regularization-2",
    "title": "Deep Learning for TS",
    "section": "NO Regularization",
    "text": "NO Regularization\nLikewise, we will test the model prediction and errors with and without regularization for each model.\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n      \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_4\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru (GRU)                   (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_4 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 283ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 23ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 19ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00003 RMSE = 0.00535\nTest MSE = 0.00281 RMSE = 0.05301\n\n\n\n\nCode\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)"
  },
  {
    "objectID": "TS.html#with-regularization-2",
    "href": "TS.html#with-regularization-2",
    "title": "Deep Learning for TS",
    "section": "With Regularization",
    "text": "With Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()  # Ensures a 1D array output\ntest_predict = model.predict(testX).flatten()\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nModel: \"sequential_5\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_1 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_5 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 244ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 19ms/step\n\n\nTrain MSE = 0.01689 RMSE = 0.12998\nTest MSE = 0.15742 RMSE = 0.39676\n\n\n\n\nCode\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n(174,) (6,)"
  },
  {
    "objectID": "TS.html#overall-effect-of-including-regularization",
    "href": "TS.html#overall-effect-of-including-regularization",
    "title": "Deep Learning for TS",
    "section": "Overall Effect of Including Regularization:",
    "text": "Overall Effect of Including Regularization:\nRegularization is used to prevent overfitting by penalizing large weights in the model. In my case, the LSTM performs the best. However, there are difference for using regularization. The results show that there are some overfitting if we do not use regularization. Therefore, in my case, the regularization is effective in preventing overfitting"
  },
  {
    "objectID": "TS.html#how-far-can-the-deep-learning-model-predict",
    "href": "TS.html#how-far-can-the-deep-learning-model-predict",
    "title": "Deep Learning for TS",
    "section": "How far can the Deep Learning Model predict",
    "text": "How far can the Deep Learning Model predict\nIt seems that the deep learning model did not preform too well for these mutivariables. The points are off with the actual values. However, the patterns and trends are captured. Therefore, I would say that for multivariable analysis, the deep learning methods might not be the most optimal choice. By adding additional dataset, the ability of predicting is not improved."
  },
  {
    "objectID": "TS.html#comparison-with-armaarima-models",
    "href": "TS.html#comparison-with-armaarima-models",
    "title": "Deep Learning for TS",
    "section": "Comparison with ARMA/ARIMA Models:",
    "text": "Comparison with ARMA/ARIMA Models:\nComparing the LSTM model with traditional ARMA/ARIMA models involves several considerations:\nPerformance Metrics: Compare the RMSE or other relevant metrics between the LSTM and ARMA/ARIMA models. Lower values indicate better predictive accuracy. Data Patterns: LSTM models are generally better at capturing complex, non-linear relationships and interactions in the data, while ARMA/ARIMA models are suited for linear relationships with well-defined autocorrelation structures. Computational Complexity: LSTM models can be more computationally intensive and require more data for training compared to ARMA/ARIMA models. Interpretability: ARMA/ARIMA models are often more interpretable in terms of understanding the influence of past values on future predictions. In summary, to answer your questions, you would need to conduct experiments that compare models with and without regularization, assess prediction accuracy over increasing future horizons, and compare the LSTM results with those obtained from ARMA/ARIMA models in previous homework. Each of these comparisons will provide insights into the strengths and limitations of the LSTM approach for your specific dataset and prediction goals.\n\nComparison with ARMA/ARIMA Models: The performance of the LSTM model should be compared to the ARMA/ARIMA models using the same dataset. If the LSTM model provides a better fit, as the graph suggests, it could be due to its ability to capture non-linear patterns and dependencies over multiple time steps that linear models like ARMA/ARIMA may miss. However, if the ARMA/ARIMA models perform comparably, they might be preferred due to their simplicity and lower computational cost.\n\nIn summary, the LSTM model seems to perform well on the Median Sale Price dataset, capturing the trend effectively with the help of regularization. The actual predictive horizon should be quantified with further analysis, and a direct comparison with ARMA/ARIMA models’ results would be needed to draw a concrete conclusion about relative performance."
  },
  {
    "objectID": "TS.html#data-preparation-1",
    "href": "TS.html#data-preparation-1",
    "title": "Deep Learning for TS",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\ndf = pd.read_csv(\"../Dataset/project/household_saving.csv\")\ndf = df.rename(columns={\"W398RC1A027NBEA\": \"y\"}) # The objective\ndf = df[[\"DATE\", \"y\"]]\nX = np.array(df[\"y\"].values.astype(\"float32\")).reshape(df.shape[0], 1)\n\n# Train and Test Split & Normalization\n\ndef train_test_split(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n   \n    split = int(n * split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\ntrain_data, test_data, data = train_test_split(X)\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps, plot_data_partition=False):\n    global X_ind, X, Y_ind, Y  # use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    # print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind = [*range(time_steps * rows_x)]\n    del X_ind[::time_steps]  # if time_steps=10 remove every 10th entry\n    X = dat[X_ind]\n\n    # PLOT\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y, \"o\", X_ind, X, \"-\")\n        plt.show()\n\n    # RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps - 1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n# PARTITION DATA\np = 3  #\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\n #USER PARAM\nrecurrent_hidden_units = 3\nepochs = 200\nf_batch = 0.2  # fraction used for batch size\noptimizer = \"RMSprop\"\nvalidation_split = 0.2\n# trainY=trainY.reshape(trainY.shape[0],1)\n# testY=testY.reshape(testY.shape[0],1)\nprint(\"Testing Array Shape:\", testX.shape, testY.shape)\nprint(\"Training Array Shape:\", trainX.shape, trainY.shape)\n\n\nTesting Array Shape: (1, 2, 1) (1,)\nTraining Array Shape: (7, 2, 1) (7,)\n\n\nAfter getting the corresponding train and test datasets, we can continue the process similarly."
  },
  {
    "objectID": "TS.html#lstm-1",
    "href": "TS.html#lstm-1",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nNo Regularization\n\n\nCode\nmodel = Sequential()\n\nmodel.add(\n    LSTM(\n        \n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n       \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\" Household Saving Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_6\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_2 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_6 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 374ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00669 RMSE = 0.08181\nTest MSE = 0.16973 RMSE = 0.41198\n(14,) (7,)\n\n\n\n\n\nThe RMSE for household saving analysis is about 0.41, we can see that the points predicted roughly the same from 1 to 15 time steps. Then it started to predict wrongly. Then, we will compare with regularization conducted to see if the result is overfitted.\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    LSTM(\n        # model.add(SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_7\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_3 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_7 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 331ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 26ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00511 RMSE = 0.07146\nTest MSE = 0.14327 RMSE = 0.37851\n(14,) (7,)\n\n\n\n\n\nFrom this result, we can see that the RMSE is actually lower. This means that in this case, the LSTM did not overfit for the household saving dataset. The possible reason could be the size of the dataset is relatively small which actually prevented the overfitting. However, I think that due to the small size of the dataset, the deep learning model is not as effective as the ARIMA models."
  },
  {
    "objectID": "TS.html#rnn-1",
    "href": "TS.html#rnn-1",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\nFor RNN, we do the same process.\n\nNo Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n     \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_8\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_2 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_8 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 89ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 13ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00400 RMSE = 0.06322\nTest MSE = 0.11479 RMSE = 0.33881\n(14,) (7,)\n\n\n\n\n\nFor RNN with no regularization, the result is higher compare to LSTM\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# Check shapes again to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_9\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_3 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_9 (Dense)             (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nShape of test_predict after flattening: (1,)\n\n\n\n\n\nTrain MSE = 0.00400 RMSE = 0.06322\nTest MSE = 0.11479 RMSE = 0.33881\n(14,) (7,)\n\n\n\n\n\nAgain, we can see that the RNN model with and without regularization yields similar results. I think the reason could be the same as LSTM, the dataset is relatively small such that there is no overfitting yet."
  },
  {
    "objectID": "TS.html#gru-1",
    "href": "TS.html#gru-1",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\nSame steps with GRU\n\nNo Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n      \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_10\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_2 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_10 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 215ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (1,)\nTrain MSE = 0.00211 RMSE = 0.04596\nTest MSE = 0.08885 RMSE = 0.29807\n(14,) (7,)\n\n\n\n\n\nWe can see that overall, GRU has the lowest RMSE for no regularization.\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()  # Ensures a 1D array output\ntest_predict = model.predict(testX).flatten()\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_11\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n gru_3 (GRU)                 (None, 3)                 54        \n\n\n                                                                 \n\n\n dense_11 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 58 (232.00 Byte)\n\n\nTrainable params: 58 (232.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 224ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\nTrain MSE = 0.00192 RMSE = 0.04377\nTest MSE = 0.08302 RMSE = 0.28813\n(14,) (7,)\n\n\n\n\n\nOverll, the GRU model performs the best for household savings analysis."
  },
  {
    "objectID": "TS.html#discussion",
    "href": "TS.html#discussion",
    "title": "Deep Learning for TS",
    "section": "Discussion",
    "text": "Discussion\nWe can see that comparing to Median Sale Price analysis, the analysis for household saving is not as effective. The reason is because of the dataset size different. With less datapoints, the models did not perform as well as they should be. We also can not determine the prediction power of models on this dataset. Therefore, in my case for household saving analysis, the deep learning methods are not as effective as the ARIMA models."
  },
  {
    "objectID": "TS.html#for-median-sale-price",
    "href": "TS.html#for-median-sale-price",
    "title": "Deep Learning for TS",
    "section": "For Median Sale Price",
    "text": "For Median Sale Price\n\nDeep learning methods comparison\nIn my case, the LSTM performed the best with lowest RMSE. In addition, the prediction plot with the actual data also shows that the LSTM is better among the three methods for my median sale price analysis.\n\n\nRegulariztaion Effect\nThe result suggests that the model is able to capture the underlying trend in the data quite well. Regularization comparsion proved that the model did not overfit too much. Also, the smoothness of the prediction curve indicates that the L2 regularization has effectively penalized overly complex models that could have fit the noise in the training data.\n\n\nHow Far can the deep learning methods predict\nIn my analsyis for median sale price, overall, the LSTM made relatively better predictions. The predictions started going wrong after 150 given steps. Therefore, I think that for deep learning methods, it can predict around half of year correctly.\n\n\nComparison to ARMA/ARIMA\nFor median sale price analsyis, the LSTM model performs the best with relatively low RMSE. Compare to the ARIMA model in ARIMA section, the model used for median sale price was ARIMA(1,1,1). And the RMSE is higher than the deep learning methods. Therefore, in my case, I believe that the LSTM model provides better predictions. The possible reason I think is that the deep learning models can evaluate the dataset by capturing more features. Especially that the dataset has an obvious upwarding trend. The deep learning model can interpret the trend better with each epoch trained therefore we can get better results. However, the overfitting could exist therefore regularization must be included to avoid overfitting."
  },
  {
    "objectID": "TS.html#for-household-saving",
    "href": "TS.html#for-household-saving",
    "title": "Deep Learning for TS",
    "section": "For Household Saving",
    "text": "For Household Saving\nWe can see that comparing to Median Sale Price analysis, the analysis for household saving is not as effective. The reason is because of the dataset size different. With less datapoints, the models for household saving did not perform as well as they should be. We also can not determine the prediction power of models on this dataset. Therefore, in my case for household saving analysis, the deep learning methods are not as effective as the ARIMA models.\n\nDeep learning methods comparison\nIn my case, the GRU performed the best with lowest RMSE. In addition, the prediction plot with the actual data also shows that the GRU is better among the three methods for my household saving analysis.\n\n\nRegularization Effect\nIn this case, the regularization actually did not change too much for the results. Regularization comparsion proved that the model did not overfit. Again, it could be due to the small size of the dataset.\n\n\nHow Far can the deep learning methods predict\nThe RMSE for household saving analysis is about 0.41, we can see that the points predicted roughly the same from 1 to 15 time steps. Then it started to predict wrongly. Again, it could be due to the small size of this dataset. It is hard to tell the exact prediction power. Overall, I do not think that deep learning methods are effective for household saving analysis.\n\n\nComparison to ARMA/ARIMA\nFor household saving analsyis, I think that due to the small size of the dataset, the deep learning model is not as effective as the ARIMA models. ARIMA model can capture better for this dataset by considering different patterns such as stationary and season. In my case, I believe that the ARIMA model performs better. For deep learning methods, although the results have smaller RMSEs, I still think that the analysis could be improved by using larger datasets."
  },
  {
    "objectID": "TS.html#data-preparation-2",
    "href": "TS.html#data-preparation-2",
    "title": "Deep Learning for TS",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this code part, I also included datasets for mutivariable analysis in later section\n\n\nCode\ndf = pd.read_csv(\"../Dataset/project/A191RI1Q225SBEA.csv\")\ndf2 = pd.read_csv(\"../Dataset/project/MSPUS.csv\")\ndf3 = pd.read_csv(\"../Dataset/project/PSAVERT.csv\")\n\ndf = df.rename(columns={\"A191RI1Q225SBEA\": \"y\"}) # The objective\ndf = df[[\"DATE\", \"y\"]]\nX = np.array(df[\"y\"].values.astype(\"float32\")).reshape(df.shape[0], 1)\n\n# Train and Test Split & Normalization\n\ndef train_test_split(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n   \n    split = int(n * split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\ntrain_data, test_data, data = train_test_split(X)\n\nprint(\"train shape:\", train_data.shape)\nprint(\"test shape:\", test_data.shape)\n\nfig, ax = plt.subplots(figsize=(15, 6), dpi=100)  # Set the size and DPI of the figure\nfig.patch.set_facecolor('#E0E0E0')  # Set the background color for the outer figure\nax.set_facecolor('#E0E0E0')  # Set the background color for the axes\n\n# Plot the training data\nax.plot(range(0, len(train_data)), train_data, \"-\", label=\"Training Data\")\n\n# Plot the test data\nax.plot(range(len(train_data), len(train_data) + len(test_data)), test_data, \"-\", label=\"Test Data\")\n\n# Set labels and title\nax.set(xlabel=\"Time (days)\", ylabel=\"Median GDP Deflator Scaled\", title=\"Median GDP Deflator Over Time\")\n\n# Add grid with white color for better visibility on the gray background\nax.grid(color='white')\n\n# Add legend to the plot\nax.legend()\n\n# Show the plot\nplt.show()\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps, plot_data_partition=False):\n    global X_ind, X, Y_ind, Y  # use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    # print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind = [*range(time_steps * rows_x)]\n    del X_ind[::time_steps]  # if time_steps=10 remove every 10th entry\n    X = dat[X_ind]\n\n    # PLOT\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y, \"o\", X_ind, X, \"-\")\n        plt.show()\n\n    # RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps - 1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n# PARTITION DATA\np = 30  #\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n #USER PARAM\nrecurrent_hidden_units = 3\nepochs = 200\nf_batch = 0.2  # fraction used for batch size\noptimizer = \"RMSprop\"\nvalidation_split = 0.2\n\nprint(\"Testing Array Shape:\", testX.shape, testY.shape)\nprint(\"Training Array Shape:\", trainX.shape, trainY.shape)\n\n\ntrain shape: (244,)\ntest shape: (61,)\n\n\n\n\n\nTesting Array Shape: (2, 29, 1) (2,)\nTraining Array Shape: (8, 29, 1) (8,)"
  },
  {
    "objectID": "TS.html#lstm-2",
    "href": "TS.html#lstm-2",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\nSimilar Process as the above sections\n\nNo Regularization\n\n\nCode\nmodel = Sequential()\n\nmodel.add(\n    LSTM(\n        \n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n       \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=epochs,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')  # Set higher DPI and background color for the figure\n    plt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\n    # ORIGINAL DATA\n    print(X.shape, Y.shape)\n    plt.plot(Y_ind, Y, \"o\", label=\"Target\")\n    plt.plot(X_ind, X, \".\", label=\"Training points\")\n    plt.plot(Y_ind, train_predict, \"b.\", label=\"Prediction\")\n    plt.plot(Y_ind, train_predict, \"r-\")\n    plt.legend()\n    plt.xlabel(\"Observation number after given time steps\")\n    plt.ylabel(\"Median GDP Deflator Scaled\")\n    plt.title(\"Actual and Predicted Values\")\n    plt.grid(color='white')  # Set grid color to white for better visibility\n    plt.show()\n\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_12\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_4 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_12 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 238ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (2,)\nTrain MSE = 0.00087 RMSE = 0.02954\nTest MSE = 0.02076 RMSE = 0.14407\n(232,) (8,)\n\n\n\n\n\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    LSTM(\n        # model.add(SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_13\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_5 (LSTM)               (None, 3)                 60        \n\n\n                                                                 \n\n\n dense_13 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 64 (256.00 Byte)\n\n\nTrainable params: 64 (256.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 233ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\nShape of test_predict after flattening: (2,)\nTrain MSE = 0.00082 RMSE = 0.02872\nTest MSE = 0.01277 RMSE = 0.11302\n(232,) (8,)"
  },
  {
    "objectID": "TS.html#rnn-2",
    "href": "TS.html#rnn-2",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\n\nNo Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n     \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n\n\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_14\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_4 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_14 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 92ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nTrain MSE = 0.00226 RMSE = 0.04756\nTest MSE = 0.03703 RMSE = 0.19243\n(232,) (8,)\n\n\n\n\n\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Ensures a 1D array output\n\n# Check shapes again to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (train_rmse**2.0, train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (test_rmse**2.0, test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_15\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n simple_rnn_5 (SimpleRNN)    (None, 3)                 15        \n\n\n                                                                 \n\n\n dense_15 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 19 (76.00 Byte)\n\n\nTrainable params: 19 (76.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 92ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\nShape of test_predict after flattening: (2,)\nTrain MSE = 0.76253 RMSE = 0.87323\nTest MSE = 0.25970 RMSE = 0.50961\n\n\n(232,) (8,)"
  },
  {
    "objectID": "TS.html#gru-2",
    "href": "TS.html#gru-2",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\nNo Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n      \n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 233ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 18ms/step\n\n\nTrain MSE = 0.01187 RMSE = 0.10895\nTest MSE = 0.00009 RMSE = 0.00930\n(232,) (8,)\n\n\n\n\n\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        # recurrent_dropout=0.8,\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()  # Ensures a 1D array output\ntest_predict = model.predict(testX).flatten()\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n# Call the function with your data\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 227ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nTrain MSE = 0.31959 RMSE = 0.56533\nTest MSE = 0.22049 RMSE = 0.46957\n(232,) (8,)"
  },
  {
    "objectID": "TS.html#for-gdp-deflator",
    "href": "TS.html#for-gdp-deflator",
    "title": "Deep Learning for TS",
    "section": "For GDP Deflator",
    "text": "For GDP Deflator\n\nDeep learning methods comparison\nIn my case, the LSTM performed the best with lowest RMSE. In addition, the prediction plot with the actual data also shows that the LSTM is better among the three methods for GDP Deflator analysis.\n\n\nRegularization Effect\nFor LSTM, the regularization actually did not give in too much difference in the results. However, for RNN and GRU, the result suggests that the model is overfitting. The RMSE for both model is different when appling regularization. Therefore, regularization help these two models for not overfitting.\n\n\nHow Far can the deep learning methods predict\nIn my analsyis for gdp deflator, overall, the LSTM made relatively better predictions. The predictions aligned the actual points for all time steps. This imply that the LSTM model has very effective prediction power for the gdp deflator. Therefore, I think it can predict the values correctly given the trend and pattern for pretty long time in to the future if there is no sudden change or other external impacts.\n\n\nComparison to VAR Model\nThe LSTM model performs the best with relatively low RMSE. Compare to the VAR model in VAR section, the model used for GDP Deflator was Var(13). For Var(13) The RMSE for cross validation is also relatively small. However, in my case, I still believe that the LSTM model provides better predictions. The possible reason I think is that the deep learning models can evaluate the dataset by capturing more features with deeper analysis. The deep learning model can interpret the trend better with each epoch trained therefore we can get better results. In addition, for LSTM the overfitting did not exist. However, for other model, the RNN and GRU are not as effective as the VAR model."
  },
  {
    "objectID": "TS.html#data-preparation-gdp-deflator-personal-saving-rate-median-sale-price",
    "href": "TS.html#data-preparation-gdp-deflator-personal-saving-rate-median-sale-price",
    "title": "Deep Learning for TS",
    "section": "Data Preparation: GDP Deflator, Personal Saving Rate, Median Sale Price",
    "text": "Data Preparation: GDP Deflator, Personal Saving Rate, Median Sale Price\n\n\nCode\ndf['DATE'] = pd.to_datetime(df['DATE'])\ndf2['DATE'] = pd.to_datetime(df2['DATE'])\ndf3['DATE'] = pd.to_datetime(df3['DATE'])\n\n\n# Filter data based on the date range\nstart_date = '1963-01-01'\nend_date = '2023-04-01'\ndf1 = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\ndf2 = df2[(df2['DATE'] >= start_date) & (df2['DATE'] <= end_date)]\ndf3 = df3[(df3['DATE'] >= start_date) & (df3['DATE'] <= end_date)]\n\n# Merge the dataframes on the 'DATE' column\ndf = pd.merge(df1, df2, on='DATE', how='inner')\ndf = pd.merge(df, df3, on='DATE', how='inner')\n\ndf.head()\n\n\n\n\n# Train/Test Split & Normalization\ndef train_test_split_normalize(data, split_percent=0.8):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data_scaled = scaler.fit_transform(data)\n    n = len(data_scaled)\n   \n    split = int(n * split_percent)\n    train_data = data_scaled[:split, :]\n    test_data = data_scaled[split:, :]\n    return train_data, test_data, data_scaled\n\n# Assuming the data from the CSVs are now in df\ntrain_data, test_data, _ = train_test_split_normalize(df.iloc[:, 1:].values)  # Exclude 'DATE' column\n\n# Prepare input X and target Y for multivariate time-series\ndef get_XY_multivariate(dat, time_steps, num_features):\n    X, Y = [], []\n    for i in range(len(dat) - time_steps):\n        X.append(dat[i:(i + time_steps), :])\n        Y.append(dat[i + time_steps, 0])  # Assuming target is the first feature\n    return np.array(X), np.array(Y)\n\n# Partition Data\np = 30  # Time steps\nnum_features = df.shape[1] - 1  # Number of features, excluding 'DATE' column\ntrainX, trainY = get_XY_multivariate(train_data, p, num_features)\ntestX, testY = get_XY_multivariate(test_data, p, num_features)\n\n# Display shapes\nprint(\"Testing Array Shape:\", testX.shape, testY.shape)\nprint(\"Training Array Shape:\", trainX.shape, trainY.shape)\n\nfig, ax = plt.subplots(figsize=(15, 6), dpi=100)  # Set the size and DPI of the figure\nfig.patch.set_facecolor('#E0E0E0')  # Set the background color for the outer figure\nax.set_facecolor('#E0E0E0')  # Set the background color for the axes\n\n# Plot the training data for each time-series\nax.plot(range(0, len(train_data)), train_data[:, 0], \"-\", label=\"Training Data - TS1\")\nax.plot(range(0, len(train_data)), train_data[:, 1], \"-\", label=\"Training Data - TS2\")\nax.plot(range(0, len(train_data)), train_data[:, 2], \"-\", label=\"Training Data - TS3\")\n\n# Plot the test data for each time-series\nax.plot(range(len(train_data), len(train_data) + len(test_data)), test_data[:, 0], \"-\", label=\"Test Data - TS1\")\nax.plot(range(len(train_data), len(train_data) + len(test_data)), test_data[:, 1], \"-\", label=\"Test Data - TS2\")\nax.plot(range(len(train_data), len(train_data) + len(test_data)), test_data[:, 2], \"-\", label=\"Test Data - TS3\")\n\n# Set labels and title\nax.set(xlabel=\"Time (days)\", ylabel=\"Values Scaled\", title=\"Multivariate Time Series Over Time\")\n\n# Add grid with white color for better visibility on the gray background\nax.grid(color='white')\n\n# Add legend to the plot\nax.legend()\n\n# Show the plot\nplt.show()\n\n\nTesting Array Shape: (19, 30, 3) (19,)\nTraining Array Shape: (163, 30, 3) (163,)"
  },
  {
    "objectID": "TS.html#array-preparations-for-modeling",
    "href": "TS.html#array-preparations-for-modeling",
    "title": "Deep Learning for TS",
    "section": "Array Preparations for Modeling",
    "text": "Array Preparations for Modeling\n\n\nCode\ndef get_XY_multivariate(dat, time_steps, num_features, plot_data_partition=False):\n    X, Y = [], []\n\n    # Prepare X and Y\n    for i in range(len(dat) - time_steps):\n        X.append(dat[i:i + time_steps, :])\n        Y.append(dat[i + time_steps, 0])  # Assuming target is the first feature at the next time step\n\n    # Convert to numpy arrays\n    X = np.array(X)\n    Y = np.array(Y)\n\n    if plot_data_partition:\n        plt.figure(figsize=(15, 6), dpi=80)\n        for i in range(num_features):\n            plt.plot(Y_ind, dat[Y_ind, i], \"o\", label=f\"Feature {i+1} Targets\")\n            plt.plot(range(len(dat)), dat[:, i], \"-\", label=f\"Feature {i+1} Data\")\n        plt.legend()\n        plt.show()\n\n    return X, Y\n\n# Use this function to partition data again\ntestX, testY = get_XY_multivariate(test_data, p, num_features)\ntrainX, trainY = get_XY_multivariate(train_data, p, num_features)\n\n# Print shapes\nprint(\"Testing Array Shape:\", testX.shape, testY.shape)\nprint(\"Training Array Shape:\", trainX.shape, trainY.shape)\n #USER PARAM\nrecurrent_hidden_units = 3\nepochs = 200\nf_batch = 0.2  # fraction used for batch size\noptimizer = \"RMSprop\"\nvalidation_split = 0.2\n\n\nTesting Array Shape: (19, 30, 3) (19,)\nTraining Array Shape: (163, 30, 3) (163,)"
  },
  {
    "objectID": "TS.html#lstm-3",
    "href": "TS.html#lstm-3",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nNo Regularization\n\n\nCode\nmodel = Sequential()\n\nmodel.add(\n    LSTM(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\nmodel.summary()\n# TRAIN MODEL\nhistory = model.fit(\n    trainX,\n    trainY,\n    epochs=200,\n    batch_size=int(f_batch * trainX.shape[0]),\n    validation_split=validation_split,\n    verbose=0,\n)\n\n# MAKE PREDICTIONS\ntrain_predict = model.predict(trainX).squeeze()\ntest_predict = model.predict(testX).squeeze()\n\n\nimport matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(facecolor='#E0E0E0', dpi=200)  # Set the background color and DPI\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'c', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.gca().set_facecolor('#E0E0E0')  # Set the axes background color\nplt.grid(color='white')  # Set the grid color to white for better visibility on the gray background\nplt.show()\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\ndef plot_result(trainY, testY, train_predict, test_predict):\n    plt.figure(figsize=(15, 6), dpi=200, facecolor='#E0E0E0')\n    plt.gca().set_facecolor('#E0E0E0')\n\n    # Create time indices for training and testing data\n    time_indices_train = range(len(trainY))\n    time_indices_test = range(len(testY))\n\n    # Plotting actual values vs. predictions for training data\n    plt.subplot(1, 2, 1)\n    plt.plot(time_indices_train, trainY, \"o\", label=\"Actual Train\")\n    plt.plot(time_indices_train, train_predict, \"r-\", label=\"Predicted Train\")\n    plt.title(\"Training: Actual vs Predicted\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Scaled Value\")\n    plt.legend()\n\n    # Plotting actual values vs. predictions for testing data\n    plt.subplot(1, 2, 2)\n    plt.plot(time_indices_test, testY, \"o\", label=\"Actual Test\")\n    plt.plot(time_indices_test, test_predict, \"r-\", label=\"Predicted Test\")\n    plt.title(\"Testing: Actual vs Predicted\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Scaled Value\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Calling the function\nplot_result(trainY, testY, train_predict, test_predict)\n\n\nModel: \"sequential_18\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_6 (LSTM)               (None, 3)                 84        \n\n\n                                                                 \n\n\n dense_18 (Dense)            (None, 1)                 4         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 88 (352.00 Byte)\n\n\nTrainable params: 88 (352.00 Byte)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\n1/6 [====>.........................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 2ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\n\n\n\n1/6 [====>.........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 2ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\nTrain MSE = 0.00729 RMSE = 0.08540\nTest MSE = 0.11003 RMSE = 0.33170\n\n\n\n\n\n\n\nWith Regularization\n\n\nCode\nmodel = Sequential()\n\nmodel.add(\n    LSTM(\n        \n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\n\n\n\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n\n\n\n# Calling the function\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/6 [====>.........................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 2ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nTrain MSE = 0.15387 RMSE = 0.39226\nTest MSE = 0.07804 RMSE = 0.27936"
  },
  {
    "objectID": "TS.html#gru-3",
    "href": "TS.html#gru-3",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\nNo Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n# Calling the function\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/6 [====>.........................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 3ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nTrain MSE = 0.16875 RMSE = 0.41079\nTest MSE = 0.16586 RMSE = 0.40726\n\n\n\n\n\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\n# Make predictions\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\n# Calling the function\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/6 [====>.........................] - ETA: 1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 3ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 21ms/step\n\n\nTrain MSE = 0.15670 RMSE = 0.39585\nTest MSE = 0.48956 RMSE = 0.69968"
  },
  {
    "objectID": "TS.html#rnn-3",
    "href": "TS.html#rnn-3",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/6 [====>.........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 2ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n\n\nShape of test_predict after flattening: (19,)\nTrain MSE = 0.25535 RMSE = 0.50532\nTest MSE = 0.17439 RMSE = 0.41760\n\n\n\n\n\n\nWith Regularization\n\n\nCode\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(\n    SimpleRNN(\n        # model.add(GRU(\n        recurrent_hidden_units,\n        return_sequences=False,\n        input_shape=(trainX.shape[1], trainX.shape[2]),\n        recurrent_regularizer=regularizers.L2(1e-2),\n        activation=\"tanh\",\n    )\n)\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation=\"linear\"))\n\n# COMPILE THE MODEL\nmodel.compile(loss=\"MeanSquaredError\", optimizer=optimizer)\ntrain_predict = model.predict(trainX).flatten()\ntest_predict = model.predict(testX).flatten()  # Flattening to ensure it is a 1D array\n\n# Now, print the shapes to verify\nprint(\"Shape of test_predict after flattening:\", test_predict.shape)\n\n# Compute RMSE\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\n# Print MSE and RMSE\nprint(\"Train MSE = %.5f RMSE = %.5f\" % (np.mean((trainY - train_predict) ** 2.0), train_rmse))\nprint(\"Test MSE = %.5f RMSE = %.5f\" % (np.mean((testY - test_predict) ** 2.0), test_rmse))\nplot_result(trainY, testY, train_predict, test_predict)\n\n\n1/6 [====>.........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 1ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 15ms/step\n\n\nShape of test_predict after flattening: (19,)\nTrain MSE = 0.06878 RMSE = 0.26225\nTest MSE = 0.11937 RMSE = 0.34551"
  }
]