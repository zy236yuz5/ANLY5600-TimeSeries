[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ABOUT ME",
    "section": "",
    "text": "Author: Zonghong Yu\nNetID: zy236\nEmail: zy236@georgetown.edu"
  },
  {
    "objectID": "ARCH/GARCH.html",
    "href": "ARCH/GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARCH/TS.html",
    "href": "ARCH/TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "ARModels.html",
    "href": "ARModels.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "After Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation.\nTo be more specific, we need to understand the concepts about the models: Certainly! Here’s a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:"
  },
  {
    "objectID": "ASV.html",
    "href": "ASV.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "Discription:"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "DataSources.html",
    "href": "DataSources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Data Gathering\n\n\nData Discription & Source\nIn this section, different datasets are gained to help provide different analysis in later sections. Each dataset serves to answer questions that are defined in introduction section. To be more specifc, there are datasets regarding the household savings, incomes, sale prices, Gini idex, and housing affordability which can help us analysis the impact of the income disparities and housing affordability throughout USA. The detialed description of each dataset is provided.\n\n1. Household Savings\n\nLink: Household Saving Dataset\nDescription: This dataset contains information related to household savings in the United States throughout the years from 1992 to 2021. It includes data information on the saving, which can be an important economic indicator for me to make analysis. The Units for dataset are Billions of Dollars, Not Seasonally Adjusted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Real Median Household Income in the United States\n\nLink: Real Median Household Income Dataset\nDescription: This dataset represents the real median household income dataset. This is an estimation of Median Incomes. The Census Bureau has computed medians using either Pareto interpolation or linear interpolation. Currently, we are using linear interpolation to estimate all medians. Pareto interpolation assumes a decreasing density of population within an income interval, whereas linear interpolation assumes a constant density of population within an income interval. The Census Bureau calculated estimates of median income and associated standard errors for 1979 through 1987 using Pareto interpolation if the estimate was larger than $20,000 for people or $40,000 for families and households. This is because the width of the income interval containing the estimate is greater than $2,500.\n\n\n\n\n\n\n\n\n\n\n\n3. Median Sales Price of Houses Sold for the United States\n\nLink: Median Sales Price of Houses Dataset\nDescription: This dataset contains information on the median sales price of houses sold in the United States. It can be a valuable indicator of the state of the real estate market.\nThis dataset can provide me with a direct view about the sale price of the house in USA. We can see clearly about the price disparities. Then, by comparing with the income and saving, we can know about the patterns regarding the impact of disparities.\n\n\n\n\n\n\n\n\n\n\n\n4. GINI Index for the United States\n\nLink: GINI Index Dataset\nDescription: This dataset shows the Gini index for United States. Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. In addition, a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nData are based on primary household survey data obtained from government statistical agencies and World Bank country departments.\nWorld Bank collection of development indicators, compiled from officially recognized international sources. It presents the most current and accurate global development data available and includes national, regional, and global estimates. The World Bank labels these annual series, but several observations are missing..\n\n Source Indicator: SI.POV.GINI\n\n\n5. Housing Affordability Index\n\nLink: Housing Affordability Index Dataset\nDescription: This dataset contains data on the Housing Affordability Index. The housing affordability index measures the degree to which a typical family can afford the monthly mortgage payments on a typical home. Value of 100 means that a family with the median income has exactly enough income to qualify for a mortgage on a median-priced home. An index above 100 signifies that family earning the median income has more than enough income to qualify for a mortgage loan on a median-priced home, assuming a 20 percent down payment. This index is calculated for fixed mortgages.\nThis dataset can directly tell us if one can easily afford a house. This can help me make analysis the resources disparities and the possible reasons for the differences in housing affordability\n\n\n\n\n\n\n\n\n\n\n\n6. Personal Saving Rate\n\nLink: Personal Saving Rate Dataset\nDescription: This dataset is to provide information on the personal saving rate. The personal saving rate measures the percentage of disposable income that individuals save, which can be indicative of financial health. In here, personal saving as a percentage of disposable personal income (DPI), frequently referred to as “the personal saving rate,” is calculated as the ratio of personal saving to DPI.\nFrom this dataset, I can evaluate the overall changes throughout years regarding the personal savings. Then, by comparing with the income and house prices, we can see if there are any patterns or relationships.\n\n\n\n\n\n\n\n\n\n\n\n7. Gross Domestic Product: Implicit Price Deflator\n\nLink: GDP: Implicit Price Deflator Dataset\nDescription: This dataset represents the Gross Domestic Product (GDP) Implicit Price Deflator for USA, which is a measure of inflation in the economy and is used to adjust GDP for price changes.\nThis dataset aims to cover the overall GDP changes quetsions that can be utilized to make analysis on the impact of the incomes and housing prices.\n\n\n\n\n8. All Home Prices in States in the USA\n\nLink: Zillow Home Prices Dataset\nDescription: This dataset provides information on home prices in various states in the United States. It is sourced from Zillow, a well-known real estate and rental marketplace.\nThese housing datasets (from zillow group) not only provide sufficient information concerning the house values and sales prices, but also include the different regions. I wish to compare different states or regions to find if there are any patterns in house values, sales prices, and rental prices. I want to find correlations among them. In addition, some of the datasets cover a great range of time even from 2000 to 2023. This can provide with a broader view of the dataset when making visualizations across the time. We can see clearly how the housing values and sales prices changed throughout the years and make forecasts for the future.\n\nA Screen shot for the dataset: \n\n\n\nReference:\n\nCodes: Rmd, Python & Qmd\nU.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.\nU.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.\nU.S. Census Bureau and U.S. Department of Housing and Urban Development, Median Sales Price of Houses Sold for the United States [MSPUS], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MSPUS, September 19, 2023.\nWorld Bank, GINI Index for the United States [SIPOVGINIUSA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/SIPOVGINIUSA, September 19, 2023.\nNational Association of Realtors, Housing Affordability Index (Fixed) [FIXHAI], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/FIXHAI, September 19, 2023.\nU.S. Bureau of Economic Analysis, Personal Saving Rate [PSAVERT], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/PSAVERT, September 19, 2023.\nU.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.\nZillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/."
  },
  {
    "objectID": "DataVis.html",
    "href": "DataVis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we will explore the datasets that we created by using visualizations. The goal is to gain insights and ideas before applying models and analysis. The data visualization and storytelling are very crucial for us to understand the impact of the income and houseing prices disparities. In this section, by utilizing different visualizations with different tools, we endeavor to delve into the depths of the datasets, finding potential trends, seasonality, and other features.\n \n\n\nFirst of all, we need to have a basic look at for the household income, household savings, and the sale price of the house. By putting the figures together, we can have a more direct idea about the trending and oscciliation throughout the time. Through interactive tableau visualization, we can see the temporal trends and oscillations that may be important for our analysis.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the above plots, we can see that the household savings throughout the years generally follow an upwarding trend, which means that more and more people are willingly to save moneny. For Median household income, however, we can see that from 1984 to 2021, it has a lot of osccilations. Bascilly, the median household income did not change too much. And then! for sale prices of the houses, we can see that it follows a positive trend as well.\nFrom here, we can have a basic insight: while sale price increases, the median household income still remains the same throughout the years. This give us questions, why? And if this is the case, can people nowadays afford the house? In order to answer these quetsions, we need to dive into other datasets as well.      \n\n\n\nFirst, we need to have a look at of the GDP as a whole. Since GDP represent economic evaluations, which can offer us invaluable insights into the financial health and trajectory of a nation. With the GDP Implicit Price Deflator data, we aim to have insights on the potential reason of the income and sale price of house. The Deflator helps to discern between changes in GDP due to alterations in prices and changes due to alterations in quantities.\n\nBased on the interactive plotly plot, we can see the percent change from preceding period. Generally from 1960 to 1980, it follows an upwarding trend which aligns with the household savings and sale prices. Then from 1980 to 2020, it follows a downwarding trend and appears to have fluctuations from 2020 to now. From 2020, the Covid-19 surely had huge impact of the general economic. However, the decreasing of the percent change from 1980 to 2020 indicates that the percent changes are generally small, which means that for these years the GDP remains roughly the same, which aligns with the incomes from previous figure.\nThis also shows that while the GDP did not change too much, the household median incomes did not change as well. However, the sale prices and savings keep increasing. In addition, as we mentioned from introduction, the other important aspect that we want to explore is about the disparity. From the visulizations, although we see that the income keep increasing, there are still disparities in distribution of the income.      \n\n\n\nTo have a brief recap, Gini index measures the extent to which the distribution of income or consumption expenditure among individuals or households within an economy deviates from a perfectly equal distribution. A Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.\nAs we all aware, the disparity between geographic area exists. The Urban areas, especially metropolitan regions, are often have more economic. In this section, we want to find the extents of disparities and correlations between the income and other factors. From exploring the Gini coefficient, we can gain insights with income disparities, housing affordability as well as the trend and seasonality.\n\nBased on the GINI index over time, we can see that it follows an upwarding trend. Which is not a good thing, this means that we are towarding more inequality throughout the time. This means that the distribution of income within an economy deviates is not equal. We can see that from 1975 to 2019, the GINI index has increased about 8. However, from 2020, it seems that we have a decline of the GINI idex. Again, the Covid also had impacts of the generaly distribution of the incomes.\nFrom here, we can see that while the median income remains roughly the same, the distribution of the income did not remain constant. Which means that while many people gaining wealth, there are still more and more proverties. This can also imply that for a part of the people, they have got more resources while the others have troubles with housing affordability. To be more specific, we need to have a look at about the relationship between personal saving rate and the house price.      \n\n\n\nIn this section, we have a focused on understanding how personal savings rates interface with average sales prices of houses in the USA. From here, we aim to identify the trend and pattern such that we can see the direct impact of the relationship between saving rate and the sale price. We can understand how changes in saving rates might reflect or impact shifts in housing market trends.\n\n\n\n\n\n\n   \n\n\n\n\nBased on the average personal saving rate and the average sales price of the houses, we can see that the personal saving rate fluctuates throughout the years while the average sales price follows an upwarding trend. The increasing of the house prices may be attributed to factors such as population growth, urbanization. However, as population grows, the saving rate did not change too much. Again, this could also means that the disparities in geographic locations play an important role.      \n\n\n\nTaking a step further into our exploration, this section presents a bar visualization, providing a categorical breakdown of pivotal economic indicators and metrics. Our goal is to compare, contrast, and evaluate different states and MSA, identifying links and disparities among districts.\nNow, we wish to have a more specific look at about the disparities among regions. Here, we have selected some Meyropolitan Areas with states and MSAs. In here, we can see that which states or regions generally have a higher prices. Which can help us find the disparities.\n\n\nBased on the linked figure, we can see that states for CA, CO, and HI generally have higher sale prices of houses. This means that the Geographic Disparities in Housing Markets did exist throughout the time. From 2018 to 2023, most of the places also have an upwarding trend in sale prices. This can make the housing affordability to change as well.\nIn addition, the real estate landscapes of CA, CO, and HI have been prospered among economists. The increasing in housing prices in these regions indicates several factors and disparities:\nHigh demand in states like CA and HI, may be the cause of increasing prices. In addition, the economic landscapes of these states, often associated with sectors like advanced technology and high tourism that can boost the economy which makes the prices higher. However the average incomes did not change too much. This could bring affordability crisis in the future."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For Exploratory Data Analysis (EDA), we aim to have a deeper understanding of the time series data. This section involves multiple medthods and datasets for us to gain insights and understandings such as decompositions, lag plots, ACF, PACF, ADF tests, detrending, and others. For these methods, we are going to identify correlations, trends, seasonalities, and stationaries. Which can help us to make analysis and apply models in later sections. To be more specific, the lag plots and decomposing methods will allow us to discover dependencies and components. ACF and PACF can help us see the correlations and stationary, and the Augmented Dickey-Fuller Test to empirically probe its stationarity."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The Impact of Income Disparities and Housing Affordability in the United States\n\nProject Backgroung:\nWith the dramatic development of the technology and society, we have witnessed the continuous increasing in economies. However, such developments did not spread through every corner in USA. The disparities of resources still exist. There are housing price disparities, rental sales disparities, and income disparities for different regions and states. These differents play a very crucial role in our society and they bring huge impact and massive pressure to people nowadays.\nThe central focus of this project lies in understanding the relationship between the income disparities between and housing affordability in the United States. I aim to help us understand and gain new insights on how varying income levels within society directly affect the ability of individuals and families to secure housing that is both suitable and affordable. Throughout the analysis, different smaller aspects and factors will be analyzed one by one. \n\n\nThe Big Picture & Literature Review:\nIncome disparity and housing affordability are not just matters of individual but also broader societal concerns that need to be attended to. The influences of such disparities and results are not only the quality of life for countless Americans citizens but also the overall stability of communities and the economic health of the nation. As I help us explore this issue, we must recognize that it’s not simply about monetary figures; it’s about the pursuit of a fundamental human need: shelter and how income inequality, housing price, rental price, GDP, personal saveing and regional difference influence us as a whole in the United States.\nIn order to bring a more comprehensive understanding of this topic, we must delve into the body of research and discourse that precedes us. Existing literature reveals the persistent challenge of housing affordability, marked by the growing gap between incomes and housing costs.(Jajtner et al., 2020). Scholars have examined the historical trends, policy interventions, and socioeconomic factors. In this exploration of the impact of income disparities on housing affordability, we will adopt multiple analytical angles to provide a well-rounded perspective.\n\nMutiple datasets are analyzed thoroughly to provide insights and conclusions. Different methods such as Exploratory Data Analysis, ARMA Models, ARIMAX Models, Spectral Analysis and Filtering,, Financial Time Series Models, and Deep Learning methods are conducted to help me achieve the goal.\n\n\nGuiding Questions that need to be answered:\nTo guide the explorations, we have conducted some questions that would be answered using different analysis and dataset:\n\nHow have income inequality trends in the United States impacted the individuals?\nWhat are the historical trends in housing prices?\nWhat is the definition of housing affordability, and what metrics are commonly used to assess it?\nHow do income disparities vary from one region to another?\nWhat regions or states have greater disparities?\nWhat impact have the economy had on the housing market?\nIs there any trends or seasonal patterns regarding the income and housing price?\nIn what ways does housing affordability intersect with broader economic and social issues, such as workforce mobility and education outcomes?\nWhat innovative solutions and strategies can be employed to improve housing affordability for all Americans?\nWhat are the correlations between the sale price, housing price, and rental price?\nWhat future trends should we anticipate in the context of income disparities and housing affordability?\nHow can addressing income disparities lead to a more equitable and sustainable housing market in the United States?\n\nThese guiding questions will serve as different milestones throughout this exploration. The questions might be changed and increased as the project proceeds."
  },
  {
    "objectID": "SAF.html",
    "href": "SAF.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "GARCH.html",
    "href": "GARCH.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "TS.html",
    "href": "TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Work in prorgess"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Author: Zonghong Yu"
  },
  {
    "objectID": "dv.html#question-2",
    "href": "dv.html#question-2",
    "title": "Data Vizes in TS",
    "section": "Question 2",
    "text": "Question 2\n\n1. Try to reproduce the Data Vizes similar to in Lab0/1 “Data Viz Examples” but with a different set of stock prices. (use the quantmod package to get stock prices from yahoo finance https://finance.yahoo.com/lookup/)\n\na.\nIn here, I utilize three different set of stock prices: GOOGL, MSFT - Microsoft Corporation, and NFLX - Netflix Inc. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretations: The three stocks as showned in figure above represent:Google, MicroSoft, and Netflix. We can see that in general, Netflix had an overall highiest stock prices throughout the years. And Google has the least over the years. However, all three of them had fluctuations especially in year 2022, which could be the impact of the Covid-19.\n\n\nb. Plot the climate data (climate.csv) using plotly. https://plotly.com/r/\n\n\nInterpretations: By using the plotly on the climate dataset provided, we can see some interesting features. From above interaction figure, we can see the relation between the Date and the max temperature. We can see that during July and August in 2021, the max temperature is higher in general while lower during October and December which is reasonable.\n\n\nc. Get any economic data / macroeconomic indicators and plot using plotly https://fred.stlouisfed.org/ https://www.bea.gov/\nI utilized the GEPUCURRENT dataset, which is Global Economic Policy Uncertainty Index: Current Price Adjusted GDP, from https://fred.stlouisfed.org/series/GEPUCURRENT\n\n\nInterpretations: By using the plotly on the economic dataset from the website givem, I chose the GDP index of some global economic policy. From above interaction figure, we can see the relation between the Date and the index. We can see that the index follows an upwarding trend throughout the years. However, there are many local maximums and fluctuations throughout the years. In addition, the idex reached at its peak on 2020/05/01.\n\n\n\n2. Make only the plots visible in your webpage. (set echo=FALSE in your R code chunck)\nOnly plots visible as required.\n\n\n3. Add interpretations to all the plots in the webpage.\nAdded as required."
  },
  {
    "objectID": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "about.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "ABOUT ME",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "href": "index.html#major-subjects-that-zonghong-interests-in-or-wants-to-explore",
    "title": "Time Series",
    "section": "Major Subjects that Zonghong interests in or wants to explore:",
    "text": "Major Subjects that Zonghong interests in or wants to explore:\n\nPython, RStudio\nData Science, Math, Time Series, Programming\nFinance, Quantitative Engineering\nData Visulization"
  },
  {
    "objectID": "EDA.html#the-gdp-exploratory-data-analysis",
    "href": "EDA.html#the-gdp-exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The GDP Exploratory Data Analysis",
    "text": "The GDP Exploratory Data Analysis\nThen, after making analysis for income, sale price, and houseing affordability. We should have a look at for analyzing the GDP, which can represent the economy as a whole. By analyzing the GDP data, we can have a more generalized view and gain more insights about the patterns, seasonalities, and stationaries.\n\nTime Series Plot of GDP\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1980, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Which is similar to the houseing affordability index. I think we can see that there is a positive correlation between GDP and housing affordability index. When GDP gets higher, people can afford a house more easily. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow a multiplicative pattern. Because it seems that it does not have constant amplitude and frequency. In order to prove these, we need to explore more using different methods.\n\n\nLag for GDP\n\n\n\n\n\nBased on the lag plot, there’s a correlation but not too strong. This shows potential linearity. However, it could also mean that the dataset is not stationary but we can not decide yet.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has almost constant amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the acf decays very quickly, which also means that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$A191RI1Q225SBEA\nDickey-Fuller = -3.8947, Lag order = 6, p-value = 0.01466\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we reject the Null hypothesis. The time series dataset is stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformation\n\n\n\n\n\n\n\n\n\n\nSince, the original dataset is already stationary. Therefore, no further action is needed to make it stationary. The detrend and log transformation is just to explore the dataset.\n\n\nMoving Average\n\n\nThe “Original” line shows the raw GDP changes. The MA(5), MA(20), and MA(50) represent moving averages with windows of 5, 20, and 50 time units, respectively. It’s clear that as the window size increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-mean-sale-price-analysis",
    "href": "EDA.html#the-mean-sale-price-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean Sale Price Analysis",
    "text": "The Mean Sale Price Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822"
  },
  {
    "objectID": "EDA.html#the-mean-house-sale-price-data-analysis",
    "href": "EDA.html#the-mean-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Mean House Sale Price Data Analysis",
    "text": "The Mean House Sale Price Data Analysis\n\n\n\n\n\n\n\n\nFrom the plot, you can visually inspect:\nTrend: Any consistent upward or downward direction. Seasonality: Any repeating patterns or cycles. Variation: Fluctuations in the data. Periodic fluctuations: Spikes or drops at consistent intervals. Determine if the time series looks multiplicative or additive. An additive time series has constant amplitude and frequency, while a multiplicative one has varying amplitude/frequency.\n\n\n\n\n\nYou’d interpret the lag plot by looking for any structure. If the points cluster along a diagonal line from bottom-left to top-right, there’s a positive correlation. Any other pattern might suggest non-linearity or some pattern not captured by mere linear correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary"
  },
  {
    "objectID": "EDA.html#the-median-house-sale-price-data-analysis",
    "href": "EDA.html#the-median-house-sale-price-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nSince our key is to discover the impact of the income and house price, we certainly need to make analysis for the sale price data.\n\nThe Time Series\n\n\n\n\n\n\n\n\nFrom the plot, the results are the same with the previous section. It has an upwarding trend.\nTo be more specific, from the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists but not too much. Periodic fluctuations: The fluctuations are presented in 2010 and 2021. multiplicative or additive： Only based on the timeseries plot, I think this follows additive, because it does not have too much variance.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plots\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. In addition, this aligns with the same conclusion and it has a strong positive correlation. This shows linearity.\n\n\nDecomposition for the data\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. In addition, it has roughly constant amplitude/frequency. Therefore, I think this follows additive.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF of the data\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log transformation\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending, the sale prices remains fluctuations espectially from 2000 to 2020 period. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$MSPUS)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Sale Price Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThe graph shows the House Sale Price from 1970 to 2020. The blue line represents the original data. MA(2), MA(8), and MA(16) depict the moving averages over 2, 8, and 16 periods respectively. The smoothed lines show the underlying trend of house sale prices over time. The higher the period of the moving average, the smoother the line, which can help in understanding long-term trends."
  },
  {
    "objectID": "EDA.html#the-housing-affordability-index-data-analysis",
    "href": "EDA.html#the-housing-affordability-index-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Housing Affordability Index Data Analysis",
    "text": "The Housing Affordability Index Data Analysis\nThen, after we have a basic understanding about the income and sale price. We need to dive into the impact. We utilize the housing affordability index to see the impact of the income and sale price. Let us firt explore the houseing affordability index first.\n\nTime Series Plot for Housing Affordability Index\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = PSAVERT)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Time Series Plot of Housing Affordability Index\",\n       x = \"Date\", \n       y = \"The Housing Affordability Index\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: It seems that it did not have a consistent trend. From 1960 to 1970, it has upwarding trend. But then it has dewarding trend. And has a huge fluctuations during 2020, Covid period. Seasonality: I think there are small patterns that show seasonality in dataset. Variation: Fluctuations in the data exist. Periodic fluctuations: Spikes or drops at consistent intervals exist. Multiplicative or additive: I think the dataset could follow an additive pattern. Because it seems that it has constant amplitude and frequency although with some fluctuations. In order to prove these, we need to explore more using different methods.\n\n\nLag Plot for Houseing Affordability\n\n\nCode\n# Lag plot\nlagged_data <- data.frame(value = df$PSAVERT[-1],\n                          lagged_value = df$PSAVERT[-length(df$PSAVERT)])\n\n# Enhanced Lag Plot\nggplot(data = lagged_data, aes(x = lagged_value, y = value)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Lag Plot\",\n       x = \"Value at t-1\",\n       y = \"Value at t\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition of the Housing Affordability\n\n\nCode\n#library(ggfortify)\n\n# Decomposition using ggplot2 styling\ndecomposed <- decompose(ts(df$PSAVERT, frequency=12), type = \"additive\")\nautoplot(decomposed) + \n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data did not follow a specific trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, mostly it has constant amplitude/frequency. Therefore, I think this follows additive pattern.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF and PACF Analysis\n\n\nCode\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$PSAVERT) +\n  labs(title = \"ACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# PACF Plot\nggPacf(df$PSAVERT) +\n  labs(title = \"PACF of Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nThe ACF plot can help determine if the series is stationary. From here, we can see that the dataset has a correlation, and it is decaying slowly which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF Test\n\n\nCode\nlibrary(tseries)\nadf.test(df$PSAVERT)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$PSAVERT\nDickey-Fuller = -3.543, Lag order = 9, p-value = 0.03817\nalternative hypothesis: stationary\n\n\nBased on the ADF test, since the p-value is smaller than the threshold value, we should reject the null hypothesis, which means that the dataset is stationary! However, the value is close to 0.05. In addition, the ADF test is not as reliable as the ACF test. Therefore, since the ACF decays slowly, and it showed strong correlation. This means that, the dataset is not stationary.\n\n\nDetrened and Log-transformation\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nCode\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$PSAVERT))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCode\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$PSAVERT))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Housing Affordability Index\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nENsure it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$PSAVERT)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Housing Affordability IndexTime Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Housing Affordability Index Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: Most of the spikes fall within the blue shaded confidence intervals, which means that the dataset is stationary now compared to the orginal one.\n\n\nMoving Average\n\n\nThis graph presents the Housing Affordability Index from 1960 to 2020. The blue line is the original data. MA(2), MA(10), and MA(20) represent moving averages over 2, 10, and 20 periods. The plot illustrates how housing affordability has changed over time, and the smoothed lines can help identify broader trends or shifts in the data."
  },
  {
    "objectID": "EDA.html#the-household-income-data-analysis",
    "href": "EDA.html#the-household-income-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nAs we did in previous section, we start with the household income. In here, we have a basic look about the household income time series.\n\n\n\n\n\n\n\n\n\n\n\nFrom the plot, we can visually inspect: Trend: There is clearly positive trend. Seasonality: It seems that it does not contain patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： Only based on the timeseries plot, I think this follows multiplicative, because it has varying amplitude/frequency.\n \nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\nLAG Plot\n\n\n\n\n\nBased on the lag plot, there’s a positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity. However, it could also mean that the dataset is not stationary.\nIn order to determine if the dataset is stationary. we should use other methods as well. Before doing that, we should first check the trend, seasonality through decompositions.\n\n\nDecomposition\n\n\n\n\n\nNow, based on the decomposition plot, we can see that it is correct that the data follows an upwarding trend. However, it seems that there is seasonal pattern involves with flutuations. In addition, it has varying amplitude/frequency. Therefore, I think this follows multiplicative.\nThen, we should utilize ACF and PACF to see about the correlation and stationary.\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nMake it Stationary by using differencing\n\n\nCode\ndiff_series <- diff(df$W398RC1A027NBEA)\n\n# Plotting the differenced series\nggplot(data = data.frame(Date = df$DATE[-1], Diff = diff_series), aes(x = Date, y = Diff)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Differenced Household Income Time Series\",\n       x = \"Date\",\n       y = \"Differenced Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nCode\n# ACF Plot for differenced series\nggAcf(diff_series) +\n  labs(title = \"ACF of Differenced Household Income Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing has succeeded in making the datasets stationary: spikes all fall within the blue shaded confidence intervals, which represent the region where correlations are not statistically significant.\n\n\nMoving Average\n\n\nThe graph plots the Household Income from 1995 to 2020. The blue line represents the original data. MA(2), MA(4), and MA(8) are the moving averages taken over 2, 4, and 8 periods respectively. As the period of the moving average increases, the smoothed line becomes less responsive to short-term fluctuations."
  },
  {
    "objectID": "EDA.html#the-sale-price-disparity-analysis",
    "href": "EDA.html#the-sale-price-disparity-analysis",
    "title": "Exploratory Data Analysis",
    "section": "The Sale Price Disparity Analysis",
    "text": "The Sale Price Disparity Analysis\nLastly, after analyzing the corresponding numeric datasets, we will explore more about the disparitis by using the Mean Sale price datasets which involves with different regions and states. From here, by making analysis on the pattern and seasonality, it can help us provide with more insights and knowledge in determining the impact of the disparities of the income and sale prices.\n\nTime Series Decomposition Plot\n\n\n\n\n\n\n\n\n\n\n\nFrom the decomposition plot, we can visually inspect: Trend: There is no clear trend. Seasonality: It seems that it has seasonal patterns. Variation: Fluctuations in the data exists. Periodic fluctuations: The fluctuations are randomly presented. multiplicative or additive： I think this follows multiplicative, because it has varying amplitude/frequency.\nThen, we want to see if there is any correlations. By utilizing lag plot, we can see if there are any correlations present:\n\n\nLag Plot\n\n\n\n\n\nBased on the lag plot, there’s a strong positive correlation since the points cluster along a diagonal line from bottom-left to top-right. This shows linearity.\nIn order to explore more, we utilize the acf and pacf to check the accuracy.\n\n\nACF and PACF plots\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation and decays slowly. Howeverm for PACF, it decays dramatically. Therefore, we can say that it is stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nADF tests\n\n\nADF Statistic: -5.271328121414676\n\n\np-value: 6.276331376552306e-06\n\n\nCritical Values:\n\n\n    1%: -3.4364647646486093\n    5%: -2.864239892228526\n    10%: -2.5682075189699822\n\n\nBased on the result, we can see that the p-value is smaller than the thershold value, this means that we can reject the Null hypothesis. The time series dataset is stationary.\nThen, we wish to make analysis for different values to see if they have correlations among each other. A heatmap can also help us to determine the correlations among the sale price, rental price, and homevalue of the dataset.\n\n\nCorrelation heatmap\n\n\n\n\n\n\n\n\n\n\nMoving Average\n\n\n\n\n\n\n\n\nFrom here, we can see that there is a strong positive correlation between the Sale price and home value. The higher of the home value, the higher of the sale price. However, the rental price did not have too much correlations with the other two. The possible reason could be the cause of the geographic difference and this could also affect the housing affordability.\nIn conclusion, in this section, we provide different observations. And these observations underscore the nature of real estate pricing, incomes, GDP changes througout the years as well as the housing affordability. In addition, different patterns and features are found to identify the trend, seasonality, patterns, and correlations. which allow us to have a more understanding about the dataset."
  },
  {
    "objectID": "ARModels.html#the-household-income-data-analysis",
    "href": "ARModels.html#the-household-income-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Household Income Data Analysis",
    "text": "The Household Income Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and transforming, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.934, Lag order = 3, p-value = 0.598\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\n\n\n\nMake it Stationary by using differencing\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n\n\n\n\n\n\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 2)\nDickey-Fuller = -2.9811, Lag order = 3, p-value = 0.1972\nalternative hypothesis: stationary\n\n\nHowever, the second order of differencing still cannot make it stationary. We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n\nThird Differencing\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 3)\nDickey-Fuller = -5.488, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n\nEvaluate the values for p & q\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.340 -0.006  0.060 -0.087 -0.038  0.241 -0.349  0.214 -0.036 -0.050 \n    11     12     13     14 \n 0.007  0.065 -0.115  0.150 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.340 -0.137  0.013 -0.069 -0.100  0.211 -0.232  0.062 -0.004 -0.007 -0.053 \n    12     13     14 \n 0.017  0.015  0.007 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\nFit Model\nAfter we determing our parameters, we can start fitting the models.\n\n\nSeries: diff \nARIMA(1,3,2) \n\nCoefficients:\n          ar1      ma1     ma2\n      -0.7911  -1.8982  0.9996\ns.e.   0.2096   0.4024  0.4155\n\nsigma^2 = 919174:  log likelihood = -202.53\nAIC=413.07   AICc=415.17   BIC=417.78\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set -104.3663 845.5254 428.3458 153.1425 163.6866 0.695587 -0.3066825\n\n\nFrom here, we get a summary about the aic and bic values.\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 2), and (d = 3), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 3, q))\n\nCoefficients:\n          ar1      ma1      ma2\n      -0.4402  -0.2544  -0.0395\ns.e.   0.0377   0.0394   0.0262\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.19,  aic = 28412.39\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n422.0558\n430.2596\n425.8740\n\n\n2\n2\n2\n413.6960\n420.3570\n416.4232\n\n\n2\n3\n2\n405.0099\n411.4891\n407.8670\n\n\n2\n1\n3\n423.9044\n433.4754\n429.2377\n\n\n2\n2\n3\n411.0795\n419.0727\n415.0795\n\n\n2\n3\n3\n407.0239\n414.7989\n411.2239\n\n\n2\n1\n4\n425.8596\n436.7980\n433.0596\n\n\n2\n2\n4\n413.0766\n422.4020\n418.6766\n\n\n3\n1\n2\n423.9406\n433.5117\n429.2739\n\n\n3\n2\n2\n415.6154\n423.6086\n419.6154\n\n\n3\n3\n2\n406.9074\n414.6824\n411.1074\n\n\n3\n1\n3\n426.0532\n436.9916\n433.2532\n\n\n3\n2\n3\n413.0776\n422.4030\n418.6776\n\n\n3\n1\n4\n427.4187\n439.7243\n436.8923\n\n\n4\n1\n2\n425.4315\n436.3699\n432.6315\n\n\n4\n2\n2\n415.1668\n424.4922\n420.7668\n\n\n4\n1\n3\n427.4290\n439.7347\n436.9027\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\nForecasting for the dataset using the best parameters\n\n\n   Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n31       4278.130 3813.470  4742.791 3567.493  4988.767\n32       3458.665 2921.482  3995.847 2637.115  4280.215\n33       5473.540 4496.374  6450.705 3979.094  6967.985\n34       4679.682 3529.482  5829.881 2920.603  6438.760\n35       6742.303 5099.447  8385.159 4229.772  9254.834\n36       5988.600 4080.957  7896.243 3071.112  8906.088\n37       8093.980 5637.246 10550.714 4336.729 11851.230\n38       7382.141 4577.705 10186.577 3093.127 11671.156\n39       9529.693 6118.454 12940.932 4312.654 14746.732\n40       8859.921 5024.086 12695.756 2993.518 14726.325\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 42.947, df = 6, p-value = 1.195e-07\n\nModel df: 0.   Total lags used: 6\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$W398RC1A027NBEA \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.2008  -0.0093\ns.e.   0.1702   0.2238\n\nsigma^2 = 111877:  log likelihood = -208.7\nAIC=423.4   AICc=424.36   BIC=427.5\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE    MAPE      MASE      ACF1\nTraining set 83.97069 317.3154 158.0274 3.851463 17.2448 0.9308784 -0.111543\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 32.73409 312.2898 183.7851 -1.612877 23.50814 1.082607 -0.08540118\n\n\nTime Series:\nStart = 31 \nEnd = 50 \nFrequency = 1 \n [1]  4278.130  3458.665  5473.540  4679.682  6742.303  5988.600  8093.980\n [8]  7382.141  9529.693  8859.921 11049.575 10421.894 12653.642 12068.054\n[15] 14341.896 13798.400 16114.336 15612.934 17970.962 17511.654\n\n\n\n\nAccuracy of the based models\n\n\n                        ME     RMSE      MAE       MPE     MAPE MASE\nTraining set -2.643441e-14 635.2356 470.2565 -51.63239 78.34547    1\n\n\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE       ACF1\nTraining set 66.26655 330.1867 169.7616 2.867066 18.06051    1 -0.2730483\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.350993e-14 323.4687 166.4792 -9.624318 20.73922 0.9806647\n                   ACF1\nTraining set -0.2730483\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending."
  },
  {
    "objectID": "ARModels.html#the-median-house-sale-price-data-analysis",
    "href": "ARModels.html#the-median-house-sale-price-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Median House Sale Price Data Analysis",
    "text": "The Median House Sale Price Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$MSPUS\nDickey-Fuller = -2.6866, Lag order = 6, p-value = 0.287\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.7007, Lag order = 6, p-value = 0.7017\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\nMake it Stationary by using differencing\n\n\n\n\n\nAfter the first order of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$MSPUS, differences = 1)\nDickey-Fuller = -6.3462, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.625  0.098  0.079 -0.054  0.021  0.024 -0.095  0.071  0.094 -0.222 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.106  0.112 -0.158  0.057  0.021 -0.059  0.114 -0.106 -0.050  0.225 -0.228 \n    22     23 \n 0.130 -0.083 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.625 -0.481 -0.278 -0.165 -0.069  0.047 -0.070 -0.119  0.136 -0.028 -0.186 \n    12     13     14     15     16     17     18     19     20     21     22 \n 0.022  0.044 -0.013  0.008 -0.042  0.052  0.040 -0.118  0.082 -0.035  0.115 \n    23 \n-0.010 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.\n\n\nFit model\n\n\nSeries: diff \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.0092  -0.9827\ns.e.  0.0663   0.0133\n\nsigma^2 = 46013474:  log likelihood = -2458.55\nAIC=4923.1   AICc=4923.2   BIC=4933.54\n\nTraining set error measures:\n                   ME     RMSE      MAE  MPE MAPE      MASE        ACF1\nTraining set 542.7894 6740.971 3865.506 -Inf  Inf 0.6808965 -0.01153987\n\n\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 1), and (d = 1), we can write out the ARIMA(1,1,1) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^1 X_t = X_t + 0.0092X_{t-1} - 0.9827Y_{t-1} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 1, q))\n\nCoefficients:\n          ma1\n      -0.1934\ns.e.   0.0098\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.07,  aic = 28408.15\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n4938.630\n4952.569\n4938.799\n\n\n1\n2\n1\n4923.100\n4933.542\n4923.202\n\n\n1\n3\n1\n4976.098\n4986.528\n4976.200\n\n\n1\n1\n2\n4937.123\n4954.547\n4937.379\n\n\n1\n2\n2\n4924.722\n4938.645\n4924.892\n\n\n1\n3\n2\n4915.708\n4929.614\n4915.879\n\n\n1\n1\n3\n4921.957\n4942.866\n4922.316\n\n\n1\n2\n3\n4926.078\n4943.482\n4926.335\n\n\n1\n3\n3\n4918.622\n4936.004\n4918.879\n\n\n2\n1\n1\n4938.832\n4956.256\n4939.087\n\n\n2\n2\n1\n4933.117\n4947.039\n4933.287\n\n\n2\n3\n1\n4931.865\n4945.771\n4932.036\n\n\n2\n1\n2\n4918.505\n4939.414\n4918.864\n\n\n2\n2\n2\n4924.928\n4942.331\n4925.185\n\n\n2\n3\n2\n4923.269\n4940.651\n4923.526\n\n\n2\n1\n3\n4918.739\n4943.132\n4919.219\n\n\n2\n2\n3\n4903.774\n4924.657\n4904.134\n\n\n2\n3\n3\n4913.868\n4934.727\n4914.230\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\n\n\n   p d q      AIC      BIC     AICc\n17 2 2 3 4903.774 4924.657 4904.134\n\n\nSince we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.\n\n\nForecasting for the dataset\n\n\n    Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n243       415627.1 406980.8 424273.4 402403.7 428850.4\n244       415720.2 403091.1 428349.3 396405.7 435034.7\n245       416226.1 400354.8 432097.4 391953.1 440499.2\n246       417033.1 398322.0 435744.1 388416.9 445649.2\n247       418059.5 396783.9 439335.2 385521.2 450597.8\n248       419246.0 395615.0 442877.1 383105.4 455386.6\n249       420549.3 394731.0 446367.6 381063.5 460035.0\n250       421937.7 394071.9 449803.4 379320.7 464554.7\n251       423388.1 393593.8 453182.5 377821.6 468954.7\n252       424883.9 393263.4 456504.3 376524.6 473243.1\n253       426412.6 393055.6 459769.6 375397.5 477427.7\n254       427965.4 392950.6 462980.2 374415.0 481515.9\n255       429535.8 392933.2 466138.3 373557.0 485514.5\n256       431118.9 392991.1 469246.7 372807.5 489430.4\n257       432711.4 393114.6 472308.3 372153.3 493269.6\n258       434310.7 393295.7 475325.8 371583.7 497037.8\n259       435915.0 393528.0 478302.1 371089.6 500740.4\n260       437522.9 393806.0 481239.8 370663.7 504382.1\n261       439133.4 394125.3 484141.5 370299.5 507967.4\n262       440745.9 394482.2 487009.6 369991.7 511500.1\n263       442359.7 394873.3 489846.2 369735.5 514984.0\n264       443974.6 395296.0 492653.3 369527.0 518422.2\n265       445590.3 395747.7 495432.8 369362.6 521817.9\n266       447206.4 396226.4 498186.4 369239.2 525173.6\n267       448823.0 396730.3 500915.7 369154.1 528491.9\n268       450439.9 397257.7 503622.1 369104.7 531775.0\n269       452056.9 397807.1 506306.8 369089.0 535024.9\n270       453674.2 398377.3 508971.1 369104.9 538243.5\n271       455291.5 398967.0 511616.0 369150.6 541432.4\n272       456908.9 399575.2 514242.7 369224.5 544593.3\n273       458526.4 400200.9 516851.9 369325.2 547727.6\n274       460143.9 400843.2 519444.6 369451.3 550836.5\n275       461761.5 401501.3 522021.6 369601.6 553921.4\n276       463379.1 402174.5 524583.6 369774.8 556983.3\n277       464996.6 402862.1 527131.2 369970.0 560023.3\n278       466614.3 403563.4 529665.1 370186.3 563042.3\n279       468231.9 404277.8 532185.9 370422.6 566041.2\n280       469849.5 405004.8 534694.2 370678.1 569020.9\n281       471467.1 405743.9 537190.4 370952.1 571982.2\n282       473084.8 406494.5 539675.0 371243.8 574925.7\n283       474702.4 407256.4 542148.4 371552.6 577852.2\n284       476320.0 408028.9 544611.1 371877.8 580762.3\n285       477937.7 408811.8 547063.6 372218.7 583656.6\n286       479555.3 409604.6 549506.0 372574.9 586535.7\n287       481172.9 410407.0 551938.8 372945.8 589400.0\n288       482790.6 411218.8 554362.4 373330.9 592250.2\n289       484408.2 412039.5 556776.9 373729.8 595086.6\n290       486025.8 412868.9 559182.8 374141.9 597909.8\n291       487643.5 413706.7 561580.3 374566.9 600720.1\n292       489261.1 414552.6 563969.7 375004.3 603518.0\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1986.7, df = 10, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$MSPUS \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n         ma1    sma1\n      0.0210  0.1671\ns.e.  0.0665  0.0793\n\nsigma^2 = 47248340:  log likelihood = -2470.36\nAIC=4946.73   AICc=4946.83   BIC=4957.18\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 1355.307 6831.004 4055.777 1.052941 2.484314 0.9922265 -0.04153127\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 2.720481 6690.746 3914.234 -0.9087146 2.734111 0.9575986\n                    ACF1\nTraining set -0.05135495\n\n\nTime Series:\nStart = 243 \nEnd = 262 \nFrequency = 1 \n [1] 415627.1 415720.2 416226.1 417033.1 418059.5 419246.0 420549.3 421937.7\n [9] 423388.1 424883.9 426412.6 427965.4 429535.8 431118.9 432711.4 434310.7\n[17] 435915.0 437522.9 439133.4 440745.9\n\n\n\n\nAccuracy of the based models\n\n\n                       ME     RMSE      MAE       MPE     MAPE MASE\nTraining set 3.001896e-12 110997.1 92601.82 -122.4252 153.9077    1\n\n\n\n\n                   ME    RMSE      MAE    MPE     MAPE MASE      ACF1\nTraining set 1652.697 6932.02 4087.552 1.2553 2.554247    1 0.0208634\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -4.089888e-12 6732.124 4019.955 -1.162417 2.944873 0.9834627\n                  ACF1\nTraining set 0.0208634\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the trending pattern."
  },
  {
    "objectID": "ARModels.html#the-household-saving-data-analysis",
    "href": "ARModels.html#the-household-saving-data-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "The Household Saving Data Analysis",
    "text": "The Household Saving Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n\n\n\n\n\n\n\n\n\n\n\nACF & PACF\n\n\n\n\n\n\n\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\nValidation with ADF Test\n\n\nWarning in adf.test(df$W398RC1A027NBEA): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df$W398RC1A027NBEA\nDickey-Fuller = 0.038046, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\nDetrended and Log-transformed\n\n\n\n\n\n\n\n\n\n\nFrom here, we can see that after detrending and transforming, the household Saving remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n\nTest it stationary again\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log_transformed_data$LogTransformed\nDickey-Fuller = -1.934, Lag order = 3, p-value = 0.598\nalternative hypothesis: stationary\n\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n\n\n\n\nMake it Stationary by using differencing\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n\n\n\n\n\n\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 2)\nDickey-Fuller = -2.9811, Lag order = 3, p-value = 0.1972\nalternative hypothesis: stationary\n\n\nHowever, the second order of differencing still cannot make it stationary. We need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n\nThird Differencing\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(df$W398RC1A027NBEA, differences = 3)\nDickey-Fuller = -5.488, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n\nEvaluate the values for p & q\n\n\n\nAutocorrelations of series 'diff_series', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.340 -0.006  0.060 -0.087 -0.038  0.241 -0.349  0.214 -0.036 -0.050 \n    11     12     13     14 \n 0.007  0.065 -0.115  0.150 \n\n\n\n\n\n\nPartial autocorrelations of series 'diff_series', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.340 -0.137  0.013 -0.069 -0.100  0.211 -0.232  0.062 -0.004 -0.007 -0.053 \n    12     13     14 \n 0.017  0.015  0.007 \n\n\n\n\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\nFit Model\nAfter we determing our parameters, we can start fitting the models.\n\n\nSeries: diff \nARIMA(1,3,2) \n\nCoefficients:\n          ar1      ma1     ma2\n      -0.7911  -1.8982  0.9996\ns.e.   0.2096   0.4024  0.4155\n\nsigma^2 = 919174:  log likelihood = -202.53\nAIC=413.07   AICc=415.17   BIC=417.78\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE       ACF1\nTraining set -104.3663 845.5254 428.3458 153.1425 163.6866 0.695587 -0.3066825\n\n\nFrom here, we get a summary about the aic and bic values.\n\n\nEquation of the Model\nGiven the specified values (p = 1), (q = 2), and (d = 3), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n[ X_t = c + 1X{t-1} + 1a{t-1} + 2a{t-2} + a_t ]\nIn my case the third difference is represented as ( ^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} ). The ARIMA equation provided is built upon this differenced series.\n\n\nModel diagnostics\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n\n\n\n\n\nCall:\narima(x = arma14, order = c(p, 3, q))\n\nCoefficients:\n          ar1      ma1      ma2\n      -0.4402  -0.2544  -0.0395\ns.e.   0.0377   0.0394   0.0262\n\nsigma^2 estimated as 1.003:  log likelihood = -14202.19,  aic = 28412.39\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n422.0558\n430.2596\n425.8740\n\n\n2\n2\n2\n413.6960\n420.3570\n416.4232\n\n\n2\n3\n2\n405.0099\n411.4891\n407.8670\n\n\n2\n1\n3\n423.9044\n433.4754\n429.2377\n\n\n2\n2\n3\n411.0795\n419.0727\n415.0795\n\n\n2\n3\n3\n407.0239\n414.7989\n411.2239\n\n\n2\n1\n4\n425.8596\n436.7980\n433.0596\n\n\n2\n2\n4\n413.0766\n422.4020\n418.6766\n\n\n3\n1\n2\n423.9406\n433.5117\n429.2739\n\n\n3\n2\n2\n415.6154\n423.6086\n419.6154\n\n\n3\n3\n2\n406.9074\n414.6824\n411.1074\n\n\n3\n1\n3\n426.0532\n436.9916\n433.2532\n\n\n3\n2\n3\n413.0776\n422.4030\n418.6776\n\n\n3\n1\n4\n427.4187\n439.7243\n436.8923\n\n\n4\n1\n2\n425.4315\n436.3699\n432.6315\n\n\n4\n2\n2\n415.1668\n424.4922\n420.7668\n\n\n4\n1\n3\n427.4290\n439.7347\n436.9027\n\n\n\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\n\n\n  p d q      AIC      BIC    AICc\n3 2 3 2 405.0099 411.4891 407.867\n\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n\nModel Compare\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\nForecasting for the dataset using the best parameters\n\n\n   Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n31       4278.130 3813.470  4742.791 3567.493  4988.767\n32       3458.665 2921.482  3995.847 2637.115  4280.215\n33       5473.540 4496.374  6450.705 3979.094  6967.985\n34       4679.682 3529.482  5829.881 2920.603  6438.760\n35       6742.303 5099.447  8385.159 4229.772  9254.834\n36       5988.600 4080.957  7896.243 3071.112  8906.088\n37       8093.980 5637.246 10550.714 4336.729 11851.230\n38       7382.141 4577.705 10186.577 3093.127 11671.156\n39       9529.693 6118.454 12940.932 4312.654 14746.732\n40       8859.921 5024.086 12695.756 2993.518 14726.325\n\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n\nBENCHMARK\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n\n\n\nThe meanf model with residual plot\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 42.947, df = 6, p-value = 1.195e-07\n\nModel df: 0.   Total lags used: 6\n\n\n\n\nThe Arima model\n\n\n\n\n\n\n\nSeries: df$W398RC1A027NBEA \nARIMA(0,1,1)(0,0,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.2008  -0.0093\ns.e.   0.1702   0.2238\n\nsigma^2 = 111877:  log likelihood = -208.7\nAIC=423.4   AICc=424.36   BIC=427.5\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE    MAPE      MASE      ACF1\nTraining set 83.97069 317.3154 158.0274 3.851463 17.2448 0.9308784 -0.111543\n\n\n\n\n\n\n\nAccuracy of the fitted models\n\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 32.73409 312.2898 183.7851 -1.612877 23.50814 1.082607 -0.08540118\n\n\nTime Series:\nStart = 31 \nEnd = 50 \nFrequency = 1 \n [1]  4278.130  3458.665  5473.540  4679.682  6742.303  5988.600  8093.980\n [8]  7382.141  9529.693  8859.921 11049.575 10421.894 12653.642 12068.054\n[15] 14341.896 13798.400 16114.336 15612.934 17970.962 17511.654\n\n\n\n\nAccuracy of the based models\n\n\n                        ME     RMSE      MAE       MPE     MAPE MASE\nTraining set -2.643441e-14 635.2356 470.2565 -51.63239 78.34547    1\n\n\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE       ACF1\nTraining set 66.26655 330.1867 169.7616 2.867066 18.06051    1 -0.2730483\n\n\n\n\n                        ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.350993e-14 323.4687 166.4792 -9.624318 20.73922 0.9806647\n                   ACF1\nTraining set -0.2730483\n\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending."
  },
  {
    "objectID": "ARModels.html#gdp-deflator-sarima-analysis",
    "href": "ARModels.html#gdp-deflator-sarima-analysis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "GDP Deflator SARIMA Analysis",
    "text": "GDP Deflator SARIMA Analysis\n\n\n\n\nBefore Covid Period\n\n\nCode\ngdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)\ngdp_df\n\n\n   Qtr1 Qtr2 Qtr3 Qtr4\n1   5.8  6.9 10.2  3.2\n2   3.6  7.6  1.2 -2.1\n3  -3.9 -1.8  0.0 -0.6\n4   1.3  9.0  7.7 15.3\n5   2.7  0.2  4.7 -0.2\n6   0.5  4.6  1.1  0.1\n7   0.8  1.7  0.7  1.3\n8   0.4  0.5  1.1  1.9\n9   1.7  2.8  4.0  4.1\n10  2.4  5.1  1.6  5.6\n11  2.8  2.4  0.3  4.4\n12  1.2  2.5  1.9  0.9\n13  0.6  1.5  1.6  1.6\n14  1.0  1.4  1.2  0.9\n15  0.9  1.0  1.3  2.1\n16  0.7  0.8  0.8  1.8\n17  0.7  0.5  3.3  1.3\n18  0.9  1.6  1.8  2.0\n19  1.8  1.6  2.8  2.6\n20  3.3  3.9  3.4  1.7\n21  2.1  3.9  4.5  4.5\n22  4.3  4.0  5.8  4.2\n23  5.2  5.7  5.3  5.7\n24  5.7  3.3  5.4  6.2\n25  5.4  4.1  3.4  6.2\n26  2.5  3.9  5.2  4.7\n27  6.3  8.0  8.2  7.8\n28  9.8 12.2 12.3  9.4\n29  6.1  7.3  6.9  4.3\n30  4.1  5.3  7.4  6.6\n31  5.8  5.0  8.9  6.0\n32  7.9  7.0  8.4  7.5\n33 10.2  9.0  7.6  8.7\n34  9.9  9.2 10.8 11.0\n35  8.2  7.7  7.1  5.6\n36  5.3  5.8  4.2  3.1\n37  3.0  4.3  3.1  4.1\n38  3.5  3.6  3.0  4.0\n39  2.6  2.4  2.3  2.0\n40  1.5  1.7  2.2  2.6\n41  2.8  3.1  3.2  3.2\n42  4.0  4.9  3.5  4.2\n43  4.3  3.0  2.9  4.4\n44  4.6  3.5  3.0  4.0\n45  3.0  3.2  2.4  1.5\n46  2.4  2.0  2.8  2.3\n47  2.4  2.4  2.2  1.9\n48  1.9  2.3  2.2  2.2\n49  1.9  2.0  1.9  1.9\n50  1.7  1.3  2.2  2.4\n51  0.8  1.7  1.3  0.6\n52  0.9  1.7  1.1  1.3\n53  1.5  1.4  2.2  2.7\n54  2.5  2.4  2.2  2.6\n55  2.4  1.6  1.3  1.3\n56  1.4  1.9  2.3  2.0\n57  1.4  2.3  2.5  2.9\n58  3.3  2.6  3.1  3.2\n59  2.9  3.7  3.3  2.8\n60  3.6  2.8  1.5  3.9\n61  2.7  2.1  1.7  1.4\n62  2.0  3.1  1.0 -0.2\n63 -0.7  0.4  1.3  1.1\n64  2.0  1.2  2.4  2.1\n65  2.7  2.5  0.5  2.4\n66  1.6  2.1  2.0  1.6\n67  1.1  1.9  2.4  1.7\n68  2.3  1.8  0.7 -0.1\n69  2.2  1.2  0.0 -0.3\n70  2.9  1.1  2.1  2.1\n71  1.3  2.0  2.8  2.5\n72  3.5  1.4  1.8  1.6\n73  2.2  1.3  1.5  1.6\n74 -1.5  3.5  2.6     \n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of GDP Deflator\",\n       x = \"Date\", \n       y = \"GDP Deflator\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCheck Decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(gdp_df, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n\n\n\nCheck Lag Plot\n\n\nCode\ngglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))\n\n\n\n\n\n\n\nSeasonal Difference AND ACF & PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.\n\n\nCode\nts_plot <- autoplot(gdp_df) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot :\nThe sharp drop after lag 1 and some lags across the range. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1. PACF Plot:\nThe sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. Order of Differencing:\nYou mentioned that you applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1, d = 1, q = 1,2 Seasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4. Therefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].\nWE can continue analysis\n\n\nModel Diagnostics\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n1353.815\n1357.485\n1353.829\n\n\n0\n1\n0\n0\n1\n1\n1168.495\n1175.834\n1168.536\n\n\n0\n1\n0\n1\n1\n0\n1257.004\n1264.344\n1257.046\n\n\n0\n1\n0\n1\n1\n1\n1168.185\n1179.194\n1168.269\n\n\n0\n1\n1\n0\n1\n0\n1321.946\n1329.286\n1321.988\n\n\n0\n1\n1\n0\n1\n1\n1129.920\n1140.930\n1130.004\n\n\n0\n1\n1\n1\n1\n0\n1232.655\n1243.665\n1232.739\n\n\n0\n1\n1\n1\n1\n1\n1129.375\n1144.054\n1129.515\n\n\n0\n1\n2\n0\n1\n0\n1288.080\n1299.090\n1288.164\n\n\n0\n1\n2\n0\n1\n1\n1131.369\n1146.049\n1131.510\n\n\n0\n1\n2\n1\n1\n0\n1233.896\n1248.575\n1234.036\n\n\n1\n1\n0\n0\n1\n0\n1325.569\n1332.909\n1325.611\n\n\n1\n1\n0\n0\n1\n1\n1138.391\n1149.401\n1138.475\n\n\n1\n1\n0\n1\n1\n0\n1237.785\n1248.794\n1237.868\n\n\n1\n1\n0\n1\n1\n1\n1138.442\n1153.122\n1138.583\n\n\n1\n1\n1\n0\n1\n0\n1275.876\n1286.886\n1275.960\n\n\n1\n1\n1\n0\n1\n1\n1127.620\n1142.300\n1127.760\n\n\n1\n1\n1\n1\n1\n0\n1204.062\n1218.741\n1204.202\n\n\n1\n1\n2\n0\n1\n0\n1271.665\n1286.345\n1271.805\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCompare the results\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q     AIC     BIC     AICc\n6 0 1 1 0 1 1 1129.92 1140.93 1130.004\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:80], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.5197  -0.8775  -0.9535\ns.e.  0.0810   0.0497   0.0279\n\nsigma^2 estimated as 2.671:  log likelihood = -559.81,  aic = 1127.62\n\n$degrees_of_freedom\n[1] 287\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.5197 0.0810   6.4164       0\nma1   -0.8775 0.0497 -17.6718       0\nsma1  -0.9535 0.0279 -34.1752       0\n\n$AIC\n[1] 3.888345\n\n$AICc\n[1] 3.888634\n\n$BIC\n[1] 3.938964\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n[1] 288\n\n$ttable\n     Estimate     SE  t.value p.value\nma1   -0.4017 0.0579  -6.9345       0\nsma1  -0.9579 0.0242 -39.6386       0\n\n$AIC\n[1] 3.896277\n\n$AICc\n[1] 3.896421\n\n$BIC\n[1] 3.934241\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant.\n\n\nFit model & Forecasting\n\n\nCode\nfit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,120)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nCode\nsarima.for(gdp_df, 36, 0,1,1,0,1,1,4)\n\n\n\n\n\n$pred\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            2.263270\n75 2.221820 2.403705 2.324961 2.241627\n76 2.200177 2.382062 2.303318 2.219983\n77 2.178533 2.360419 2.281674 2.198340\n78 2.156890 2.338775 2.260031 2.176696\n79 2.135247 2.317132 2.238388 2.155053\n80 2.113603 2.295488 2.216744 2.133410\n81 2.091960 2.273845 2.195101 2.111766\n82 2.070316 2.252202 2.173457 2.090123\n83 2.048673 2.230558 2.151814         \n\n$se\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            1.650936\n75 1.923859 2.162607 2.377500 2.602077\n76 2.798329 2.981682 3.154396 3.340432\n77 3.507987 3.667883 3.821094 3.987789\n78 4.140176 4.287134 4.429218 4.584383\n79 4.727550 4.866489 5.001569 5.149237\n80 5.286381 5.420036 5.550474 5.693022\n81 5.826077 5.956138 6.083420 6.222379\n82 6.352607 6.480195 6.605319 6.741734\n83 6.870006 6.995903 7.119573         \n\n\n\n\nBenchMark Comparsion\n\n\nCode\nautoplot(gdp_df) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\",PI=FALSE) +\n  autolayer(meanf(gdp_df, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_df, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_df, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_df, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf2 <- snaive(gdp_df, h=36) \n\naccuracy(f2)\n\n\n                      ME     RMSE     MAE  MPE MAPE MASE      ACF1\nTraining set -0.06838488 2.442416 1.45945 -Inf  Inf    1 0.4805284\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\nOur model fitting is much better than benchmark methods\n\n\nCross validation\n\nOne Step ahead\n\n\nCode\nn <- length(gdp_df)\nn \n\n\n[1] 295\n\n\n\n\nCode\nk <- 89 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 206\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_df[k+i] #76th observation as the test set\n\n  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n       MAE1      MAE2      MSE1      MSE2\n1 0.8997526 0.8742994 0.9035127 0.8766414\n\n\nWe can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.\n\n\n4 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}\n\n# Compute cross-validated errors for up to 4 steps ahead\ne <- tsCV(gdp_df, forecastfunction = farima1, h = 4)\n \nlength(e) \n\n\n[1] 1180\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:4, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case."
  },
  {
    "objectID": "ARModels.html#before-covid-period",
    "href": "ARModels.html#before-covid-period",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Before Covid Period",
    "text": "Before Covid Period\n\n\nCode\ngdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)\ngdp_df\n\n\n   Qtr1 Qtr2 Qtr3 Qtr4\n1   5.8  6.9 10.2  3.2\n2   3.6  7.6  1.2 -2.1\n3  -3.9 -1.8  0.0 -0.6\n4   1.3  9.0  7.7 15.3\n5   2.7  0.2  4.7 -0.2\n6   0.5  4.6  1.1  0.1\n7   0.8  1.7  0.7  1.3\n8   0.4  0.5  1.1  1.9\n9   1.7  2.8  4.0  4.1\n10  2.4  5.1  1.6  5.6\n11  2.8  2.4  0.3  4.4\n12  1.2  2.5  1.9  0.9\n13  0.6  1.5  1.6  1.6\n14  1.0  1.4  1.2  0.9\n15  0.9  1.0  1.3  2.1\n16  0.7  0.8  0.8  1.8\n17  0.7  0.5  3.3  1.3\n18  0.9  1.6  1.8  2.0\n19  1.8  1.6  2.8  2.6\n20  3.3  3.9  3.4  1.7\n21  2.1  3.9  4.5  4.5\n22  4.3  4.0  5.8  4.2\n23  5.2  5.7  5.3  5.7\n24  5.7  3.3  5.4  6.2\n25  5.4  4.1  3.4  6.2\n26  2.5  3.9  5.2  4.7\n27  6.3  8.0  8.2  7.8\n28  9.8 12.2 12.3  9.4\n29  6.1  7.3  6.9  4.3\n30  4.1  5.3  7.4  6.6\n31  5.8  5.0  8.9  6.0\n32  7.9  7.0  8.4  7.5\n33 10.2  9.0  7.6  8.7\n34  9.9  9.2 10.8 11.0\n35  8.2  7.7  7.1  5.6\n36  5.3  5.8  4.2  3.1\n37  3.0  4.3  3.1  4.1\n38  3.5  3.6  3.0  4.0\n39  2.6  2.4  2.3  2.0\n40  1.5  1.7  2.2  2.6\n41  2.8  3.1  3.2  3.2\n42  4.0  4.9  3.5  4.2\n43  4.3  3.0  2.9  4.4\n44  4.6  3.5  3.0  4.0\n45  3.0  3.2  2.4  1.5\n46  2.4  2.0  2.8  2.3\n47  2.4  2.4  2.2  1.9\n48  1.9  2.3  2.2  2.2\n49  1.9  2.0  1.9  1.9\n50  1.7  1.3  2.2  2.4\n51  0.8  1.7  1.3  0.6\n52  0.9  1.7  1.1  1.3\n53  1.5  1.4  2.2  2.7\n54  2.5  2.4  2.2  2.6\n55  2.4  1.6  1.3  1.3\n56  1.4  1.9  2.3  2.0\n57  1.4  2.3  2.5  2.9\n58  3.3  2.6  3.1  3.2\n59  2.9  3.7  3.3  2.8\n60  3.6  2.8  1.5  3.9\n61  2.7  2.1  1.7  1.4\n62  2.0  3.1  1.0 -0.2\n63 -0.7  0.4  1.3  1.1\n64  2.0  1.2  2.4  2.1\n65  2.7  2.5  0.5  2.4\n66  1.6  2.1  2.0  1.6\n67  1.1  1.9  2.4  1.7\n68  2.3  1.8  0.7 -0.1\n69  2.2  1.2  0.0 -0.3\n70  2.9  1.1  2.1  2.1\n71  1.3  2.0  2.8  2.5\n72  3.5  1.4  1.8  1.6\n73  2.2  1.3  1.5  1.6\n74 -1.5  3.5  2.6     \n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of GDP Deflator\",\n       x = \"Date\", \n       y = \"GDP Deflator\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))"
  },
  {
    "objectID": "ARModels.html#check-decomposition",
    "href": "ARModels.html#check-decomposition",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Check Decomposition",
    "text": "Check Decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(gdp_df, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")"
  },
  {
    "objectID": "ARModels.html#check-lag-plot",
    "href": "ARModels.html#check-lag-plot",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Check Lag Plot",
    "text": "Check Lag Plot\n\n\nCode\ngglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))"
  },
  {
    "objectID": "ARModels.html#seasonal-difference-and-acf-pacf",
    "href": "ARModels.html#seasonal-difference-and-acf-pacf",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Seasonal Difference AND ACF & PACF",
    "text": "Seasonal Difference AND ACF & PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.\n\n\nCode\nts_plot <- autoplot(gdp_df) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot :\nThe sharp drop after lag 1 and some lags across the range. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1. PACF Plot:\nThe sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. Order of Differencing:\nYou mentioned that you applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1, d = 1, q = 1,2 Seasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4. Therefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].\nWE can continue analysis"
  },
  {
    "objectID": "ARModels.html#model-diagnostics-2",
    "href": "ARModels.html#model-diagnostics-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n1353.815\n1357.485\n1353.829\n\n\n0\n1\n0\n0\n1\n1\n1168.495\n1175.834\n1168.536\n\n\n0\n1\n0\n1\n1\n0\n1257.004\n1264.344\n1257.046\n\n\n0\n1\n0\n1\n1\n1\n1168.185\n1179.194\n1168.269\n\n\n0\n1\n1\n0\n1\n0\n1321.946\n1329.286\n1321.988\n\n\n0\n1\n1\n0\n1\n1\n1129.920\n1140.930\n1130.004\n\n\n0\n1\n1\n1\n1\n0\n1232.655\n1243.665\n1232.739\n\n\n0\n1\n1\n1\n1\n1\n1129.375\n1144.054\n1129.515\n\n\n0\n1\n2\n0\n1\n0\n1288.080\n1299.090\n1288.164\n\n\n0\n1\n2\n0\n1\n1\n1131.369\n1146.049\n1131.510\n\n\n0\n1\n2\n1\n1\n0\n1233.896\n1248.575\n1234.036\n\n\n1\n1\n0\n0\n1\n0\n1325.569\n1332.909\n1325.611\n\n\n1\n1\n0\n0\n1\n1\n1138.391\n1149.401\n1138.475\n\n\n1\n1\n0\n1\n1\n0\n1237.785\n1248.794\n1237.868\n\n\n1\n1\n0\n1\n1\n1\n1138.442\n1153.122\n1138.583\n\n\n1\n1\n1\n0\n1\n0\n1275.876\n1286.886\n1275.960\n\n\n1\n1\n1\n0\n1\n1\n1127.620\n1142.300\n1127.760\n\n\n1\n1\n1\n1\n1\n0\n1204.062\n1218.741\n1204.202\n\n\n1\n1\n2\n0\n1\n0\n1271.665\n1286.345\n1271.805\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "ARModels.html#compare",
    "href": "ARModels.html#compare",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Compare",
    "text": "Compare\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q     AIC     BIC     AICc\n6 0 1 1 0 1 1 1129.92 1140.93 1130.004\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q     AIC    BIC    AICc\n17 1 1 1 0 1 1 1127.62 1142.3 1127.76\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:80], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.5197  -0.8775  -0.9535\ns.e.  0.0810   0.0497   0.0279\n\nsigma^2 estimated as 2.671:  log likelihood = -559.81,  aic = 1127.62\n\n$degrees_of_freedom\n[1] 287\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.5197 0.0810   6.4164       0\nma1   -0.8775 0.0497 -17.6718       0\nsma1  -0.9535 0.0279 -34.1752       0\n\n$AIC\n[1] 3.888345\n\n$AICc\n[1] 3.888634\n\n$BIC\n[1] 3.938964\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n[1] 288\n\n$ttable\n     Estimate     SE  t.value p.value\nma1   -0.4017 0.0579  -6.9345       0\nsma1  -0.9579 0.0242 -39.6386       0\n\n$AIC\n[1] 3.896277\n\n$AICc\n[1] 3.896421\n\n$BIC\n[1] 3.934241\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant."
  },
  {
    "objectID": "ARModels.html#fit-model-2",
    "href": "ARModels.html#fit-model-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Fit model",
    "text": "Fit model\n\n\nCode\nfit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,120)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nCode\nsarima.for(gdp_df, 36, 0,1,1,0,1,1,4)\n\n\n\n\n\n$pred\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            2.263270\n75 2.221820 2.403705 2.324961 2.241627\n76 2.200177 2.382062 2.303318 2.219983\n77 2.178533 2.360419 2.281674 2.198340\n78 2.156890 2.338775 2.260031 2.176696\n79 2.135247 2.317132 2.238388 2.155053\n80 2.113603 2.295488 2.216744 2.133410\n81 2.091960 2.273845 2.195101 2.111766\n82 2.070316 2.252202 2.173457 2.090123\n83 2.048673 2.230558 2.151814         \n\n$se\n       Qtr1     Qtr2     Qtr3     Qtr4\n74                            1.650936\n75 1.923859 2.162607 2.377500 2.602077\n76 2.798329 2.981682 3.154396 3.340432\n77 3.507987 3.667883 3.821094 3.987789\n78 4.140176 4.287134 4.429218 4.584383\n79 4.727550 4.866489 5.001569 5.149237\n80 5.286381 5.420036 5.550474 5.693022\n81 5.826077 5.956138 6.083420 6.222379\n82 6.352607 6.480195 6.605319 6.741734\n83 6.870006 6.995903 7.119573"
  },
  {
    "objectID": "ARModels.html#benchmark-comparsion",
    "href": "ARModels.html#benchmark-comparsion",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "BenchMark Comparsion",
    "text": "BenchMark Comparsion\n\n\nCode\nautoplot(gdp_df) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\",PI=FALSE) +\n  autolayer(meanf(gdp_df, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_df, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_df, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_df, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf2 <- snaive(gdp_df, h=36) \n\naccuracy(f2)\n\n\n                      ME     RMSE     MAE  MPE MAPE MASE      ACF1\nTraining set -0.06838488 2.442416 1.45945 -Inf  Inf    1 0.4805284\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = gdp_df, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 4))\n\nCoefficients:\n          ma1     sma1\n      -0.4017  -0.9579\ns.e.   0.0579   0.0242\n\nsigma^2 estimated as 2.725:  log likelihood = -561.96,  aic = 1129.92\n\nTraining set error measures:\n                     ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 0.08485662 1.636759 1.074369 NaN  Inf 0.9718906 0.006601766\n\n\nOur model fitting is much better than benchmark methods"
  },
  {
    "objectID": "ARModels.html#cross-validation",
    "href": "ARModels.html#cross-validation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Cross validation",
    "text": "Cross validation\n\nOne Step ahead\n\n\nCode\nn <- length(gdp_df)\nn \n\n\n[1] 295\n\n\n\n\nCode\nk <- 89 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 206\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_df[k+i] #76th observation as the test set\n\n  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n       MAE1      MAE2      MSE1      MSE2\n1 0.8997526 0.8742994 0.9035127 0.8766414\n\n\nWe can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.\n\n\n4 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}\n\n# Compute cross-validated errors for up to 4 steps ahead\ne <- tsCV(gdp_df, forecastfunction = farima1, h = 4)\n \nlength(e) \n\n\n[1] 1180\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:4, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case."
  },
  {
    "objectID": "ARModels.html#real-median-household-income-in-the-united-states-sarima-analsyis",
    "href": "ARModels.html#real-median-household-income-in-the-united-states-sarima-analsyis",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Real Median Household Income in the United States SARIMA Analsyis",
    "text": "Real Median Household Income in the United States SARIMA Analsyis\n\n\n\n\nBefore Covid Period\n\n\nCode\nrmhi <- ts(df$MEHOINUSA672N,frequency = 2)\nrmhi\n\n\nTime Series:\nStart = c(1, 1) \nEnd = c(19, 2) \nFrequency = 2 \n [1] 55828 56871 58920 59624 60115 61153 60370 58607 58153 57843 58515 60348\n[13] 61225 62484 64781 66385 66248 64779 64047 63967 63745 64427 64930 65801\n[25] 63455 63011 61364 60428 60313 62425 61468 64631 66657 67571 68168 72808\n[37] 71186 70784\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = MEHOINUSA672N)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Income\",\n       x = \"Date\", \n       y = \"Household Income\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n\n\n\n\n\nCheck with decomposition\n\n\nCode\n# Decompose the time series data\ndec <- decompose(rmhi, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n\n\n\n\nCode\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n\n\n\nCheck Seasonal Pattern using Lag\n\n\nCode\ngglagplot(rmhi, do.lines=FALSE, set.lags = c(2, 4, 8, 12))\n\n\n\n\n\n\n\nSeasonal Difference and ACF, PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 2 relatively.\n\n\nCode\nts_plot <- autoplot(rmhi) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nMost spikes are within range, it is stationary. ACF Plot (Autocorrelation Function Plot):\nThe sharp drop after lag 2. This gives us q = 1,2. Since there’s a noticeable autocorrelation at lag 2,5, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 1, which implies Q = 1,2. PACF Plot (Partial Autocorrelation Function Plot):\nThe sharp drop after lag 2 in the PACF plot indicates a possible AR(1) process. This gives us p = 1,2. The seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. However, the dataset did not have a strong seasonal pattern. We can also consider P to be 0 Order of Differencing:\nWe applied first order differencing, so d = 1. You also mentioned seasonal differencing with a lag of 4, so D = 1. Combining these, we get:\nNon-seasonal parameters: p = 1,2, d = 1, q = 1,2 Seasonal parameters: P = 0,1 D = 1, Q = 1, and the seasonal period (or frequency) is 2. Therefore, the ARIMA model can be represented as ARIMA(2,1,1)(0,1,1)[2].\nWE can continue analysis\n\n\nModel Diagnostic\n\n\nCode\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=2\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*23),nrow=23)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n\n\nCompare Results\n\n\nCode\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=rmhi)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n629.7060\n631.2613\n629.8272\n\n\n0\n1\n0\n0\n1\n1\n618.0458\n621.1565\n618.4208\n\n\n0\n1\n0\n1\n1\n0\n626.3264\n629.4371\n626.7014\n\n\n0\n1\n0\n1\n1\n1\n619.6015\n624.2676\n620.3757\n\n\n0\n1\n1\n0\n1\n0\n622.7853\n625.8960\n623.1603\n\n\n0\n1\n1\n0\n1\n1\n615.4804\n620.1464\n616.2546\n\n\n0\n1\n1\n1\n1\n0\n618.5424\n623.2084\n619.3166\n\n\n0\n1\n1\n1\n1\n1\n617.1456\n623.3670\n618.4789\n\n\n0\n1\n2\n0\n1\n0\n620.1165\n624.7825\n620.8907\n\n\n0\n1\n2\n0\n1\n1\n617.2042\n623.4256\n618.5375\n\n\n0\n1\n2\n1\n1\n0\n619.5969\n625.8183\n620.9302\n\n\n1\n1\n0\n0\n1\n0\n631.0309\n634.1416\n631.4059\n\n\n1\n1\n0\n0\n1\n1\n616.4825\n621.1486\n617.2567\n\n\n1\n1\n0\n1\n1\n0\n623.7348\n628.4008\n624.5090\n\n\n1\n1\n0\n1\n1\n1\n617.5213\n623.7427\n618.8547\n\n\n1\n1\n1\n0\n1\n0\n622.7940\n627.4601\n623.5682\n\n\n1\n1\n1\n0\n1\n1\n617.2456\n623.4670\n618.5790\n\n\n1\n1\n1\n1\n1\n0\n619.8546\n626.0760\n621.1880\n\n\n1\n1\n2\n0\n1\n0\n617.1006\n623.3219\n618.4339\n\n\n2\n1\n0\n0\n1\n0\n627.3197\n631.9857\n628.0938\n\n\n2\n1\n0\n0\n1\n1\n618.3728\n624.5942\n619.7062\n\n\n2\n1\n0\n1\n1\n0\n624.6816\n630.9030\n626.0149\n\n\n2\n1\n1\n0\n1\n0\n616.9212\n623.1426\n618.2545\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n6 0 1 1 0 1 1 615.4804 620.1464 616.2546\n\n\n\n\nCode\nset.seed(236)\nmodel_output1 <- capture.output(sarima(rmhi, 2,1,1,0,1,1,2))\n\n\n\n\n\nCode\nmodel_output2 <- capture.output(sarima(rmhi, 0,1,1,0,1,1,2))\n\n\n\n\n\nThe second one is a little better.\n\n\nCode\ncat(model_output1[50:67], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n[1] 31\n\n$ttable\n     Estimate     SE t.value p.value\nar1   -0.1325 0.2471 -0.5362  0.5956\nar2   -0.1839 0.3213 -0.5723  0.5712\nma1    0.6255 0.2393  2.6142  0.0137\nsma1  -0.5330 0.3657 -1.4576  0.1550\n\n$AIC\n[1] 17.68248\n\n$AICc\n[1] 17.72058\n\n$BIC\n[1] 17.90468\n\n\n\n\nCode\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n\n$ttable\n     Estimate     SE t.value p.value\nma1    0.4932 0.2136  2.3085  0.0274\nsma1  -0.6924 0.1661 -4.1678  0.0002\n\n$AIC\n[1] 17.58515\n\n$AICc\n[1] 17.59587\n\n$BIC\n[1] 17.71847\n\nNA\n\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n$ttable: all coefficients are significant. The second one is better indeed.\n\n\nFit model & Forecast\n\n\nCode\nfit2=arima(rmhi, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\nsummary(fit2)\n\n\n\nCall:\narima(x = rmhi, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 2))\n\nCoefficients:\n         ma1     sma1\n      0.4932  -0.6924\ns.e.  0.2136   0.1661\n\nsigma^2 estimated as 2022906:  log likelihood = -304.74,  aic = 615.48\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -72.4368 1365.071 1031.092 -0.1227066 1.603226 0.8711332\n                    ACF1\nTraining set -0.03840144\n\n\n\n\nCode\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,36)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n\n\n\n\n\n\nBenchMark Comparsion\n\n\nCode\nsarima.for(rmhi, 36, 0,1,1,0,1,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(20, 1) \nEnd = c(37, 2) \nFrequency = 2 \n [1] 69952.81 71400.41 70921.82 72369.41 71890.82 73338.42 72859.82 74307.42\n [9] 73828.83 75276.43 74797.83 76245.43 75766.84 77214.43 76735.84 78183.44\n[17] 77704.84 79152.44 78673.85 80121.44 79642.85 81090.45 80611.86 82059.45\n[25] 81580.86 83028.46 82549.86 83997.46 83518.87 84966.46 84487.87 85935.47\n[33] 85456.87 86904.47 86425.88 87873.47\n\n$se\nTime Series:\nStart = c(20, 1) \nEnd = c(37, 2) \nFrequency = 2 \n [1]  1422.290  2555.981  3618.397  4561.172  5580.054  6550.064  7606.739\n [8]  8633.488  9745.571 10835.708 12007.705 13161.940 14394.301 15611.416\n[15] 16903.132 18181.267 19530.792 20867.921 22273.543 23667.660 25127.664\n[22] 26576.861 28089.590 29592.080 31155.970 32710.092 34323.673 35927.889\n[29] 37589.789 39242.674 40951.612 42651.843 44406.626 46152.975 47952.485\n[36] 49743.807\n\n\n\n\nCode\nautoplot(rmhi) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\") +\n  autolayer(meanf(rmhi, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(rmhi, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(rmhi, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(rmhi, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nCode\nf1 <- meanf(rmhi, h=36)\nf2 <- snaive(rmhi, h=36) \nf3 <-naive(rmhi, h=36)\naccuracy(f1)\n\n\n                       ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set 1.914539e-12 3930.345 3192.816 -0.3813871 5.052014 1.549179\n                  ACF1\nTraining set 0.8332796\n\n\nCode\naccuracy(f2)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 813.0833 2378.943 2060.972 1.213902 3.223489    1 0.6130337\n\n\nCode\naccuracy(f3)\n\n\n                   ME     RMSE      MAE      MPE     MAPE      MASE      ACF1\nTraining set 404.2162 1503.922 1183.622 0.614466 1.854535 0.5743026 0.1656062\n\n\n\n\nCode\nsummary(fit2)\n\n\n\nCall:\narima(x = rmhi, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 2))\n\nCoefficients:\n         ma1     sma1\n      0.4932  -0.6924\ns.e.  0.2136   0.1661\n\nsigma^2 estimated as 2022906:  log likelihood = -304.74,  aic = 615.48\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -72.4368 1365.071 1031.092 -0.1227066 1.603226 0.8711332\n                    ACF1\nTraining set -0.03840144\n\n\nOur model fitting is better than benchmark methods with smaller RMSE\n\n\nCross validation\n\nOne Step ahead\n\n\nCode\nn <- length(rmhi)\nn * 0.3\n\n\n[1] 11.4\n\n\n\n\nCode\nk <- 12 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n\n\n[1] 26\n\n\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- rmhi[1:(k-1)+i] \n  xtest <- rmhi[k+i] \n  fit <- arima(xtrain, order = c(2,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n\n\nCode\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n\n\n\n\nCode\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n\n\n     MAE1     MAE2     MSE1     MSE2\n1 1415.04 1494.363 410152.8 801771.5\n\n\nWe can see that the corresponding results for model 2: (2,1,1)(0,1,1) is slightly better. Which is not the same as the conclusion from above, the reason could be that the data points are relatively small. The train and test split can not capture the datasets effectively, which can cause the different conclusion. The model diagnotics from previsou section indicates that the (0,1,1)(0,1,1) is better with smaller errors.\n\n\n2 step ahead in my case\n\n\nCode\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) ),h=h)}\n\n# Compute cross-validated errors for up to 2 steps ahead\ne <- tsCV(rmhi, forecastfunction = farima1, h = 2)\n \nlength(e) \n\n\n[1] 76\n\n\n\n\nCode\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:2, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n\n\n\n\n\nFrom here we can see that the one step ahead has lower MSE, which is better than two step ahead in my case."
  },
  {
    "objectID": "ASV.html#set-up-the-variables",
    "href": "ASV.html#set-up-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Set Up The Variables",
    "text": "Set Up The Variables\n\n\nCode\ndd1<-data.frame(homevalue,saleprice,rentalprice,df8$date)\n\ncolnames(dd1)<-c(\"homevalue\",\"saleprice\",\"rentalprice\",'date')\n\nknitr::kable(head(dd1))\n\n\n\n\n\nhomevalue\nsaleprice\nrentalprice\ndate\n\n\n\n\n381885.7\n394588\n1489.368\n2018-08-31\n\n\n383510.8\n396351\n1495.171\n2018-09-30\n\n\n384594.7\n391015\n1492.019\n2018-10-31\n\n\n385442.7\n385500\n1483.661\n2018-11-30\n\n\n387043.6\n384032\n1482.177\n2018-12-31\n\n\n389061.8\n385450\n1477.878\n2019-01-31\n\n\n\n\n\n\n\nCode\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"homevalue\"=log(dd1$homevalue),\"saleprice\"=log(dd1$saleprice),\n                                        \"rentalprice\"=log(dd1$rentalprice))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model",
    "href": "ASV.html#auto-fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit The Model",
    "text": "Auto Fit The Model\n\n\nCode\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"homevalue\"], xreg = xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"homevalue\"] \nRegression with ARIMA(2,1,2) errors \n\nCoefficients:\n         ar1     ar2     ma1      ma2  saleprice  rentalprice\n      0.0357  0.4180  -0.463  -0.4912     0.8765       0.1160\ns.e.  0.1418  0.0786   0.147   0.1386     0.0422       0.0354\n\nsigma^2 = 0.007652:  log likelihood = 1100.12\nAIC=-2186.23   AICc=-2186.13   BIC=-2151.35\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.001968005 0.08719397 0.02695833 0.01034377 0.2024629 0.3681705\n                    ACF1\nTraining set 0.002220954"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model",
    "href": "ASV.html#manual-fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The Model",
    "text": "Manual Fit The Model\n\n\nCode\nfit.reg1 <- lm( homevalue ~ saleprice+ rentalprice, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = homevalue ~ saleprice + rentalprice, data = lg.dd.ts1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68312 -0.03521 -0.00042  0.03629  0.27205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49700    0.18542   2.680  0.00747 ** \nsaleprice    0.93209    0.01680  55.472  < 2e-16 ***\nrentalprice  0.05678    0.01422   3.993 6.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1184 on 1077 degrees of freedom\nMultiple R-squared:  0.823, Adjusted R-squared:  0.8227 \nF-statistic:  2504 on 2 and 1077 DF,  p-value: < 2.2e-16\n\n\nWe can see that the variables are pretty significant."
  },
  {
    "objectID": "ASV.html#check-the-residuals",
    "href": "ASV.html#check-the-residuals",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check The Residuals",
    "text": "Check The Residuals\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,1,2) errors\nQ* = 8.9982, df = 4, p-value = 0.06114\n\nModel df: 4.   Total lags used: 8\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#check-acf-and-pacf",
    "href": "ASV.html#check-acf-and-pacf",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check ACF and PACF",
    "text": "Check ACF and PACF\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit1)\n\n\n\n\n\nSince there is no major seasonal pattern. We use differencing directly"
  },
  {
    "objectID": "ASV.html#differencing-acf-and-pacf",
    "href": "ASV.html#differencing-acf-and-pacf",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Differencing ACF and PACF",
    "text": "Differencing ACF and PACF\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(res.fit1, differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(res.fit1, differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is mostly stationary for us to continue From ACF and PACF, it seems that we can consider: p = 1,2,3 q = 1,2,3. We use first order of differencing so d = 1. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics",
    "href": "ASV.html#model-diagnotistics",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n-2185.822\n-2155.919\n-2185.743\n\n\n2\n2\n2\n-2163.551\n-2138.637\n-2163.495\n\n\n2\n3\n2\n-2031.719\n-2006.809\n-2031.663\n\n\n2\n1\n3\n-2183.857\n-2148.971\n-2183.753\n\n\n2\n2\n3\n-2168.936\n-2139.039\n-2168.858\n\n\n2\n3\n3\n-2024.416\n-1994.525\n-2024.338\n\n\n2\n1\n4\n-2194.287\n-2154.417\n-2194.153\n\n\n2\n2\n4\n-2166.447\n-2131.567\n-2166.343\n\n\n3\n1\n2\n-2183.841\n-2148.954\n-2183.736\n\n\n3\n2\n2\n-2164.637\n-2134.740\n-2164.558\n\n\n3\n3\n2\n-2075.699\n-2045.808\n-2075.621\n\n\n3\n1\n3\n-2183.029\n-2143.159\n-2182.895\n\n\n3\n2\n3\n-2167.183\n-2132.303\n-2167.078\n\n\n3\n1\n4\n-2192.632\n-2147.778\n-2192.464\n\n\n4\n1\n2\n-2192.881\n-2153.011\n-2192.747\n\n\n4\n2\n2\n-2171.137\n-2136.257\n-2171.032\n\n\n4\n1\n3\n-2190.880\n-2146.026\n-2190.712\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 2 1 4 -2194.287 -2154.417 -2194.153\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -2185.822 -2155.919 -2185.743\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 2 1 4 -2194.287 -2154.417 -2194.153\n\n\nFrom here, we have two potential ones: (2,1,2) and (2,1,4)"
  },
  {
    "objectID": "ASV.html#compare-the-models",
    "href": "ASV.html#compare-the-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare The models",
    "text": "Compare The models\n\n\nCode\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 2,1,4)) \n\n\n\n\n\nCode\nmodel_output12 <- capture.output(sarima(res.fit1, 2,1,2)) \n\n\n\n\n\n\n\nCode\ncat(model_output11[90:123], model_output11[length(model_output11)], sep = \"\\n\")\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ma1     ma2      ma3     ma4  constant\n      0.8239  0.0553  -1.2526  0.1997  -0.1235  0.1795     1e-04\ns.e.  0.1938  0.1511   0.1916  0.2323   0.0649  0.0393     1e-04\n\nsigma^2 estimated as 0.007533:  log likelihood = 1105.14,  aic = -2194.29\n\n$degrees_of_freedom\n[1] 1072\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.8239 0.1938  4.2522  0.0000\nar2        0.0553 0.1511  0.3662  0.7143\nma1       -1.2526 0.1916 -6.5386  0.0000\nma2        0.1997 0.2323  0.8597  0.3901\nma3       -0.1235 0.0649 -1.9028  0.0573\nma4        0.1795 0.0393  4.5726  0.0000\nconstant   0.0001 0.0001  0.6131  0.5399\n\n$AIC\n[1] -2.03363\n\n$AICc\n[1] -2.033534\n\n$BIC\n[1] -1.996679\n\n\nCode\ncat(model_output12[40:70], model_output12[length(model_output12)], sep = \"\\n\")\n\n\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ma1      ma2  constant\n      0.0580  0.4279  -0.4812  -0.4893     1e-04\ns.e.  0.1428  0.0806   0.1486   0.1408     2e-04\n\nsigma^2 estimated as 0.007625:  log likelihood = 1098.91,  aic = -2185.82\n\n$degrees_of_freedom\n[1] 1074\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.0580 0.1428  0.4063  0.6846\nar2        0.4279 0.0806  5.3077  0.0000\nma1       -0.4812 0.1486 -3.2384  0.0012\nma2       -0.4893 0.1408 -3.4748  0.0005\nconstant   0.0001 0.0002  0.6602  0.5093\n\n$AIC\n[1] -2.025785\n\n$AICc\n[1] -2.025733\n\n$BIC\n[1] -1.998071\n\n\nBased on this, I think the second one (2,1,4) is slightly better with less correlation and smaller AIC value."
  },
  {
    "objectID": "ASV.html#cross-validation",
    "href": "ASV.html#cross-validation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit1)\nn *0.3 \n\n\n[1] 324\n\n\n\n\nCode\nk=324\n \nrmse1 <- matrix(NA, 53,4)\nrmse2 <- matrix(NA,53,4)\nrmse3 <- matrix(NA,53,4)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:53)\n{\n  xtrain <- window(res.fit1, end=st + i-1)\n  xtest <- window(res.fit1, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,4),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:4, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\n\n\nCode\ncolMeans( rmse1,na.rm=TRUE)\n\n\n[1] 0.01282484 0.01701977 0.02336799 0.02397791\n\n\nCode\ncolMeans( rmse2,na.rm=TRUE)\n\n\n[1] 0.01368543 0.02028240 0.02719078 0.02635619\n\n\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (2,1,4) performs better with smaller errors."
  },
  {
    "objectID": "ASV.html#fit-the-model",
    "href": "ASV.html#fit-the-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model",
    "text": "Fit the model\n\n\nCode\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"homevalue\"],order=c(2,1,4),xreg=xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"homevalue\"] \nRegression with ARIMA(2,1,4) errors \n\nCoefficients:\n         ar1     ar2      ma1     ma2      ma3     ma4  saleprice  rentalprice\n      0.8151  0.0766  -1.2454  0.1814  -0.1233  0.1903     0.8651       0.1058\ns.e.  0.1952  0.1493   0.1934  0.2331   0.0684  0.0400     0.0433       0.0358\n\nsigma^2 = 0.007576:  log likelihood = 1106.15\nAIC=-2194.31   AICc=-2194.14   BIC=-2149.45\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.002244825 0.08667672 0.02677427 0.01221504 0.2010633 0.3656567\n                     ACF1\nTraining set -0.001078051"
  },
  {
    "objectID": "ASV.html#write-down-the-equation",
    "href": "ASV.html#write-down-the-equation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Write Down The equation:",
    "text": "Write Down The equation:\nGiven that y_t represents the log-transformed homevalue at time t, the ARIMA(2,1,4) can be:\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\nBased on the summary, my equation is\n(1 - 0.8151B - 0.0766B²)(1 - B)yₜ = α + 0.8651 * salepriceₜ + 0.1058 * rentalpriceₜ + (1 - 1.2454B + 0.1814B² - 0.1233B³ + 0.1903B⁴)εₜ"
  },
  {
    "objectID": "ASV.html#forecasting",
    "href": "ASV.html#forecasting",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nCode\nspfit<-auto.arima(lg.dd.ts1[, \"saleprice\"]) \nsummary(spfit) \n\n\nSeries: lg.dd.ts1[, \"saleprice\"] \nARIMA(0,1,0) \n\nsigma^2 = 0.004229:  log likelihood = 1417.77\nAIC=-2833.53   AICc=-2833.53   BIC=-2828.55\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.0005143487 0.06500146 0.02430292 0.002720093 0.1819437 0.3527218\n                   ACF1\nTraining set 0.05157596\n\n\n\n\nCode\nfsp<-forecast(spfit,80) #obtaining forecasts\n\nrpfit<-auto.arima(lg.dd.ts1[, \"rentalprice\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n\n\nSeries: lg.dd.ts1[, \"rentalprice\"] \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.1423\ns.e.   0.0308\n\nsigma^2 = 0.006889:  log likelihood = 1155.01\nAIC=-2306.01   AICc=-2306   BIC=-2296.05\n\nTraining set error measures:\n                     ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.00063182 0.08292248 0.02052085 0.001505318 0.2698055 0.3434587\n                    ACF1\nTraining set 0.001944753\n\n\n\n\nCode\nfrp<-forecast(rpfit,80)\n\nfxreg <- cbind(saleprice = fsp$mean, \n              rentalprice = frp$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")"
  },
  {
    "objectID": "ASV.html#creating-ts-objects-for-variables",
    "href": "ASV.html#creating-ts-objects-for-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Creating Ts Objects For Variables",
    "text": "Creating Ts Objects For Variables\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nrate <- ts(df6_sub$PSAVERT, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\n\n\nsaving rate, income, saveing can be interesting to be evaluated"
  },
  {
    "objectID": "ASV.html#combine-the-varaibles",
    "href": "ASV.html#combine-the-varaibles",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Varaibles",
    "text": "Combine the Varaibles\n\n\nCode\ndd2<-data.frame(rate,saving,income,df1_sub$DATE)\n\ncolnames(dd2)<-c(\"saving_rate\",\"household_saving\",\"household_income\",'date')\n\nknitr::kable(head(dd2))\n\n\n\n\n\nsaving_rate\nhousehold_saving\nhousehold_income\ndate\n\n\n\n\n9.4\n431.693\n58153\n1992-01-01\n\n\n9.8\n377.899\n57843\n1993-01-01\n\n\n9.7\n345.198\n58515\n1994-01-01\n\n\n9.9\n366.774\n60348\n1995-01-01\n\n\n9.9\n353.639\n61225\n1996-01-01\n\n\n10.1\n339.566\n62484\n1997-01-01"
  },
  {
    "objectID": "ASV.html#make-a-log-transformation-for-seasonal-pattern",
    "href": "ASV.html#make-a-log-transformation-for-seasonal-pattern",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Make a Log transformation For Seasonal Pattern",
    "text": "Make a Log transformation For Seasonal Pattern\n\n\nCode\nlg.dd2 <- data.frame(\"date\" =dd2$date,\"saving_rate\"=log(dd2$saving_rate),\"household_saving\"=log(dd2$household_saving),\n                                        \"household_income\"=log(dd2$household_income))\n\n#### converting to time series component #########\nlg.dd.ts2<-ts(lg.dd2,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts2[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model-1",
    "href": "ASV.html#auto-fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit The Model",
    "text": "Auto Fit The Model\n\n\nCode\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\nfit2 <- auto.arima(lg.dd.ts2[, \"saving_rate\"], xreg = xreg2)\nsummary(fit2)\n\n\nSeries: lg.dd.ts2[, \"saving_rate\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      household_income  household_saving\n                0.3057           -0.1981\ns.e.            0.0191            0.0328\n\nsigma^2 = 0.01216:  log likelihood = 23.83\nAIC=-41.66   AICc=-40.7   BIC=-37.56\n\nTraining set error measures:\n                       ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set 0.0001058585 0.1063899 0.08808777 -0.2509742 4.206073 0.7528751\n                  ACF1\nTraining set 0.1430342"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model-1",
    "href": "ASV.html#manual-fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The model",
    "text": "Manual Fit The model\n\n\nCode\nfit.reg2 <- lm( saving_rate ~  household_saving + household_income, data=lg.dd.ts2)\nsummary(fit.reg2)\n\n\n\nCall:\nlm(formula = saving_rate ~ household_saving + household_income, \n    data = lg.dd.ts2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20674 -0.06359 -0.01653  0.06697  0.23007 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.24855    4.53045   1.159    0.257    \nhousehold_saving -0.18141    0.03676  -4.934    4e-05 ***\nhousehold_income -0.17843    0.41833  -0.427    0.673    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1096 on 26 degrees of freedom\nMultiple R-squared:  0.5556,    Adjusted R-squared:  0.5214 \nF-statistic: 16.25 on 2 and 26 DF,  p-value: 2.637e-05\n\n\nIt seems that the household income did not play any important role. Therefore, We can consider to remove it."
  },
  {
    "objectID": "ASV.html#manual-fit-again-without-insignificant-variable",
    "href": "ASV.html#manual-fit-again-without-insignificant-variable",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit Again Without Insignificant Variable",
    "text": "Manual Fit Again Without Insignificant Variable\n\n\nCode\nfit.reg2 <- lm( saving_rate ~  household_saving, data=lg.dd.ts2)\nsummary(fit.reg2)\n\n\n\nCall:\nlm(formula = saving_rate ~ household_saving, data = lg.dd.ts2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20039 -0.07323 -0.01942  0.07709  0.22147 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.3183     0.2097  15.828 3.48e-15 ***\nhousehold_saving  -0.1882     0.0326  -5.774 3.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1079 on 27 degrees of freedom\nMultiple R-squared:  0.5525,    Adjusted R-squared:  0.5359 \nF-statistic: 33.33 on 1 and 27 DF,  p-value: 3.85e-06\n\n\nWe can see that the variables are pretty significant."
  },
  {
    "objectID": "ASV.html#check-residuals",
    "href": "ASV.html#check-residuals",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check residuals",
    "text": "Check residuals\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 5.2247, df = 6, p-value = 0.5153\n\nModel df: 0.   Total lags used: 6\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#acf-and-pacf-check",
    "href": "ASV.html#acf-and-pacf-check",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "ACF and PACF Check",
    "text": "ACF and PACF Check\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit2<-ts(residuals(fit.reg2),frequency = 2)\n\n############## Then look at the residuals ############\nggAcf(res.fit2)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit2)\n\n\n\n\n\nSince there is no major seasonal pattern. And it seems that the data is already stationary"
  },
  {
    "objectID": "ASV.html#no-need-for-seasonal-differencing",
    "href": "ASV.html#no-need-for-seasonal-differencing",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "No Need For Seasonal Differencing",
    "text": "No Need For Seasonal Differencing\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit2) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit2) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is mostly stationary for us to continue From ACF and PACF, it seems that we can consider: p = 1,2 q = 1,2. We use first order of differencing so d = 0. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics-1",
    "href": "ASV.html#model-diagnotistics-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit2,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n2\n-35.62908\n-27.635848\n-31.629075\n\n\n2\n2\n2\n-30.44483\n-23.965644\n-27.587686\n\n\n2\n3\n2\n-14.32464\n-8.034157\n-11.324640\n\n\n2\n1\n3\n-34.33747\n-25.012041\n-28.737473\n\n\n2\n2\n3\n-27.95423\n-20.179204\n-23.754225\n\n\n2\n3\n3\n-11.59444\n-4.045857\n-7.173384\n\n\n2\n1\n4\n-32.35833\n-21.700692\n-24.779380\n\n\n2\n2\n4\n-27.40495\n-18.334089\n-21.510210\n\n\n3\n1\n2\n-34.29408\n-24.968649\n-28.694081\n\n\n3\n2\n2\n-29.01339\n-21.238370\n-24.813392\n\n\n3\n3\n2\n-13.51554\n-5.966961\n-9.094488\n\n\n3\n1\n3\n-32.86641\n-22.208777\n-25.287466\n\n\n3\n2\n3\n-27.18329\n-18.112427\n-21.288549\n\n\n3\n1\n4\n-33.13762\n-21.147775\n-23.137616\n\n\n4\n1\n2\n-34.88942\n-24.231784\n-27.310473\n\n\n4\n2\n2\n-27.67647\n-18.605616\n-21.781737\n\n\n4\n1\n3\n-33.00327\n-21.013431\n-23.003272\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n1 2 1 2 -35.62907 -27.63585 -31.62907\n\n\nFrom here, we have two potential ones: (2,1,2) and (0,0,0) from part 1"
  },
  {
    "objectID": "ASV.html#compare-the-models-1",
    "href": "ASV.html#compare-the-models-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare The Models",
    "text": "Compare The Models\n\n\nCode\nset.seed(236)\n\nmodel_output21 <- capture.output(sarima(res.fit2, 2,1,2)) \n\n\n\n\n\nCode\nmodel_output22 <- capture.output(sarima(res.fit2, 0,0,0)) \n\n\n\n\n\n\n\nCode\ncat(model_output21[145:160], model_output21[length(model_output21)], sep = \"\\n\")\n\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.5752 0.3322 -1.7319  0.0967\nar2       -0.1902 0.2359 -0.8064  0.4283\nma1       -0.2555 0.2865 -0.8917  0.3818\nma2       -0.7445 0.2726 -2.7308  0.0119\nconstant  -0.0032 0.0022 -1.4586  0.1582\n\n$AIC\n[1] -1.272467\n\n$AICc\n[1] -1.175064\n\n$BIC\n[1] -0.9869946\n\n\n\n\nCode\ncat(model_output22[20:38], model_output22[length(model_output22)], sep = \"\\n\")\n\n\n\nsigma^2 estimated as 0.01084:  log likelihood = 24.46,  aic = -44.92\n\n$degrees_of_freedom\n[1] 28\n\n$ttable\n      Estimate     SE t.value p.value\nxmean        0 0.0193       0       1\n\n$AIC\n[1] -1.548841\n\n$AICc\n[1] -1.543732\n\n$BIC\n[1] -1.454545\n\n\nBased on this, I think the first one (2,1,2) is slightly better with less correlation and closer to the significant level."
  },
  {
    "objectID": "ASV.html#cross-validation-1",
    "href": "ASV.html#cross-validation-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit2)\nn *0.3 \n\n\n[1] 8.7\n\n\n\n\nCode\nk=9\n \nrmse1 <- matrix(NA, 40,2)\nrmse2 <- matrix(NA,40,2)\nrmse3 <- matrix(NA,40,2)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:12)\n{\n  xtrain <- window(res.fit2, end=st + i-1)\n  xtest <- window(res.fit2, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:2, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:2, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\nHere, although\n\n\nCode\ncolMeans( rmse1,na.rm=TRUE)\n\n\n[1] 0.09762763 0.10927137\n\n\nCode\ncolMeans( rmse2,na.rm=TRUE)\n\n\n[1] 0.09827109 0.07812204\n\n\nBased on the cross validation, overall, the two method is simiarly within expectation. We can see that although model (0,0,0) has smaller errors after, the dataset is relatively biased due to the size of the date overlapping. We should still use the fit1 which is (2,1,2) to perform later analysis."
  },
  {
    "objectID": "ASV.html#fit-the-model-with-the-best-one",
    "href": "ASV.html#fit-the-model-with-the-best-one",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model With The Best One",
    "text": "Fit the model With The Best One\n\n\nCode\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\n\nfit2 <- Arima(lg.dd.ts2[, \"saving_rate\"],order=c(2,1,2),xreg=xreg2)\nsummary(fit2)\n\n\nSeries: lg.dd.ts2[, \"saving_rate\"] \nRegression with ARIMA(2,1,2) errors \n\nCoefficients:\n         ar1      ar2      ma1     ma2  household_income  household_saving\n      0.4885  -0.4602  -1.2187  0.5136            0.1188           -0.1330\ns.e.  0.3344   0.2419   0.3265  0.3180            0.3668            0.0648\n\nsigma^2 = 0.0124:  log likelihood = 24.41\nAIC=-34.81   AICc=-29.21   BIC=-25.48\n\nTraining set error measures:\n                      ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set -0.01566954 0.0969955 0.06932111 -0.8864638 3.345804 0.5924788\n                    ACF1\nTraining set -0.07898355"
  },
  {
    "objectID": "ASV.html#write-doen-the-equation",
    "href": "ASV.html#write-doen-the-equation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Write Doen The equation:",
    "text": "Write Doen The equation:\nGiven that y_t represents the log-transformed homevalue at time t, the ARIMA(2,1,4) can be:\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\nBased on the summary, my equation for this model is\n(1 - 0.4885B - (-0.4602)B^2)(1 - B)y_t = + 0.1188 _t - 0.1330 _t + (1 - 1.2187B + 0.5136B^2)_t"
  },
  {
    "objectID": "ASV.html#forecasting-1",
    "href": "ASV.html#forecasting-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nCode\nhsfit<-auto.arima(lg.dd.ts2[, \"household_saving\"]) \nsummary(hsfit) \n\n\nSeries: lg.dd.ts2[, \"household_saving\"] \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0701\ns.e.  0.0447\n\nsigma^2 = 0.05808:  log likelihood = 0.62\nAIC=2.76   AICc=3.24   BIC=5.42\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.0002068136 0.2325379 0.1666254 -0.1667371 2.642544 0.4511904\n                    ACF1\nTraining set -0.04495977\n\n\n\n\nCode\nfhs<-forecast(hsfit,12) #obtaining forecasts\n\nhifit<-auto.arima(lg.dd.ts2[, \"household_income\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n\n\nSeries: lg.dd.ts1[, \"rentalprice\"] \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.1423\ns.e.   0.0308\n\nsigma^2 = 0.006889:  log likelihood = 1155.01\nAIC=-2306.01   AICc=-2306   BIC=-2296.05\n\nTraining set error measures:\n                     ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.00063182 0.08292248 0.02052085 0.001505318 0.2698055 0.3434587\n                    ACF1\nTraining set 0.001944753\n\n\n\n\nCode\nfhi<-forecast(hifit,12)\n\nfxreg <- cbind(household_income = fhi$mean,household_saving = fhs$mean\n              ) #fimp$mean gives the forecasted values\n\nfcast <- forecast(fit2, xreg=fxreg,12) \nautoplot(fcast, main=\"Forecast of Saving Rate\") + xlab(\"Year\") +\n  ylab(\"Saving Rate\")"
  },
  {
    "objectID": "ASV.html#transform-all-to-time-series",
    "href": "ASV.html#transform-all-to-time-series",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Transform all to time series",
    "text": "Transform all to time series\n\n\nCode\nsaving =ts(df1$W398RC1A027NBEA)\nincome =ts(df2$MEHOINUSA672N)\nsale = ts(df3$MSPUS)\ngini =ts(df4$SIPOVGINIUSA)\nafford =ts(df5$FIXHAI)\nsaverate =ts(df6$PSAVERT)\ngdp =ts(df7$A191RI1Q225SBEA)\n\nsaleprice =ts(df8$Mean.Sale.Price)\nhomevalue =ts(df8$Mean.Home.Value)\nrentalprice =ts(df8$mean)"
  },
  {
    "objectID": "ASV.html#prepare-variables-saving-rate-sale-price-gdp-deflator",
    "href": "ASV.html#prepare-variables-saving-rate-sale-price-gdp-deflator",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Prepare Variables: saving rate, sale price, gdp deflator",
    "text": "Prepare Variables: saving rate, sale price, gdp deflator\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1965-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 5\n\n# Create ts objects\nsale_price <- ts(df3_sub$MSPUS, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nsaving_rate <- ts(df6_sub$PSAVERT, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(1965, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-variables",
    "href": "ASV.html#combine-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine Variables",
    "text": "Combine Variables\n\n\nCode\ndd3<-data.frame(sale_price,saving_rate,gdp,DATE)\n\ncolnames(dd3)<-c(\"sale_price\",\"saving_rate\",\"gdp_deflator\",'date')\n\nknitr::kable(head(dd3))\n\n\n\n\n\nsale_price\nsaving_rate\ngdp_deflator\ndate\n\n\n\n\n20200\n12.1\n2.0\n-1826\n\n\n19800\n10.7\n1.8\n-1736\n\n\n20200\n10.8\n1.6\n-1645\n\n\n20300\n10.2\n2.8\n-1553\n\n\n21000\n11.2\n2.6\n-1461\n\n\n22100\n12.1\n3.3\n-1371"
  },
  {
    "objectID": "ASV.html#plot-the-variables-together",
    "href": "ASV.html#plot-the-variables-together",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Plot the Variables Together",
    "text": "Plot the Variables Together\n\n\nCode\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale_price\"=log(dd3$sale_price),\"saving_rate\"=log(dd3$saving_rate),\n                                        \"gdp_deflator\"=log(dd3$gdp_deflator))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 5)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate in USA\")"
  },
  {
    "objectID": "ASV.html#finding-out-the-best-p",
    "href": "ASV.html#finding-out-the-best-p",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Finding out the best p:",
    "text": "Finding out the best p:\n\n\nCode\nVARselect(dd3[, c(2:4)], lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     3      1      1      3 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.398087e+01 1.398082e+01 1.391871e+01 1.393094e+01 1.395240e+01\nHQ(n)  1.406205e+01 1.411071e+01 1.409731e+01 1.415825e+01 1.422842e+01\nSC(n)  1.418295e+01 1.430414e+01 1.436328e+01 1.449676e+01 1.463946e+01\nFPE(n) 1.179834e+06 1.179822e+06 1.108866e+06 1.122687e+06 1.147305e+06\n                  6            7            8            9           10\nAIC(n) 1.398304e+01 1.403364e+01 1.406570e+01 1.408240e+01 1.412592e+01\nHQ(n)  1.430777e+01 1.440708e+01 1.448785e+01 1.455325e+01 1.464549e+01\nSC(n)  1.479134e+01 1.496320e+01 1.511650e+01 1.525444e+01 1.541922e+01\nFPE(n) 1.183385e+06 1.245368e+06 1.286686e+06 1.309306e+06 1.368790e+06\n\n\nFrom the results, we can see that 3,4 have relatively smaller criterias: We can fit several models with p=1, 3, and 4.=> VAR(1), VAR(3), VAR(4)"
  },
  {
    "objectID": "ASV.html#fitting-a-var-model-with-different-p",
    "href": "ASV.html#fitting-a-var-model-with-different-p",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting a VAR model with different p:",
    "text": "Fitting a VAR model with different p:\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 275 \nLog Likelihood: -3075.874 \nRoots of the characteristic polynomial:\n0.9816 0.8407 0.7771\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 1, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   8.296e-01  3.447e-02  24.070  < 2e-16 ***\ngdp_deflator.l1 -1.619e-02  2.129e-02  -0.761  0.44760    \ndate.l1          9.867e-06  9.057e-06   1.089  0.27692    \nconst            2.241e+00  4.772e-01   4.696 4.23e-06 ***\ntrend           -2.320e-03  7.247e-04  -3.202  0.00153 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7166 on 270 degrees of freedom\nMultiple R-Squared: 0.8099, Adjusted R-squared: 0.807 \nF-statistic: 287.5 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1  -8.763e-02  5.740e-02  -1.527  0.12803    \ngdp_deflator.l1  8.022e-01  3.545e-02  22.625  < 2e-16 ***\ndate.l1         -4.714e-05  1.508e-05  -3.126  0.00197 ** \nconst            2.191e+00  7.946e-01   2.757  0.00623 ** \ntrend           -7.055e-04  1.207e-03  -0.585  0.55934    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.193 on 270 degrees of freedom\nMultiple R-Squared: 0.7827, Adjusted R-squared: 0.7795 \nF-statistic: 243.1 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1    90.19249   59.02459   1.528    0.128    \ngdp_deflator.l1  -16.61672   36.45826  -0.456    0.649    \ndate.l1            0.96759    0.01551  62.390   <2e-16 ***\nconst           -811.20104  817.15553  -0.993    0.322    \ntrend              0.63544    1.24101   0.512    0.609    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1227 on 270 degrees of freedom\nMultiple R-Squared: 0.961,  Adjusted R-squared: 0.9605 \nF-statistic:  1665 on 4 and 270 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate     0.513560    -0.007928 -8.194e+00\ngdp_deflator   -0.007928     1.424116  3.316e+01\ndate           -8.193763    33.159659  1.506e+06\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator      date\nsaving_rate     1.000000    -0.009271 -0.009317\ngdp_deflator   -0.009271     1.000000  0.022643\ndate           -0.009317     0.022643  1.000000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 273 \nLog Likelihood: -3025.413 \nRoots of the characteristic polynomial:\n0.9806 0.9118 0.8704 0.4113 0.4113 0.3397 0.3397 0.2947 0.2947\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 3, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   6.770e-01  6.060e-02  11.172  < 2e-16 ***\ngdp_deflator.l1 -1.671e-02  3.692e-02  -0.452 0.651324    \ndate.l1         -4.871e-06  3.464e-05  -0.141 0.888274    \nsaving_rate.l2   9.846e-02  7.264e-02   1.356 0.176419    \ngdp_deflator.l2  8.798e-02  4.378e-02   2.010 0.045496 *  \ndate.l2         -2.431e-05  4.825e-05  -0.504 0.614747    \nsaving_rate.l3   9.630e-02  6.066e-02   1.588 0.113581    \ngdp_deflator.l3 -9.371e-02  3.629e-02  -2.582 0.010364 *  \ndate.l3          3.352e-05  3.475e-05   0.965 0.335646    \nconst            1.786e+00  5.123e-01   3.487 0.000573 ***\ntrend           -2.034e-03  7.360e-04  -2.763 0.006128 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6969 on 262 degrees of freedom\nMultiple R-Squared: 0.8253, Adjusted R-squared: 0.8187 \nF-statistic: 123.8 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1  -2.610e-03  1.015e-01  -0.026  0.97949    \ngdp_deflator.l1  6.218e-01  6.182e-02  10.059  < 2e-16 ***\ndate.l1          2.358e-05  5.799e-05   0.407  0.68463    \nsaving_rate.l2  -1.154e-01  1.216e-01  -0.949  0.34333    \ngdp_deflator.l2  4.379e-02  7.330e-02   0.597  0.55073    \ndate.l2         -7.619e-06  8.078e-05  -0.094  0.92493    \nsaving_rate.l3   6.686e-03  1.016e-01   0.066  0.94756    \ngdp_deflator.l3  1.687e-01  6.076e-02   2.776  0.00591 ** \ndate.l3         -5.886e-05  5.817e-05  -1.012  0.31256    \nconst            2.347e+00  8.578e-01   2.736  0.00663 ** \ntrend           -8.907e-04  1.232e-03  -0.723  0.47040    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.167 on 262 degrees of freedom\nMultiple R-Squared: 0.7974, Adjusted R-squared: 0.7897 \nF-statistic: 103.1 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   5.357e+01  1.048e+02   0.511  0.60968    \ngdp_deflator.l1 -7.718e+00  6.386e+01  -0.121  0.90389    \ndate.l1          9.861e-01  5.990e-02  16.464  < 2e-16 ***\nsaving_rate.l2  -5.835e+01  1.256e+02  -0.464  0.64269    \ngdp_deflator.l2 -2.332e+02  7.571e+01  -3.080  0.00229 ** \ndate.l2         -7.233e-03  8.344e-02  -0.087  0.93099    \nsaving_rate.l3   1.095e+02  1.049e+02   1.044  0.29752    \ngdp_deflator.l3  2.502e+02  6.276e+01   3.987 8.69e-05 ***\ndate.l3         -8.317e-03  6.009e-02  -0.138  0.89003    \nconst           -1.124e+03  8.860e+02  -1.269  0.20574    \ntrend            8.568e-01  1.273e+00   0.673  0.50141    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1205 on 262 degrees of freedom\nMultiple R-Squared: 0.963,  Adjusted R-squared: 0.9616 \nF-statistic: 681.9 on 10 and 262 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate      0.48565      0.01418      20.31\ngdp_deflator     0.01418      1.36133     -14.92\ndate            20.30792    -14.91718 1452449.04\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator     date\nsaving_rate      1.00000      0.01744  0.02418\ngdp_deflator     0.01744      1.00000 -0.01061\ndate             0.02418     -0.01061  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:4)], p=4, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving_rate, gdp_deflator, date \nDeterministic variables: both \nSample size: 272 \nLog Likelihood: -3007.166 \nRoots of the characteristic polynomial:\n0.9868 0.9304 0.8686 0.5835 0.5835 0.5395 0.4834 0.4834 0.416 0.416 0.1904 0.1904\nCall:\nvars::VAR(y = dd3[, c(2:4)], p = 4, type = \"both\")\n\n\nEstimation results for equation saving_rate: \n============================================ \nsaving_rate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   6.421e-01  6.088e-02  10.546  < 2e-16 ***\ngdp_deflator.l1 -1.304e-02  3.695e-02  -0.353  0.72434    \ndate.l1          1.551e-06  3.519e-05   0.044  0.96487    \nsaving_rate.l2   8.610e-02  7.251e-02   1.187  0.23615    \ngdp_deflator.l2  9.791e-02  4.326e-02   2.263  0.02444 *  \ndate.l2         -3.474e-05  4.867e-05  -0.714  0.47604    \nsaving_rate.l3  -4.977e-03  7.196e-02  -0.069  0.94491    \ngdp_deflator.l3 -5.673e-02  4.425e-02  -1.282  0.20102    \ndate.l3          2.894e-05  4.753e-05   0.609  0.54317    \nsaving_rate.l4   1.658e-01  6.035e-02   2.747  0.00645 ** \ngdp_deflator.l4 -5.754e-02  3.783e-02  -1.521  0.12943    \ndate.l4          5.146e-06  3.435e-05   0.150  0.88102    \nconst            1.631e+00  5.249e-01   3.108  0.00210 ** \ntrend           -1.970e-03  7.385e-04  -2.668  0.00811 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6861 on 258 degrees of freedom\nMultiple R-Squared: 0.833,  Adjusted R-squared: 0.8245 \nF-statistic: 98.96 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   1.883e-02  1.041e-01   0.181  0.85656    \ngdp_deflator.l1  6.149e-01  6.315e-02   9.738  < 2e-16 ***\ndate.l1          1.349e-05  6.014e-05   0.224  0.82265    \nsaving_rate.l2  -1.195e-01  1.239e-01  -0.964  0.33594    \ngdp_deflator.l2  3.769e-02  7.393e-02   0.510  0.61066    \ndate.l2          5.842e-06  8.318e-05   0.070  0.94406    \nsaving_rate.l3   5.427e-02  1.230e-01   0.441  0.65940    \ngdp_deflator.l3  1.298e-01  7.563e-02   1.716  0.08737 .  \ndate.l3         -6.911e-05  8.124e-05  -0.851  0.39568    \nsaving_rate.l4  -7.248e-02  1.031e-01  -0.703  0.48287    \ngdp_deflator.l4  5.796e-02  6.465e-02   0.896  0.37084    \ndate.l4          8.775e-06  5.870e-05   0.149  0.88128    \nconst            2.407e+00  8.971e-01   2.683  0.00776 ** \ntrend           -9.482e-04  1.262e-03  -0.751  0.45319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.173 on 258 degrees of freedom\nMultiple R-Squared: 0.7984, Adjusted R-squared: 0.7882 \nF-statistic: 78.58 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving_rate.l1 + gdp_deflator.l1 + date.l1 + saving_rate.l2 + gdp_deflator.l2 + date.l2 + saving_rate.l3 + gdp_deflator.l3 + date.l3 + saving_rate.l4 + gdp_deflator.l4 + date.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(>|t|)    \nsaving_rate.l1   3.404e+01  1.071e+02   0.318  0.75097    \ngdp_deflator.l1  8.281e+00  6.502e+01   0.127  0.89876    \ndate.l1          1.012e+00  6.192e-02  16.340  < 2e-16 ***\nsaving_rate.l2  -3.016e+01  1.276e+02  -0.236  0.81334    \ngdp_deflator.l2 -2.322e+02  7.613e+01  -3.050  0.00253 ** \ndate.l2         -3.837e-02  8.566e-02  -0.448  0.65459    \nsaving_rate.l3   1.588e+02  1.266e+02   1.254  0.21100    \ngdp_deflator.l3  3.224e+02  7.788e+01   4.140  4.7e-05 ***\ndate.l3          2.568e-03  8.365e-02   0.031  0.97553    \nsaving_rate.l4  -7.421e+01  1.062e+02  -0.699  0.48533    \ngdp_deflator.l4 -1.040e+02  6.657e+01  -1.562  0.11947    \ndate.l4         -6.418e-03  6.044e-02  -0.106  0.91552    \nconst           -8.460e+02  9.237e+02  -0.916  0.36063    \ntrend            6.347e-01  1.300e+00   0.488  0.62569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1208 on 258 degrees of freedom\nMultiple R-Squared: 0.9632, Adjusted R-squared: 0.9613 \nF-statistic: 518.9 on 13 and 258 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             saving_rate gdp_deflator       date\nsaving_rate      0.47080      0.02528  1.839e+01\ngdp_deflator     0.02528      1.37521 -9.918e+00\ndate            18.38808     -9.91797  1.458e+06\n\nCorrelation matrix of residuals:\n             saving_rate gdp_deflator      date\nsaving_rate      1.00000     0.031413  0.022193\ngdp_deflator     0.03141     1.000000 -0.007004\ndate             0.02219    -0.007004  1.000000\n\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one."
  },
  {
    "objectID": "ASV.html#cross-validations",
    "href": "ASV.html#cross-validations",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross Validations",
    "text": "Cross Validations\n\nDefine length\n\n\nCode\nn=length(dd3$gdp_deflator)\nk=85 #19*4\n\nn*0.3\n\n\n[1] 82.8\n\n\n\n\nCode\ndat = ts(dd3[,c(1,2,3)])\n\n\n\n\nCross Validation For gdp deflator\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nNormalize the RMSE For Ploting\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare the errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 3.802198\n\n\nCode\nerror2\n\n\n[1] 3.871752\n\n\nThe first one is better but not too different for GDP Deflator\n\n\n\nCross Validation For Saving rate\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$saving_rate-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$saving_rate-xtest)^2))\n\n}\n\n\n\nNormalize the errors for ploting\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare errors\nLet us reorganize the errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 4.48152\n\n\nCode\nerror2\n\n\n[1] 4.607221\n\n\nThe first one is also better for saving rate\n\n\n\nCross Validation For Sale price\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$sale_price-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$sale_price-xtest)^2))\n\n}\n\n\n\nNormalize The Errors\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nCompare The Errors\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 125445.5\n\n\nCode\nerror2\n\n\n[1] 125987.7\n\n\nOverall, we can see that in my case, all variables have smaller RMSE with VAR(3)"
  },
  {
    "objectID": "ASV.html#forecasting-2",
    "href": "ASV.html#forecasting-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting",
    "text": "Forecasting\nForecast using the best one which is VAR(3)\n\n\nCode\nfit1 <- vars::VAR(dat[,1:3], p=3, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n\n\n         fcst    lower     upper       CI\n[1,] 7.894615 5.607798 10.181432 2.286817\n[2,] 7.282398 4.582903  9.981893 2.699495\n[3,] 7.492970 4.611125 10.374815 2.881845\n[4,] 7.372062 4.294439 10.449686 3.077623\n[5,] 7.295611 4.073932 10.517289 3.221678\n[6,] 7.253375 3.915601 10.591150 3.337774\n[7,] 7.186365 3.748134 10.624597 3.438231\n[8,] 7.131866 3.607845 10.655887 3.524021\n\n\n\nCheck Corresponind Forecasting\n\n\nCode\nfcast1$fcst$sale_price\n\n\n         fcst     lower     upper       CI\n[1,] 52039.84  11740.19  92339.49 40299.65\n[2,] 34492.15 -20395.42  89379.73 54887.58\n[3,] 38522.27 -27352.88 104397.41 65875.14\n[4,] 39360.19 -35082.57 113802.95 74442.76\n[5,] 37722.35 -44022.64 119467.33 81744.99\n[6,] 37462.86 -50890.48 125816.19 88353.33\n[7,] 36986.76 -57304.47 131277.99 94291.23\n[8,] 36584.86 -63106.89 136276.61 99691.75\n\n\n\n\nCode\nfcast1$fcst$saving_rate\n\n\n         fcst    lower    upper       CI\n[1,] 8.674407 7.292309 10.05651 1.382098\n[2,] 9.136267 7.444615 10.82792 1.691652\n[3,] 9.041880 7.142789 10.94097 1.899091\n[4,] 9.091204 7.003047 11.17936 2.088156\n[5,] 9.210690 6.970091 11.45129 2.240599\n[6,] 9.265283 6.897064 11.63350 2.368219\n[7,] 9.332344 6.853394 11.81129 2.478950\n[8,] 9.401929 6.826736 11.97712 2.575193"
  },
  {
    "objectID": "ASV.html#forecast-results",
    "href": "ASV.html#forecast-results",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast Results",
    "text": "Forecast Results\n\n\nCode\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")"
  },
  {
    "objectID": "ASV.html#prepare-the-variables",
    "href": "ASV.html#prepare-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Prepare the Variables",
    "text": "Prepare the Variables\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf4_sub <- subset(df4, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nsale <- ts(df3_sub$MSPUS, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\ngini <- ts(df4_sub$SIPOVGINIUSA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-the-variables",
    "href": "ASV.html#combine-the-variables",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Variables",
    "text": "Combine the Variables\n\n\nCode\ndd1<-data.frame(gini,saving,income,sale,df1_sub$DATE)\n\ncolnames(dd1)<-c(\"gini\",\"saving\",\"income\",'sale','date')\ndd1$gini <- ts(as.numeric(dd1$gini))\nknitr::kable(head(dd1))\n\n\n\n\n\ngini\nsaving\nincome\nsale\ndate\n\n\n\n\n38.4\n431.693\n58153\n119500\n1992-01-01\n\n\n40.4\n377.899\n57843\n120000\n1993-01-01\n\n\n40.0\n345.198\n58515\n120000\n1994-01-01\n\n\n39.9\n366.774\n60348\n126000\n1995-01-01\n\n\n40.3\n353.639\n61225\n125000\n1996-01-01\n\n\n40.5\n339.566\n62484\n127000\n1997-01-01\n\n\n\n\n\n\n\nCode\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"gini\"=log(dd1$gini),\"saving\"=log(dd1$saving),\n                                        \"income\"=log(dd1$income),\"sale\"=log(dd1$sale))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")"
  },
  {
    "objectID": "ASV.html#auto-fit-the-model-2",
    "href": "ASV.html#auto-fit-the-model-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Auto Fit the Model",
    "text": "Auto Fit the Model\n\n\nCode\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              income = lg.dd.ts1[, \"income\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"gini\"], xreg = xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"gini\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n       saving   income    sale\n      -0.0304  -0.0279  0.3559\ns.e.   0.0052   0.0516  0.0503\n\nsigma^2 = 0.0001483:  log likelihood = 88.27\nAIC=-168.54   AICc=-166.87   BIC=-163.07\n\nTraining set error measures:\n                     ME       RMSE         MAE           MPE      MAPE\nTraining set 6.9935e-06 0.01153159 0.009483874 -0.0005959199 0.2563307\n                  MASE       ACF1\nTraining set 0.7926458 -0.2180884"
  },
  {
    "objectID": "ASV.html#manual-fit-the-model-2",
    "href": "ASV.html#manual-fit-the-model-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit The Model",
    "text": "Manual Fit The Model\n\n\nCode\nfit.reg1 <- lm( gini ~ saving+ income + sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = gini ~ saving + income + sale, data = lg.dd.ts1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.026995 -0.007527  0.001984  0.005942  0.019001 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.660654   0.696434   0.949 0.351892    \nsaving      -0.024639   0.008162  -3.019 0.005773 ** \nincome      -0.039670   0.055946  -0.709 0.484839    \nsale         0.307851   0.073432   4.192 0.000302 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0122 on 25 degrees of freedom\nMultiple R-squared:  0.5148,    Adjusted R-squared:  0.4566 \nF-statistic: 8.842 on 3 and 25 DF,  p-value: 0.0003612\n\n\nWe can see that the sale and saving are pretty significant. For income, it seems that it does not provide too much impact. We can consider to remove it"
  },
  {
    "objectID": "ASV.html#manual-fit-again",
    "href": "ASV.html#manual-fit-again",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual Fit Again",
    "text": "Manual Fit Again\n\n\nCode\nfit.reg1 <- lm( gini ~ saving+ sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n\n\n\nCall:\nlm(formula = gini ~ saving + sale, data = lg.dd.ts1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.026489 -0.006684  0.002481  0.005509  0.020068 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.551544   0.672696   0.820  0.41973    \nsaving      -0.022918   0.007718  -2.969  0.00634 ** \nsale         0.279022   0.060561   4.607 9.48e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01208 on 26 degrees of freedom\nMultiple R-squared:  0.5051,    Adjusted R-squared:  0.467 \nF-statistic: 13.27 on 2 and 26 DF,  p-value: 0.000107"
  },
  {
    "objectID": "ASV.html#check-residuals-1",
    "href": "ASV.html#check-residuals-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check Residuals",
    "text": "Check Residuals\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 4.509, df = 6, p-value = 0.6081\n\nModel df: 0.   Total lags used: 6\n\n\nBased on the output, there’s no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model."
  },
  {
    "objectID": "ASV.html#check-acf-and-pacf-1",
    "href": "ASV.html#check-acf-and-pacf-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Check ACF and PACF",
    "text": "Check ACF and PACF\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit1)\n\n\n\n\n\nSince there is no major seasonal pattern. And the dataset is stationary, We can actually use it as it is"
  },
  {
    "objectID": "ASV.html#no-need-to-differencing",
    "href": "ASV.html#no-need-to-differencing",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "NO Need To Differencing",
    "text": "NO Need To Differencing\n\n\nCode\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit1) +\n  labs(title = \"ACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit1) +\n  labs(title = \"PACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n\n\n\n\nNow, it is stationary for us to continue\nFrom ACF and PACF, although there are not too many spikes and seasonal patterns, it seems that we can still consider: p = 1,2 q = 1,2. We use d = 0. There is no major seasonal pattern, therefore, no P,Q,D in this case."
  },
  {
    "objectID": "ASV.html#model-diagnotistics-2",
    "href": "ASV.html#model-diagnotistics-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnotistics",
    "text": "Model Diagnotistics\nFinding the model parameters.\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*70),nrow=70) \n\n\nfor (p in 1:5)# p=1,2,3 : 3\n{\n  for(q in 1:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d-1,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-171.1425\n-167.0406\n-170.1825\n\n\n0\n1\n0\n-145.8315\n-143.1671\n-145.3515\n\n\n0\n2\n0\n-120.2833\n-118.9874\n-120.1233\n\n\n0\n0\n1\n-172.1225\n-166.6533\n-170.4559\n\n\n0\n1\n1\n-160.6843\n-156.6877\n-159.6843\n\n\n0\n2\n1\n-136.1663\n-133.5746\n-135.6663\n\n\n0\n0\n2\n-175.6608\n-168.8243\n-173.0521\n\n\n0\n1\n2\n-160.6057\n-155.2769\n-158.8665\n\n\n0\n2\n2\n-146.7495\n-142.8620\n-145.7060\n\n\n0\n0\n3\n-174.5409\n-166.3371\n-170.7227\n\n\n0\n1\n3\n-161.7076\n-155.0466\n-158.9804\n\n\n0\n2\n3\n-145.5431\n-140.3598\n-143.7250\n\n\n0\n0\n4\n-174.8678\n-165.2967\n-169.5345\n\n\n0\n1\n4\n-158.8306\n-150.8374\n-154.8306\n\n\n0\n2\n4\n-140.6901\n-134.2109\n-137.8329\n\n\n1\n0\n0\n-171.0691\n-165.6000\n-169.4025\n\n\n1\n1\n0\n-149.5256\n-145.5290\n-148.5256\n\n\n1\n2\n0\n-125.4846\n-122.8930\n-124.9846\n\n\n1\n0\n1\n-174.9728\n-168.1363\n-172.3641\n\n\n1\n1\n1\n-160.0436\n-154.7148\n-158.3045\n\n\n1\n2\n1\n-138.8040\n-134.9164\n-137.7605\n\n\n1\n0\n2\n-175.2214\n-167.0176\n-171.4032\n\n\n1\n1\n2\n-161.2007\n-154.5397\n-158.4735\n\n\n1\n2\n2\n-145.3671\n-140.1837\n-143.5489\n\n\n1\n0\n3\n-173.4902\n-163.9191\n-168.1568\n\n\n1\n1\n3\n-160.8444\n-152.8512\n-156.8444\n\n\n1\n2\n3\n-144.4574\n-137.9782\n-141.6003\n\n\n1\n0\n4\n-173.1838\n-162.2455\n-165.9838\n\n\n1\n1\n4\n-157.5276\n-148.2022\n-151.9276\n\n\n1\n2\n4\n-144.0399\n-136.2649\n-139.8399\n\n\n2\n0\n0\n-170.7698\n-163.9333\n-168.1611\n\n\n2\n1\n0\n-156.3436\n-151.0148\n-154.6045\n\n\n2\n2\n0\n-135.5731\n-131.6856\n-134.5296\n\n\n2\n0\n1\n-170.4178\n-162.2140\n-166.5996\n\n\n2\n1\n1\n-159.2853\n-152.6243\n-156.5580\n\n\n2\n2\n1\n-144.3626\n-139.1793\n-142.5444\n\n\n2\n0\n2\n-173.5908\n-164.0197\n-168.2575\n\n\n2\n1\n2\n-159.2471\n-151.2539\n-155.2471\n\n\n2\n2\n2\n-142.8931\n-136.4140\n-140.0360\n\n\n2\n0\n3\n-171.6011\n-160.6627\n-164.4011\n\n\n2\n1\n3\n-157.3864\n-148.0609\n-151.7864\n\n\n2\n2\n3\n-144.3482\n-136.5732\n-140.1482\n\n\n2\n0\n4\n-171.8233\n-159.5177\n-162.3496\n\n\n2\n1\n4\n-155.9225\n-145.2648\n-148.3435\n\n\n3\n0\n0\n-169.1140\n-160.9102\n-165.2958\n\n\n3\n1\n0\n-155.1027\n-148.4417\n-152.3754\n\n\n3\n2\n0\n-137.2481\n-132.0647\n-135.4299\n\n\n3\n0\n1\n-168.7809\n-159.2099\n-163.4476\n\n\n3\n1\n1\n-157.8894\n-149.8962\n-153.8894\n\n\n3\n2\n1\n-142.7900\n-136.3108\n-139.9329\n\n\n3\n0\n2\n-171.6359\n-160.6975\n-164.4359\n\n\n3\n1\n2\n-156.0375\n-146.7120\n-150.4375\n\n\n3\n2\n2\n-143.0555\n-135.2804\n-138.8555\n\n\n3\n0\n3\n-169.6948\n-157.3891\n-160.2211\n\n\n3\n1\n3\n-157.8215\n-147.1639\n-150.2426\n\n\n3\n0\n4\n-177.8730\n-164.2001\n-165.6508\n\n\n4\n0\n0\n-167.5350\n-157.9639\n-162.2017\n\n\n4\n1\n0\n-153.4391\n-145.4459\n-149.4391\n\n\n4\n2\n0\n-139.8623\n-133.3832\n-137.0052\n\n\n4\n0\n1\n-173.9016\n-162.9633\n-166.7016\n\n\n4\n1\n1\n-156.1053\n-146.7798\n-150.5053\n\n\n4\n2\n1\n-140.9387\n-133.1636\n-136.7387\n\n\n4\n0\n2\n-179.3839\n-167.0783\n-169.9102\n\n\n4\n1\n2\n-155.4640\n-144.8063\n-147.8850\n\n\n4\n0\n3\n-175.6302\n-161.9573\n-163.4080\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n63 4 0 2 -179.3839 -167.0783 -169.9102\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 0 0 2 -175.6608 -168.8243 -173.0521\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 0 0 2 -175.6608 -168.8243 -173.0521\n\n\nFrom here, we can see that (4,0,2) and (0,0,2) is better. Therefore, we will compare them"
  },
  {
    "objectID": "ASV.html#compare-with-the-two-models",
    "href": "ASV.html#compare-with-the-two-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Compare with the two models",
    "text": "Compare with the two models\n\n\nCode\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 4,0,2)) \n\n\n\n\n\nCode\nmodel_output12 <- capture.output(sarima(res.fit1, 0,0,2)) \n\n\n\n\n\n\n\nCode\ncat(model_output11[150:173], model_output11[length(model_output11)], sep = \"\\n\")\n\n\nsigma^2 estimated as 8.309e-05:  log likelihood = 92.98,  aic = -169.95\n\n$degrees_of_freedom\n[1] 22\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.8855 0.3372  2.6261  0.0154\nar2    -0.2522 0.3647 -0.6915  0.4965\nar3     0.3589 0.3174  1.1306  0.2704\nar4    -0.4707 0.2172 -2.1672  0.0413\nma1    -1.4055 0.3923 -3.5824  0.0017\nma2     0.4056 0.3622  1.1198  0.2749\nxmean   0.0002 0.0004  0.6298  0.5353\n\n$AIC\n[1] -5.860371\n\n$AICc\n[1] -5.676463\n\n$BIC\n[1] -5.483186\n\n\nCode\ncat(model_output12[40:67], model_output12[length(model_output12)], sep = \"\\n\")\n\n\n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1      ma2  xmean\n      -0.2594  -0.7406  2e-04\ns.e.   0.3312   0.3122  4e-04\n\nsigma^2 estimated as 0.000101:  log likelihood = 90.7,  aic = -173.4\n\n$degrees_of_freedom\n[1] 26\n\n$ttable\n      Estimate     SE t.value p.value\nma1    -0.2594 0.3312 -0.7831  0.4406\nma2    -0.7406 0.3122 -2.3721  0.0254\nxmean   0.0002 0.0004  0.5482  0.5883\n\n$AIC\n[1] -5.979431\n\n$AICc\n[1] -5.946327\n\n$BIC\n[1] -5.790838\n\nNA\n\n\nBased on this, I think the second one (0,0,2) is slightly better with less correlation and smaller AIC value."
  },
  {
    "objectID": "ASV.html#cross-validation-2",
    "href": "ASV.html#cross-validation-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross validation",
    "text": "Cross validation\n\n\nCode\nn=length(res.fit1)\nn *0.3 \n\n\n[1] 8.7\n\n\nCode\ndat = ts(dd1[,c(1,2,4)])\n\n\n\n\nCode\n# The number of folds for cross-validation\nn_folds <- 5\nhorizon <- 4  # Forecasting horizon\nwindow_size <- length(dat[, 3]) - (n_folds * horizon)\n\n# Initialize an empty list to store forecasts\nforecasts <- list()\nforecasts2 <- list()\n\n# Initialize vectors to store error metrics for each model\nrmse1 <- numeric(n_folds)\nrmse2 <- numeric(n_folds)\n\nfor (i in 1:n_folds) {\n  # Define the training set for this fold\n  train_set <- window(dat[, 3], end = c(window_size + ((i - 1) * horizon)))\n\n  # Fit the ARIMA models on the training set\n  fit <- Arima(train_set, order = c(4, 0, 2), include.drift = TRUE, method = \"ML\")\n  fit2 <- Arima(train_set, order = c(0, 0, 2), include.drift = TRUE, method = \"ML\")\n\n  # Forecast on the horizon\n  fcast <- forecast(fit, h = horizon)\n  fcast2 <- forecast(fit2, h = horizon)\n\n  # Store forecasts\n  forecasts[[i]] <- fcast\n  forecasts2[[i]] <- fcast2\n\n  # Define the test set for this fold\n  test_set <- window(dat[, 3], start = window_size + ((i - 1) * horizon) + 1, end = window_size + (i * horizon))\n\n  # Calculate and store the RMSE for each model\n  rmse1[i] <- sqrt(mean((fcast$mean - test_set)^2, na.rm = TRUE))\n  rmse2[i] <- sqrt(mean((fcast2$mean - test_set)^2, na.rm = TRUE))\n}\n\n# Calculate the average RMSE for each model\nmean_rmse1 <- mean(rmse1)\nmean_rmse2 <- mean(rmse2)\n\n\n\n# Plot RMSE values for both models\nplot(rmse1, type = \"b\", col = \"blue\", ylim = range(c(rmse1, rmse2)), \n     xlab = \"Fold\", ylab = \"RMSE\", pch = 19, \n     main = \"Cross-Validation RMSE for ARIMA Models\")\nlines(rmse2, type = \"b\", col = \"red\", pch = 18)\npoints(rmse2, type = \"b\", col = \"red\", pch = 18)\n\n# Add a legend to the plot\nlegend(\"topright\", legend = c(\"Model 1 (ARIMA(4,0,2))\", \"Model 2 (ARIMA(0,0,2))\"), \n       col = c(\"blue\", \"red\"), pch = c(19, 18), lty = 1)\n\n\n\n\n\n\n\nCode\n# Output \nmean_rmse1\n\n\n[1] 2804.59\n\n\nCode\nmean_rmse2\n\n\n[1] 2507.994\n\n\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (0,0,2) performs better with smaller errors."
  },
  {
    "objectID": "ASV.html#fit-the-model-1",
    "href": "ASV.html#fit-the-model-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit the model",
    "text": "Fit the model\n\n\nCode\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"gini\"],order=c(0,0,2),xreg=xreg1)\nsummary(fit1)\n\n\nSeries: lg.dd.ts1[, \"gini\"] \nRegression with ARIMA(0,0,2) errors \n\nCoefficients:\n          ma1      ma2  intercept   saving    sale\n      -0.4973  -0.5027     0.5375  -0.0204  0.2789\ns.e.   0.2694   0.2576     0.1358   0.0007  0.0119\n\nsigma^2 = 0.0001067:  log likelihood = 92.76\nAIC=-173.52   AICc=-169.7   BIC=-165.31\n\nTraining set error measures:\n                        ME       RMSE         MAE         MPE     MAPE\nTraining set -9.655453e-05 0.00939564 0.007461685 -0.00333216 0.201711\n                  MASE        ACF1\nTraining set 0.6236347 -0.06097811\n\n\nThe equation: Given that y_t represents the log-transformed homevalue at time t, the ARIMA(0,0,2) can be:\n( y_t = c + 1 e{t-1} + 2 e{t-2} + _1 _t + _2 _t + e_t )\nBased on the summary, my equation is\n( y_t = 0.5375 - 0.4973 e_{t-1} - 0.5027 e_{t-2} - 0.0204 _t + 0.2789 _t + e_t )"
  },
  {
    "objectID": "ASV.html#forecast",
    "href": "ASV.html#forecast",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast",
    "text": "Forecast\n\n\nCode\nsfit<-auto.arima(lg.dd.ts1[, \"sale\"]) \nsummary(sfit) \n\n\nSeries: lg.dd.ts1[, \"sale\"] \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1   drift\n      -0.6624  0.0097\ns.e.   0.1365  0.0016\n\nsigma^2 = 0.0002195:  log likelihood = 79.3\nAIC=-152.6   AICc=-151.6   BIC=-148.6\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set 0.0003253088 0.01402717 0.01068364 0.002634758 0.0903859 0.2864612\n                   ACF1\nTraining set -0.1025856\n\n\n\n\nCode\nfs<-forecast(sfit,80) #obtaining forecasts\n\ns2fit<-auto.arima(lg.dd.ts1[, \"saving\"]) #fitting an ARIMA model to the Import variable\nsummary(s2fit)\n\n\nSeries: lg.dd.ts1[, \"saving\"] \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0701\ns.e.  0.0447\n\nsigma^2 = 0.05808:  log likelihood = 0.62\nAIC=2.76   AICc=3.24   BIC=5.42\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.0002068136 0.2325379 0.1666254 -0.1667371 2.642544 0.4511904\n                    ACF1\nTraining set -0.04495977\n\n\n\n\nCode\nfs2<-forecast(s2fit,80)\n\nfxreg <- cbind(\n              sale = fs2$mean,saving = fs$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")"
  },
  {
    "objectID": "ASV.html#time-series-transformation",
    "href": "ASV.html#time-series-transformation",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Time Series Transformation",
    "text": "Time Series Transformation"
  },
  {
    "objectID": "ASV.html#select-the-variables-with-common-dates",
    "href": "ASV.html#select-the-variables-with-common-dates",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Select the Variables with common Dates",
    "text": "Select the Variables with common Dates\nHome Value ~ Saving Rate + Gdp Deflator + Sale Price\n\n\nCode\n# Define start and end dates\nstart_date <- as.Date(\"2000-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\ndf9_sub <- subset(df9, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 4\n\n# Create ts objects\nsale <- ts(df3_sub$MSPUS, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nsaving <- ts(df6_sub$PSAVERT, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nhomevalue <- ts(df9_sub$USAUCSFRCONDOSMSAMID, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(2000, 1), end = c(2020, 1), frequency = frequency)"
  },
  {
    "objectID": "ASV.html#combine-the-variables-1",
    "href": "ASV.html#combine-the-variables-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Combine the Variables",
    "text": "Combine the Variables\n\n\nCode\ndd3<-data.frame(homevalue,saving,gdp,sale,DATE)\n\ncolnames(dd3)<-c(\"homevalue\",\"saving\",\"gdp_deflator\", \"sale\",'date')\n\nknitr::kable(head(dd3))\n\n\n\n\n\nhomevalue\nsaving\ngdp_deflator\nsale\ndate\n\n\n\n\n121428.3\n5.0\n2.7\n165300\n10957\n\n\n121642.0\n4.5\n2.5\n163200\n11048\n\n\n121906.9\n4.3\n2.4\n168800\n11139\n\n\n122475.1\n4.8\n2.2\n172900\n11231\n\n\n123129.1\n4.8\n2.6\n169800\n11323\n\n\n123830.3\n4.8\n2.4\n179000\n11413"
  },
  {
    "objectID": "ASV.html#plot-the-figure-together",
    "href": "ASV.html#plot-the-figure-together",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Plot the Figure Together",
    "text": "Plot the Figure Together\n\n\nCode\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale\"=log(dd3$sale),\"saving\"=log(dd3$saving),\"homevalue\"=log(dd3$homevalue),\n                                        \"gdp_deflator\"=log(dd3$gdp))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate, Home value in USA\")"
  },
  {
    "objectID": "ASV.html#fitting-a-var-model",
    "href": "ASV.html#fitting-a-var-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting a VAR model",
    "text": "Fitting a VAR model\nFinding out the best p:\n\n\nCode\nVARselect(dd3[, c(2:5)], lag.max=14, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    14     14      1     14 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.623230e+01 1.585994e+01 1.597680e+01 1.591038e+01 1.530104e+01\nHQ(n)  1.654480e+01 1.638078e+01 1.670598e+01 1.684788e+01 1.644688e+01\nSC(n)  1.702204e+01 1.717618e+01 1.781953e+01 1.827960e+01 1.819676e+01\nFPE(n) 1.123135e+07 7.794514e+06 8.902039e+06 8.575733e+06 4.886908e+06\n                  6            7            8            9           10\nAIC(n) 1.540760e+01 1.514539e+01 1.518739e+01 1.460313e+01 1.459530e+01\nHQ(n)  1.676178e+01 1.670790e+01 1.695824e+01 1.658231e+01 1.678281e+01\nSC(n)  1.882981e+01 1.909409e+01 1.966259e+01 1.960482e+01 2.012348e+01\nFPE(n) 5.834120e+06 4.967706e+06 5.969293e+06 4.042891e+06 5.231765e+06\n                 11           12           13           14\nAIC(n) 1.364742e+01 1.256891e+01 1.182785e+01      9.56155\nHQ(n)  1.604327e+01 1.517310e+01 1.464037e+01     12.58241\nSC(n)  1.970209e+01 1.915008e+01 1.893551e+01     17.19571\nFPE(n) 2.917799e+06 1.647224e+06 1.628958e+06 519454.19470\n\n\nFrom the results, we can see that 13,14 have relatively smaller criterias:\nWe can fit several models with p=13, 14.=> VAR(13), VAR(14)\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:5)], p=13, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving, gdp_deflator, sale, date \nDeterministic variables: both \nSample size: 68 \nLog Likelihood: -567.72 \nRoots of the characteristic polynomial:\n1.035 1.021 1.021 1.008 1.008 1.004 1.004     1     1 0.9997 0.9997 0.9985 0.9985 0.9915 0.9915 0.9857 0.9834 0.9834 0.9832 0.9832 0.9815 0.9815 0.9749 0.9749 0.9734 0.9734 0.9729 0.9729 0.9669 0.9669 0.9637 0.9637 0.9621 0.9621 0.9476 0.9476 0.9337 0.9337 0.9328 0.9328 0.9302 0.9302 0.9299 0.9299 0.909 0.909 0.8471 0.7737 0.695 0.695 0.4355 0.4355\nCall:\nvars::VAR(y = dd3[, c(2:5)], p = 13, type = \"both\")\n\n\nEstimation results for equation saving: \n======================================= \nsaving = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1         2.908e-01  2.872e-01   1.013   0.3285  \ngdp_deflator.l1   1.711e-01  3.511e-01   0.487   0.6336  \nsale.l1           2.098e-05  3.099e-05   0.677   0.5094  \ndate.l1           1.046e+00  6.562e-01   1.594   0.1333  \nsaving.l2        -1.198e-01  2.610e-01  -0.459   0.6533  \ngdp_deflator.l2   1.933e-02  3.535e-01   0.055   0.9572  \nsale.l2          -2.719e-05  2.607e-05  -1.043   0.3146  \ndate.l2          -3.554e-01  6.401e-01  -0.555   0.5875  \nsaving.l3        -1.422e-01  2.480e-01  -0.574   0.5753  \ngdp_deflator.l3   5.727e-02  3.110e-01   0.184   0.8565  \nsale.l3          -4.226e-06  2.668e-05  -0.158   0.8764  \ndate.l3           5.122e-01  6.753e-01   0.758   0.4608  \nsaving.l4        -2.747e-02  2.182e-01  -0.126   0.9016  \ngdp_deflator.l4   6.959e-01  2.947e-01   2.362   0.0332 *\nsale.l4          -3.909e-05  2.674e-05  -1.462   0.1658  \ndate.l4          -8.648e-01  6.756e-01  -1.280   0.2213  \nsaving.l5         1.842e-01  2.431e-01   0.758   0.4612  \ngdp_deflator.l5   5.206e-02  3.233e-01   0.161   0.8744  \nsale.l5           2.488e-05  2.213e-05   1.124   0.2798  \ndate.l5           3.609e-02  6.480e-01   0.056   0.9564  \nsaving.l6         4.881e-02  2.016e-01   0.242   0.8122  \ngdp_deflator.l6   8.258e-02  2.831e-01   0.292   0.7748  \nsale.l6          -2.184e-05  2.360e-05  -0.925   0.3705  \ndate.l6          -2.468e-01  6.530e-01  -0.378   0.7111  \nsaving.l7        -5.310e-01  2.362e-01  -2.248   0.0412 *\ngdp_deflator.l7   5.376e-02  2.009e-01   0.268   0.7929  \nsale.l7           1.010e-05  2.576e-05   0.392   0.7008  \ndate.l7           1.775e+00  7.591e-01   2.339   0.0347 *\nsaving.l8         3.975e-01  3.213e-01   1.237   0.2363  \ngdp_deflator.l8   3.341e-01  1.765e-01   1.893   0.0792 .\nsale.l8          -6.328e-05  4.354e-05  -1.453   0.1682  \ndate.l8          -1.411e-01  7.180e-01  -0.196   0.8471  \nsaving.l9        -1.211e-01  3.235e-01  -0.374   0.7138  \ngdp_deflator.l9   5.550e-02  1.902e-01   0.292   0.7747  \nsale.l9           3.448e-05  3.337e-05   1.033   0.3190  \ndate.l9           4.678e-01  8.289e-01   0.564   0.5815  \nsaving.l10       -2.254e-01  2.921e-01  -0.772   0.4530  \ngdp_deflator.l10 -1.552e-01  1.714e-01  -0.905   0.3805  \nsale.l10         -2.976e-05  3.162e-05  -0.941   0.3624  \ndate.l10          1.580e-01  9.071e-01   0.174   0.8642  \nsaving.l11       -2.268e-02  2.522e-01  -0.090   0.9296  \ngdp_deflator.l11  1.209e-01  1.778e-01   0.680   0.5075  \nsale.l11          9.539e-05  3.521e-05   2.709   0.0169 *\ndate.l11         -8.571e-01  7.812e-01  -1.097   0.2911  \nsaving.l12        3.879e-01  2.473e-01   1.569   0.1390  \ngdp_deflator.l12  1.344e-01  1.553e-01   0.865   0.4014  \nsale.l12         -2.117e-05  4.142e-05  -0.511   0.6172  \ndate.l12          7.070e-01  6.852e-01   1.032   0.3196  \nsaving.l13        4.278e-02  2.301e-01   0.186   0.8552  \ngdp_deflator.l13  2.689e-01  2.413e-01   1.114   0.2838  \nsale.l13         -1.205e-05  3.414e-05  -0.353   0.7293  \ndate.l13         -2.870e-01  6.661e-01  -0.431   0.6731  \nconst            -2.031e+04  3.379e+04  -0.601   0.5573  \ntrend            -1.780e+02  3.017e+02  -0.590   0.5646  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6358 on 14 degrees of freedom\nMultiple R-Squared: 0.9397, Adjusted R-squared: 0.7116 \nF-statistic: 4.119 on 53 and 14 DF,  p-value: 0.002798 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)   \nsaving.l1        -4.611e-01  2.481e-01  -1.859  0.08420 . \ngdp_deflator.l1   4.021e-01  3.032e-01   1.326  0.20603   \nsale.l1           2.180e-05  2.676e-05   0.815  0.42889   \ndate.l1           4.124e-01  5.667e-01   0.728  0.47874   \nsaving.l2         3.869e-01  2.254e-01   1.716  0.10819   \ngdp_deflator.l2   6.262e-02  3.053e-01   0.205  0.84044   \nsale.l2          -1.508e-06  2.251e-05  -0.067  0.94753   \ndate.l2          -7.359e-01  5.528e-01  -1.331  0.20441   \nsaving.l3        -3.211e-02  2.141e-01  -0.150  0.88296   \ngdp_deflator.l3   2.799e-02  2.686e-01   0.104  0.91848   \nsale.l3           4.193e-07  2.305e-05   0.018  0.98574   \ndate.l3          -2.387e-01  5.832e-01  -0.409  0.68851   \nsaving.l4        -4.318e-01  1.885e-01  -2.291  0.03798 * \ngdp_deflator.l4   3.419e-01  2.545e-01   1.344  0.20043   \nsale.l4           1.016e-05  2.309e-05   0.440  0.66679   \ndate.l4          -1.510e-02  5.835e-01  -0.026  0.97972   \nsaving.l5         4.398e-03  2.099e-01   0.021  0.98358   \ngdp_deflator.l5   4.860e-02  2.792e-01   0.174  0.86430   \nsale.l5          -4.313e-05  1.911e-05  -2.257  0.04050 * \ndate.l5          -6.851e-02  5.597e-01  -0.122  0.90431   \nsaving.l6         4.401e-01  1.741e-01   2.527  0.02415 * \ngdp_deflator.l6   6.141e-02  2.445e-01   0.251  0.80532   \nsale.l6           2.741e-05  2.039e-05   1.344  0.20018   \ndate.l6           9.817e-01  5.640e-01   1.741  0.10366   \nsaving.l7        -6.433e-01  2.040e-01  -3.154  0.00704 **\ngdp_deflator.l7  -9.428e-02  1.735e-01  -0.543  0.59539   \nsale.l7          -5.432e-05  2.225e-05  -2.441  0.02852 * \ndate.l7          -5.684e-01  6.556e-01  -0.867  0.40052   \nsaving.l8         5.461e-03  2.775e-01   0.020  0.98458   \ngdp_deflator.l8  -8.684e-02  1.525e-01  -0.570  0.57800   \nsale.l8           4.609e-05  3.760e-05   1.226  0.24054   \ndate.l8           1.083e+00  6.201e-01   1.747  0.10248   \nsaving.l9         3.778e-01  2.794e-01   1.352  0.19781   \ngdp_deflator.l9   1.485e-01  1.642e-01   0.904  0.38123   \nsale.l9          -3.166e-05  2.882e-05  -1.098  0.29056   \ndate.l9          -1.140e+00  7.159e-01  -1.592  0.13369   \nsaving.l10       -7.754e-02  2.522e-01  -0.307  0.76306   \ngdp_deflator.l10  1.545e-01  1.480e-01   1.043  0.31449   \nsale.l10         -2.887e-05  2.730e-05  -1.057  0.30832   \ndate.l10          3.145e-03  7.834e-01   0.004  0.99685   \nsaving.l11       -6.629e-02  2.178e-01  -0.304  0.76536   \ngdp_deflator.l11 -9.043e-02  1.536e-01  -0.589  0.56541   \nsale.l11          3.491e-05  3.041e-05   1.148  0.27019   \ndate.l11          8.235e-01  6.747e-01   1.221  0.24243   \nsaving.l12       -1.859e-01  2.136e-01  -0.871  0.39861   \ngdp_deflator.l12  2.547e-01  1.342e-01   1.898  0.07846 . \nsale.l12          7.403e-06  3.578e-05   0.207  0.83905   \ndate.l12         -9.841e-01  5.918e-01  -1.663  0.11856   \nsaving.l13        2.576e-01  1.987e-01   1.296  0.21588   \ngdp_deflator.l13 -2.652e-01  2.084e-01  -1.272  0.22397   \nsale.l13          1.495e-05  2.948e-05   0.507  0.61990   \ndate.l13          6.931e-01  5.752e-01   1.205  0.24820   \nconst            -2.276e+03  2.918e+04  -0.078  0.93894   \ntrend            -2.257e+01  2.606e+02  -0.087  0.93221   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5491 on 14 degrees of freedom\nMultiple R-Squared: 0.9365, Adjusted R-squared: 0.696 \nF-statistic: 3.895 on 53 and 14 DF,  p-value: 0.003752 \n\n\nEstimation results for equation sale: \n===================================== \nsale = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -4.084e+03  2.979e+03  -1.371   0.1920  \ngdp_deflator.l1  -9.744e+02  3.642e+03  -0.268   0.7929  \nsale.l1           7.623e-01  3.214e-01   2.372   0.0326 *\ndate.l1           2.490e+03  6.806e+03   0.366   0.7199  \nsaving.l2        -4.804e+02  2.707e+03  -0.177   0.8617  \ngdp_deflator.l2   4.768e+03  3.666e+03   1.300   0.2145  \nsale.l2           5.903e-02  2.703e-01   0.218   0.8303  \ndate.l2          -6.124e+02  6.639e+03  -0.092   0.9278  \nsaving.l3         1.587e+03  2.572e+03   0.617   0.5471  \ngdp_deflator.l3  -5.637e+03  3.226e+03  -1.747   0.1024  \nsale.l3           2.499e-01  2.768e-01   0.903   0.3818  \ndate.l3          -7.087e+02  7.005e+03  -0.101   0.9208  \nsaving.l4        -2.482e+03  2.264e+03  -1.097   0.2913  \ngdp_deflator.l4   3.322e+03  3.056e+03   1.087   0.2955  \nsale.l4          -3.578e-01  2.773e-01  -1.290   0.2179  \ndate.l4           1.388e+04  7.007e+03   1.981   0.0676 .\nsaving.l5        -5.417e+02  2.521e+03  -0.215   0.8330  \ngdp_deflator.l5   1.615e+03  3.353e+03   0.482   0.6376  \nsale.l5          -1.824e-01  2.295e-01  -0.795   0.4400  \ndate.l5          -9.008e+03  6.721e+03  -1.340   0.2015  \nsaving.l6         1.384e+03  2.091e+03   0.662   0.5188  \ngdp_deflator.l6   7.061e+02  2.936e+03   0.240   0.8134  \nsale.l6          -7.230e-02  2.448e-01  -0.295   0.7721  \ndate.l6           9.888e+02  6.773e+03   0.146   0.8860  \nsaving.l7        -2.816e+03  2.450e+03  -1.149   0.2696  \ngdp_deflator.l7  -8.100e+02  2.084e+03  -0.389   0.7033  \nsale.l7           3.045e-01  2.672e-01   1.139   0.2737  \ndate.l7           3.220e+03  7.873e+03   0.409   0.6887  \nsaving.l8        -2.854e+03  3.332e+03  -0.856   0.4062  \ngdp_deflator.l8  -9.438e+02  1.831e+03  -0.515   0.6143  \nsale.l8          -9.160e-02  4.516e-01  -0.203   0.8422  \ndate.l8          -4.669e+03  7.447e+03  -0.627   0.5408  \nsaving.l9         4.690e+03  3.356e+03   1.398   0.1840  \ngdp_deflator.l9   5.201e+03  1.973e+03   2.637   0.0195 *\nsale.l9           1.949e-01  3.461e-01   0.563   0.5823  \ndate.l9           1.035e+04  8.598e+03   1.204   0.2486  \nsaving.l10       -1.195e+03  3.029e+03  -0.394   0.6993  \ngdp_deflator.l10 -6.464e+02  1.778e+03  -0.364   0.7216  \nsale.l10         -6.476e-01  3.279e-01  -1.975   0.0683 .\ndate.l10         -4.637e+03  9.408e+03  -0.493   0.6298  \nsaving.l11       -4.289e+03  2.616e+03  -1.639   0.1234  \ngdp_deflator.l11 -1.225e+03  1.845e+03  -0.664   0.5173  \nsale.l11          5.896e-01  3.652e-01   1.615   0.1287  \ndate.l11          7.608e+03  8.103e+03   0.939   0.3637  \nsaving.l12        1.989e+03  2.565e+03   0.776   0.4509  \ngdp_deflator.l12 -9.021e+02  1.611e+03  -0.560   0.5844  \nsale.l12         -1.698e-02  4.296e-01  -0.040   0.9690  \ndate.l12         -1.176e+04  7.107e+03  -1.654   0.1203  \nsaving.l13        1.331e+03  2.387e+03   0.558   0.5859  \ngdp_deflator.l13  2.295e+03  2.503e+03   0.917   0.3747  \nsale.l13         -2.559e-01  3.541e-01  -0.723   0.4817  \ndate.l13          9.671e+03  6.909e+03   1.400   0.1833  \nconst            -1.721e+08  3.504e+08  -0.491   0.6310  \ntrend            -1.535e+06  3.130e+06  -0.490   0.6314  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6595 on 14 degrees of freedom\nMultiple R-Squared: 0.9948, Adjusted R-squared: 0.9751 \nF-statistic: 50.47 on 53 and 14 DF,  p-value: 3.505e-10 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)    \nsaving.l1         9.351e-02  8.226e-02   1.137  0.27471    \ngdp_deflator.l1   1.711e-02  1.005e-01   0.170  0.86728    \nsale.l1          -1.142e-05  8.874e-06  -1.287  0.21899    \ndate.l1           1.523e-02  1.879e-01   0.081  0.93656    \nsaving.l2        -9.768e-02  7.475e-02  -1.307  0.21237    \ngdp_deflator.l2  -1.439e-01  1.012e-01  -1.422  0.17696    \nsale.l2           1.466e-05  7.464e-06   1.964  0.06976 .  \ndate.l2          -4.554e-01  1.833e-01  -2.485  0.02624 *  \nsaving.l3        -5.419e-02  7.100e-02  -0.763  0.45800    \ngdp_deflator.l3   1.575e-01  8.905e-02   1.769  0.09868 .  \nsale.l3           4.743e-06  7.641e-06   0.621  0.54474    \ndate.l3          -2.674e-01  1.934e-01  -1.383  0.18847    \nsaving.l4         1.874e-01  6.249e-02   2.998  0.00959 ** \ngdp_deflator.l4  -1.150e-01  8.438e-02  -1.363  0.19454    \nsale.l4          -5.368e-06  7.656e-06  -0.701  0.49469    \ndate.l4          -2.522e-01  1.935e-01  -1.304  0.21338    \nsaving.l5        -9.269e-02  6.961e-02  -1.332  0.20428    \ngdp_deflator.l5   3.426e-02  9.257e-02   0.370  0.71686    \nsale.l5          -1.229e-06  6.336e-06  -0.194  0.84902    \ndate.l5          -1.950e-01  1.856e-01  -1.051  0.31127    \nsaving.l6        -4.992e-03  5.774e-02  -0.086  0.93233    \ngdp_deflator.l6   3.084e-02  8.107e-02   0.380  0.70932    \nsale.l6          -3.542e-06  6.760e-06  -0.524  0.60848    \ndate.l6          -6.598e-01  1.870e-01  -3.528  0.00334 ** \nsaving.l7        -1.778e-02  6.763e-02  -0.263  0.79650    \ngdp_deflator.l7  -1.476e-02  5.753e-02  -0.257  0.80119    \nsale.l7           5.976e-07  7.378e-06   0.081  0.93659    \ndate.l7          -2.933e-01  2.174e-01  -1.349  0.19861    \nsaving.l8         5.116e-02  9.201e-02   0.556  0.58692    \ngdp_deflator.l8  -7.410e-02  5.055e-02  -1.466  0.16480    \nsale.l8           1.489e-05  1.247e-05   1.194  0.25226    \ndate.l8           4.650e-03  2.056e-01   0.023  0.98228    \nsaving.l9        -1.112e-01  9.265e-02  -1.200  0.25004    \ngdp_deflator.l9  -6.423e-02  5.446e-02  -1.179  0.25794    \nsale.l9          -6.880e-06  9.556e-06  -0.720  0.48339    \ndate.l9          -6.280e-01  2.374e-01  -2.646  0.01920 *  \nsaving.l10        1.487e-01  8.364e-02   1.777  0.09724 .  \ngdp_deflator.l10  1.083e-01  4.909e-02   2.206  0.04455 *  \nsale.l10         -1.814e-06  9.053e-06  -0.200  0.84405    \ndate.l10          2.086e-02  2.598e-01   0.080  0.93714    \nsaving.l11        1.941e-02  7.223e-02   0.269  0.79208    \ngdp_deflator.l11  4.475e-02  5.093e-02   0.879  0.39444    \nsale.l11         -1.522e-05  1.008e-05  -1.509  0.15343    \ndate.l11         -8.110e-01  2.237e-01  -3.625  0.00276 ** \nsaving.l12       -1.298e-01  7.081e-02  -1.833  0.08822 .  \ngdp_deflator.l12 -9.417e-02  4.448e-02  -2.117  0.05267 .  \nsale.l12          1.914e-05  1.186e-05   1.614  0.12886    \ndate.l12         -3.757e-02  1.962e-01  -0.191  0.85092    \nsaving.l13        6.755e-02  6.589e-02   1.025  0.32267    \ngdp_deflator.l13 -3.158e-02  6.910e-02  -0.457  0.65469    \nsale.l13         -5.844e-06  9.775e-06  -0.598  0.55952    \ndate.l13         -6.054e-01  1.907e-01  -3.174  0.00676 ** \nconst             5.316e+04  9.675e+03   5.494 7.90e-05 ***\ntrend             4.716e+02  8.641e+01   5.457 8.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1821 on 14 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.243e+08 on 53 and 14 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                 saving gdp_deflator       sale       date\nsaving          0.40430     -0.02814     -128.6    0.03227\ngdp_deflator   -0.02814      0.30155     1527.7    0.02139\nsale         -128.63360   1527.68477 43494825.8 -140.74083\ndate            0.03227      0.02139     -140.7    0.03315\n\nCorrelation matrix of residuals:\n               saving gdp_deflator     sale    date\nsaving        1.00000     -0.08061 -0.03067  0.2787\ngdp_deflator -0.08061      1.00000  0.42182  0.2139\nsale         -0.03067      0.42182  1.00000 -0.1172\ndate          0.27873      0.21394 -0.11720  1.0000\n\n\n\n\nCode\nsummary(vars::VAR(dd3[, c(2:5)], p=14, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: saving, gdp_deflator, sale, date \nDeterministic variables: both \nSample size: 67 \nLog Likelihood: -468.587 \nRoots of the characteristic polynomial:\n1.016 1.016 1.009 1.007 1.007 1.005 1.005 1.001 1.001     1     1 0.9992 0.9992 0.9966 0.9966 0.9939 0.9939 0.9908 0.9908 0.9863 0.9863 0.9859 0.9859 0.9855 0.9855 0.9851 0.9851 0.9789 0.9762 0.9762 0.9755 0.9755 0.9669 0.9669 0.9593 0.9593 0.9561 0.9561 0.9555 0.9555 0.9545 0.9545 0.9493 0.9489 0.9489 0.9425 0.9425 0.9253 0.9253 0.9249 0.9249 0.8538 0.8383 0.8383 0.7951 0.5564\nCall:\nvars::VAR(y = dd3[, c(2:5)], p = 14, type = \"both\")\n\n\nEstimation results for equation saving: \n======================================= \nsaving = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1         1.463e-01  2.654e-01   0.551   0.5949  \ngdp_deflator.l1   4.820e-01  3.451e-01   1.397   0.1960  \nsale.l1           1.220e-05  3.068e-05   0.398   0.7001  \ndate.l1           7.393e-01  9.500e-01   0.778   0.4564  \nsaving.l2        -9.046e-02  3.074e-01  -0.294   0.7752  \ngdp_deflator.l2  -5.264e-01  3.864e-01  -1.362   0.2062  \nsale.l2           8.937e-06  3.454e-05   0.259   0.8017  \ndate.l2          -2.666e-01  6.697e-01  -0.398   0.6999  \nsaving.l3        -5.992e-01  3.002e-01  -1.996   0.0771 .\ngdp_deflator.l3   2.768e-01  3.652e-01   0.758   0.4678  \nsale.l3          -1.071e-05  2.917e-05  -0.367   0.7218  \ndate.l3           6.780e-01  7.349e-01   0.923   0.3803  \nsaving.l4         1.815e-01  2.328e-01   0.780   0.4557  \ngdp_deflator.l4   5.535e-01  3.862e-01   1.433   0.1855  \nsale.l4          -3.200e-05  2.680e-05  -1.194   0.2631  \ndate.l4          -1.277e+00  6.798e-01  -1.878   0.0931 .\nsaving.l5         1.830e-01  3.068e-01   0.597   0.5655  \ngdp_deflator.l5   3.652e-01  3.758e-01   0.972   0.3566  \nsale.l5          -1.319e-05  2.858e-05  -0.461   0.6555  \ndate.l5          -3.563e-01  7.888e-01  -0.452   0.6622  \nsaving.l6        -1.334e-01  2.455e-01  -0.543   0.6000  \ngdp_deflator.l6   3.820e-01  3.002e-01   1.273   0.2351  \nsale.l6           2.371e-07  2.365e-05   0.010   0.9922  \ndate.l6          -8.171e-01  6.998e-01  -1.168   0.2729  \nsaving.l7        -6.584e-01  2.305e-01  -2.856   0.0189 *\ngdp_deflator.l7   2.827e-01  2.916e-01   0.969   0.3578  \nsale.l7          -1.994e-05  2.543e-05  -0.784   0.4531  \ndate.l7           1.395e+00  9.481e-01   1.472   0.1752  \nsaving.l8         7.202e-01  3.210e-01   2.244   0.0515 .\ngdp_deflator.l8   3.057e-01  1.972e-01   1.550   0.1556  \nsale.l8          -3.316e-05  4.185e-05  -0.792   0.4486  \ndate.l8           5.727e-01  8.882e-01   0.645   0.5352  \nsaving.l9        -2.317e-01  3.587e-01  -0.646   0.5344  \ngdp_deflator.l9   2.613e-01  2.002e-01   1.305   0.2242  \nsale.l9          -5.261e-05  4.653e-05  -1.131   0.2874  \ndate.l9          -1.794e-01  8.084e-01  -0.222   0.8293  \nsaving.l10       -2.787e-01  3.470e-01  -0.803   0.4425  \ngdp_deflator.l10 -1.458e-01  2.135e-01  -0.683   0.5117  \nsale.l10          6.185e-06  3.344e-05   0.185   0.8574  \ndate.l10          1.604e+00  1.151e+00   1.394   0.1968  \nsaving.l11       -2.069e-02  3.097e-01  -0.067   0.9482  \ngdp_deflator.l11  9.249e-03  1.897e-01   0.049   0.9622  \nsale.l11          7.680e-05  3.803e-05   2.020   0.0742 .\ndate.l11         -1.994e+00  8.221e-01  -2.425   0.0383 *\nsaving.l12        3.251e-01  2.492e-01   1.305   0.2244  \ngdp_deflator.l12  2.376e-01  1.676e-01   1.418   0.1900  \nsale.l12          2.661e-06  4.714e-05   0.056   0.9562  \ndate.l12          4.892e-01  1.058e+00   0.463   0.6547  \nsaving.l13        2.713e-01  2.801e-01   0.969   0.3581  \ngdp_deflator.l13  1.783e-01  2.297e-01   0.776   0.4575  \nsale.l13         -1.524e-05  4.259e-05  -0.358   0.7287  \ndate.l13          9.146e-02  7.736e-01   0.118   0.9085  \nsaving.l14       -2.111e-01  2.352e-01  -0.897   0.3929  \ngdp_deflator.l14  5.930e-01  2.528e-01   2.346   0.0436 *\nsale.l14          2.412e-05  3.334e-05   0.723   0.4878  \ndate.l14         -6.787e-01  8.703e-01  -0.780   0.4555  \nconst            -4.824e+02  5.852e+04  -0.008   0.9936  \ntrend            -8.263e-02  5.245e+02   0.000   0.9999  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5664 on 9 degrees of freedom\nMultiple R-Squared: 0.9692, Adjusted R-squared: 0.7742 \nF-statistic: 4.971 on 57 and 9 DF,  p-value: 0.00691 \n\n\nEstimation results for equation gdp_deflator: \n============================================= \ngdp_deflator = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -5.310e-01  2.657e-01  -1.998   0.0768 .\ngdp_deflator.l1   5.984e-01  3.456e-01   1.731   0.1174  \nsale.l1           2.347e-06  3.072e-05   0.076   0.9408  \ndate.l1           7.860e-03  9.513e-01   0.008   0.9936  \nsaving.l2         2.042e-01  3.078e-01   0.663   0.5238  \ngdp_deflator.l2  -2.973e-01  3.870e-01  -0.768   0.4621  \nsale.l2           2.167e-05  3.459e-05   0.627   0.5465  \ndate.l2          -2.858e-01  6.707e-01  -0.426   0.6800  \nsaving.l3        -2.568e-01  3.007e-01  -0.854   0.4152  \ngdp_deflator.l3   2.612e-01  3.657e-01   0.714   0.4932  \nsale.l3           1.394e-05  2.921e-05   0.477   0.6444  \ndate.l3          -2.505e-02  7.359e-01  -0.034   0.9736  \nsaving.l4        -2.762e-01  2.331e-01  -1.185   0.2665  \ngdp_deflator.l4   2.561e-01  3.867e-01   0.662   0.5244  \nsale.l4           4.054e-08  2.684e-05   0.002   0.9988  \ndate.l4          -2.489e-01  6.808e-01  -0.366   0.7231  \nsaving.l5         3.847e-02  3.072e-01   0.125   0.9031  \ngdp_deflator.l5   1.682e-01  3.764e-01   0.447   0.6655  \nsale.l5          -7.211e-05  2.862e-05  -2.520   0.0328 *\ndate.l5           1.852e-01  7.899e-01   0.234   0.8199  \nsaving.l6         1.954e-01  2.458e-01   0.795   0.4471  \ngdp_deflator.l6   3.326e-01  3.006e-01   1.107   0.2972  \nsale.l6           3.402e-05  2.369e-05   1.436   0.1847  \ndate.l6           2.362e-01  7.008e-01   0.337   0.7438  \nsaving.l7        -5.999e-01  2.309e-01  -2.598   0.0288 *\ngdp_deflator.l7   1.030e-01  2.920e-01   0.353   0.7325  \nsale.l7          -6.390e-05  2.547e-05  -2.509   0.0334 *\ndate.l7          -8.538e-01  9.494e-01  -0.899   0.3919  \nsaving.l8         1.751e-01  3.214e-01   0.545   0.5991  \ngdp_deflator.l8  -1.805e-01  1.975e-01  -0.914   0.3846  \nsale.l8           7.564e-05  4.191e-05   1.805   0.1046  \ndate.l8           1.581e+00  8.895e-01   1.777   0.1093  \nsaving.l9         6.421e-02  3.592e-01   0.179   0.8621  \ngdp_deflator.l9   2.489e-01  2.004e-01   1.242   0.2457  \nsale.l9          -7.093e-05  4.660e-05  -1.522   0.1623  \ndate.l9          -1.109e+00  8.095e-01  -1.370   0.2038  \nsaving.l10        2.516e-02  3.475e-01   0.072   0.9439  \ngdp_deflator.l10  3.055e-01  2.138e-01   1.429   0.1868  \nsale.l10         -2.801e-05  3.349e-05  -0.836   0.4246  \ndate.l10          6.123e-01  1.152e+00   0.531   0.6081  \nsaving.l11       -2.733e-02  3.101e-01  -0.088   0.9317  \ngdp_deflator.l11 -8.317e-02  1.900e-01  -0.438   0.6719  \nsale.l11          1.825e-05  3.808e-05   0.479   0.6432  \ndate.l11          5.851e-01  8.233e-01   0.711   0.4953  \nsaving.l12       -3.538e-01  2.495e-01  -1.418   0.1899  \ngdp_deflator.l12  1.778e-01  1.678e-01   1.059   0.3171  \nsale.l12          1.953e-05  4.720e-05   0.414   0.6888  \ndate.l12         -1.210e+00  1.059e+00  -1.142   0.2828  \nsaving.l13        2.470e-01  2.805e-01   0.880   0.4015  \ngdp_deflator.l13 -3.622e-01  2.300e-01  -1.575   0.1497  \nsale.l13          4.599e-05  4.265e-05   1.078   0.3090  \ndate.l13          6.696e-01  7.747e-01   0.864   0.4099  \nsaving.l14        2.484e-01  2.356e-01   1.054   0.3192  \ngdp_deflator.l14  4.051e-01  2.531e-01   1.600   0.1440  \nsale.l14         -1.083e-05  3.339e-05  -0.324   0.7531  \ndate.l14         -9.017e-02  8.716e-01  -0.103   0.9199  \nconst            -3.195e+02  5.860e+04  -0.005   0.9958  \ntrend            -4.929e+00  5.253e+02  -0.009   0.9927  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5672 on 9 degrees of freedom\nMultiple R-Squared: 0.9563, Adjusted R-squared: 0.6792 \nF-statistic: 3.452 on 57 and 9 DF,  p-value: 0.02509 \n\n\nEstimation results for equation sale: \n===================================== \nsale = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)  \nsaving.l1        -5.784e+03  2.978e+03  -1.942   0.0840 .\ngdp_deflator.l1   1.594e+02  3.873e+03   0.041   0.9681  \nsale.l1           6.880e-01  3.443e-01   1.998   0.0767 .\ndate.l1           1.287e+04  1.066e+04   1.208   0.2580  \nsaving.l2        -2.261e+03  3.450e+03  -0.656   0.5285  \ngdp_deflator.l2   1.876e+03  4.337e+03   0.433   0.6755  \nsale.l2           6.351e-01  3.876e-01   1.638   0.1357  \ndate.l2           3.733e+03  7.516e+03   0.497   0.6313  \nsaving.l3         1.248e+03  3.369e+03   0.370   0.7196  \ngdp_deflator.l3  -1.625e+03  4.098e+03  -0.396   0.7010  \nsale.l3          -4.350e-02  3.273e-01  -0.133   0.8972  \ndate.l3           7.415e+03  8.247e+03   0.899   0.3920  \nsaving.l4        -1.349e+02  2.612e+03  -0.052   0.9599  \ngdp_deflator.l4   9.301e+02  4.334e+03   0.215   0.8348  \nsale.l4          -4.541e-01  3.008e-01  -1.510   0.1654  \ndate.l4           1.650e+04  7.629e+03   2.163   0.0588 .\nsaving.l5        -2.476e+03  3.443e+03  -0.719   0.4902  \ngdp_deflator.l5   6.603e+03  4.218e+03   1.565   0.1519  \nsale.l5          -5.115e-01  3.207e-01  -1.595   0.1452  \ndate.l5          -6.944e+03  8.852e+03  -0.784   0.4530  \nsaving.l6         1.271e+03  2.755e+03   0.461   0.6555  \ngdp_deflator.l6   2.518e+03  3.369e+03   0.747   0.4739  \nsale.l6           1.281e-01  2.654e-01   0.483   0.6409  \ndate.l6          -4.265e+02  7.853e+03  -0.054   0.9579  \nsaving.l7        -3.353e+03  2.587e+03  -1.296   0.2272  \ngdp_deflator.l7  -1.840e+03  3.273e+03  -0.562   0.5877  \nsale.l7           1.813e-01  2.854e-01   0.635   0.5411  \ndate.l7           1.409e+04  1.064e+04   1.324   0.2180  \nsaving.l8        -1.446e+03  3.602e+03  -0.401   0.6975  \ngdp_deflator.l8  -1.376e+03  2.214e+03  -0.621   0.5497  \nsale.l8           8.313e-02  4.697e-01   0.177   0.8634  \ndate.l8           9.431e+03  9.968e+03   0.946   0.3688  \nsaving.l9         4.737e+03  4.025e+03   1.177   0.2695  \ngdp_deflator.l9   6.886e+03  2.246e+03   3.066   0.0134 *\nsale.l9          -5.729e-01  5.222e-01  -1.097   0.3011  \ndate.l9           8.875e+03  9.072e+03   0.978   0.3535  \nsaving.l10        1.197e+03  3.894e+03   0.308   0.7654  \ngdp_deflator.l10  1.482e+03  2.396e+03   0.618   0.5516  \nsale.l10         -3.825e-01  3.753e-01  -1.019   0.3346  \ndate.l10          9.854e+03  1.292e+04   0.763   0.4650  \nsaving.l11       -7.255e+03  3.475e+03  -2.088   0.0664 .\ngdp_deflator.l11 -3.763e+03  2.129e+03  -1.767   0.1110  \nsale.l11          6.217e-01  4.267e-01   1.457   0.1791  \ndate.l11          2.048e+02  9.226e+03   0.022   0.9828  \nsaving.l12        7.267e+02  2.796e+03   0.260   0.8008  \ngdp_deflator.l12 -8.956e+02  1.881e+03  -0.476   0.6453  \nsale.l12          5.491e-01  5.290e-01   1.038   0.3264  \ndate.l12         -2.756e+03  1.187e+04  -0.232   0.8216  \nsaving.l13        5.873e+03  3.144e+03   1.868   0.0946 .\ngdp_deflator.l13  2.631e+03  2.577e+03   1.021   0.3339  \nsale.l13         -4.564e-01  4.780e-01  -0.955   0.3646  \ndate.l13          1.527e+04  8.682e+03   1.759   0.1125  \nsaving.l14       -3.648e+03  2.640e+03  -1.382   0.2003  \ngdp_deflator.l14  4.575e+03  2.837e+03   1.613   0.1413  \nsale.l14         -2.910e-02  3.742e-01  -0.078   0.9397  \ndate.l14          1.203e+04  9.767e+03   1.232   0.2493  \nconst            -1.019e+09  6.567e+08  -1.551   0.1553  \ntrend            -9.144e+06  5.886e+06  -1.553   0.1547  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6356 on 9 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9762 \nF-statistic: 48.56 on 57 and 9 DF,  p-value: 5.12e-07 \n\n\nEstimation results for equation date: \n===================================== \ndate = saving.l1 + gdp_deflator.l1 + sale.l1 + date.l1 + saving.l2 + gdp_deflator.l2 + sale.l2 + date.l2 + saving.l3 + gdp_deflator.l3 + sale.l3 + date.l3 + saving.l4 + gdp_deflator.l4 + sale.l4 + date.l4 + saving.l5 + gdp_deflator.l5 + sale.l5 + date.l5 + saving.l6 + gdp_deflator.l6 + sale.l6 + date.l6 + saving.l7 + gdp_deflator.l7 + sale.l7 + date.l7 + saving.l8 + gdp_deflator.l8 + sale.l8 + date.l8 + saving.l9 + gdp_deflator.l9 + sale.l9 + date.l9 + saving.l10 + gdp_deflator.l10 + sale.l10 + date.l10 + saving.l11 + gdp_deflator.l11 + sale.l11 + date.l11 + saving.l12 + gdp_deflator.l12 + sale.l12 + date.l12 + saving.l13 + gdp_deflator.l13 + sale.l13 + date.l13 + saving.l14 + gdp_deflator.l14 + sale.l14 + date.l14 + const + trend \n\n                   Estimate Std. Error t value Pr(>|t|)    \nsaving.l1         8.299e-02  7.324e-02   1.133 0.286439    \ngdp_deflator.l1   1.194e-01  9.525e-02   1.254 0.241473    \nsale.l1          -1.590e-05  8.467e-06  -1.878 0.093170 .  \ndate.l1          -4.492e-01  2.622e-01  -1.713 0.120843    \nsaving.l2        -7.984e-02  8.484e-02  -0.941 0.371238    \ngdp_deflator.l2  -2.103e-01  1.067e-01  -1.972 0.080143 .  \nsale.l2           9.861e-06  9.533e-06   1.034 0.327939    \ndate.l2          -4.547e-01  1.848e-01  -2.460 0.036172 *  \nsaving.l3        -1.341e-01  8.286e-02  -1.619 0.139954    \ngdp_deflator.l3   1.625e-01  1.008e-01   1.612 0.141445    \nsale.l3           1.319e-05  8.049e-06   1.638 0.135816    \ndate.l3          -3.961e-01  2.028e-01  -1.953 0.082614 .  \nsaving.l4         1.795e-01  6.425e-02   2.794 0.020915 *  \ngdp_deflator.l4  -9.927e-02  1.066e-01  -0.931 0.375966    \nsale.l4          -5.617e-06  7.398e-06  -0.759 0.467135    \ndate.l4          -5.027e-01  1.876e-01  -2.679 0.025241 *  \nsaving.l5        -1.752e-02  8.467e-02  -0.207 0.840662    \ngdp_deflator.l5  -4.003e-02  1.037e-01  -0.386 0.708537    \nsale.l5          -5.790e-06  7.888e-06  -0.734 0.481610    \ndate.l5          -2.319e-01  2.177e-01  -1.065 0.314529    \nsaving.l6        -7.085e-02  6.775e-02  -1.046 0.322971    \ngdp_deflator.l6   1.248e-01  8.285e-02   1.506 0.166408    \nsale.l6          -2.178e-06  6.528e-06  -0.334 0.746331    \ndate.l6          -8.145e-01  1.931e-01  -4.217 0.002248 ** \nsaving.l7        -1.446e-02  6.363e-02  -0.227 0.825273    \ngdp_deflator.l7  -8.114e-03  8.049e-02  -0.101 0.921911    \nsale.l7          -2.832e-06  7.019e-06  -0.404 0.695986    \ndate.l7          -7.434e-01  2.617e-01  -2.841 0.019371 *  \nsaving.l8         1.013e-01  8.858e-02   1.144 0.282109    \ngdp_deflator.l8  -4.556e-02  5.444e-02  -0.837 0.424296    \nsale.l8           2.081e-05  1.155e-05   1.801 0.105199    \ndate.l8          -1.276e-01  2.451e-01  -0.521 0.615182    \nsaving.l9        -1.781e-01  9.900e-02  -1.799 0.105569    \ngdp_deflator.l9  -5.781e-02  5.524e-02  -1.047 0.322605    \nsale.l9          -1.051e-05  1.284e-05  -0.818 0.434331    \ndate.l9          -7.510e-01  2.231e-01  -3.366 0.008311 ** \nsaving.l10        1.108e-01  9.577e-02   1.157 0.276891    \ngdp_deflator.l10  8.634e-02  5.892e-02   1.465 0.176890    \nsale.l10          1.325e-06  9.229e-06   0.144 0.888979    \ndate.l10         -6.388e-02  3.176e-01  -0.201 0.845087    \nsaving.l11        7.038e-02  8.547e-02   0.823 0.431554    \ngdp_deflator.l11  6.357e-02  5.237e-02   1.214 0.255697    \nsale.l11         -1.509e-05  1.050e-05  -1.438 0.184362    \ndate.l11         -8.454e-01  2.269e-01  -3.726 0.004727 ** \nsaving.l12       -1.632e-01  6.878e-02  -2.372 0.041738 *  \ngdp_deflator.l12 -8.413e-02  4.625e-02  -1.819 0.102280    \nsale.l12          7.241e-06  1.301e-05   0.557 0.591356    \ndate.l12         -4.219e-01  2.919e-01  -1.445 0.182287    \nsaving.l13        4.446e-02  7.732e-02   0.575 0.579376    \ngdp_deflator.l13 -8.406e-02  6.339e-02  -1.326 0.217429    \nsale.l13          1.197e-05  1.176e-05   1.019 0.335032    \ndate.l13         -6.007e-01  2.135e-01  -2.813 0.020262 *  \nsaving.l14        5.157e-02  6.493e-02   0.794 0.447430    \ngdp_deflator.l14  8.936e-02  6.977e-02   1.281 0.232283    \nsale.l14         -1.014e-05  9.203e-06  -1.101 0.299312    \ndate.l14         -5.418e-01  2.402e-01  -2.255 0.050550 .  \nconst             8.139e+04  1.615e+04   5.040 0.000700 ***\ntrend             7.255e+02  1.448e+02   5.011 0.000728 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1563 on 9 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.5e+08 on 57 and 9 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                 saving gdp_deflator       sale      date\nsaving        3.208e-01     -0.16908 -1.498e+03  0.002709\ngdp_deflator -1.691e-01      0.32167  2.226e+03 -0.011232\nsale         -1.498e+03   2225.66391  4.040e+07 44.867011\ndate          2.709e-03     -0.01123  4.487e+01  0.024434\n\nCorrelation matrix of residuals:\n              saving gdp_deflator     sale     date\nsaving        1.0000      -0.5264 -0.41625  0.03060\ngdp_deflator -0.5264       1.0000  0.61741 -0.12670\nsale         -0.4162       0.6174  1.00000  0.04516\ndate          0.0306      -0.1267  0.04516  1.00000\n\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one."
  },
  {
    "objectID": "ASV.html#cross-validations-1",
    "href": "ASV.html#cross-validations-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Cross Validations",
    "text": "Cross Validations\n\n\nCode\nn=length(dd3$gdp_deflator)\nn*0.3\n\n\n[1] 24.3\n\n\n\n\nCode\nk=25 #19*4\nn-k\n\n\n[1] 56\n\n\n\n\nCode\ndd3\n\n\n   homevalue saving gdp_deflator   sale  date\n1   121428.3    5.0          2.7 165300 10957\n2   121642.0    4.5          2.5 163200 11048\n3   121906.9    4.3          2.4 168800 11139\n4   122475.1    4.8          2.2 172900 11231\n5   123129.1    4.8          2.6 169800 11323\n6   123830.3    4.8          2.4 179000 11413\n7   124572.9    5.1          1.6 172500 11504\n8   125374.5    5.2          1.3 171100 11596\n9   126208.3    4.5          1.3 188700 11688\n10  127045.3    4.8          1.4 187200 11778\n11  127878.3    4.7          1.9 178100 11869\n12  128712.4    4.4          2.3 190100 11961\n13  129478.1    4.9          2.0 186000 12053\n14  130156.2    5.0          1.4 191800 12143\n15  130810.2    5.3          2.3 191900 12234\n16  131517.4    5.1          2.5 198800 12326\n17  132280.5    4.5          2.9 212700 12418\n18  133063.4    4.4          3.3 217600 12509\n19  133869.0    5.6          2.6 213500 12600\n20  134710.7    6.6          3.1 228800 12692\n21  135559.8    7.0          3.2 232500 12784\n22  136405.3    3.1          2.9 233700 12874\n23  137193.3    3.5          3.7 236400 12965\n24  137932.1    3.8          3.3 243600 13057\n25  138585.5    5.7          2.8 247700 13149\n26  139210.5    5.5          3.6 246300 13239\n27  139847.6    5.6          2.8 235600 13330\n28  140540.8    5.5          1.5 245400 13422\n29  141306.4    6.2          3.9 257400 13514\n30  142135.7    6.2          2.7 242200 13604\n31  143044.9    5.4          2.1 241800 13695\n32  144015.6    5.4          1.7 238400 13787\n33  144999.2    5.9          1.4 233900 13879\n34  145981.3    5.8          2.0 235300 13970\n35  146916.7    5.8          3.1 226500 14061\n36  147827.4    5.4          1.0 222500 14153\n37  148651.4    5.4         -0.2 208400 14245\n38  149440.0    5.3         -0.7 220900 14335\n39  150228.1    5.0          0.4 214300 14426\n40  151086.1    5.1          1.3 219000 14518\n41  152030.1    5.5          1.1 222900 14610\n42  153009.4    5.3          2.0 219500 14700\n43  154040.0    6.1          1.2 224100 14791\n44  155134.6    5.9          2.4 224300 14883\n45  156292.9    5.1          2.1 226900 14975\n46  157461.6    5.3          2.7 228100 15065\n47  158548.5    5.3          2.5 223500 15156\n48  159570.5    5.2          0.5 221100 15248\n49  160546.3    4.9          2.4 238400 15340\n50  161588.7    4.9          1.6 238700 15431\n51  162738.7    4.7          2.1 248800 15522\n52  164024.4    5.1          2.0 251700 15614\n53  165472.1    5.1          1.6 258400 15706\n54  167048.8    5.6          1.1 268100 15796\n55  168747.7    5.0          1.9 264800 15887\n56  170471.2    5.0          2.4 273600 15979\n57  172172.7    4.4          1.7 275200 16071\n58  173803.4    4.3          2.3 288000 16161\n59  175333.8    3.8          1.8 281000 16252\n60  176841.6    6.6          0.7 298900 16344\n61  178300.5    3.2         -0.1 289200 16436\n62  179788.9    2.9          2.2 289100 16526\n63  181324.4    3.1          1.2 295800 16617\n64  183054.1    2.7          0.0 302500 16709\n65  184891.6    3.2         -0.3 299800 16801\n66  186816.3    2.6          2.9 306000 16892\n67  188755.2    2.1          1.1 303800 16983\n68  190716.5    2.6          2.1 310900 17075\n69  192645.2    2.7          2.1 313100 17167\n70  194471.9    2.9          1.3 318200 17257\n71  196139.6    3.2          2.0 320500 17348\n72  197646.4    3.1          2.8 337900 17440\n73  198936.7    3.7          2.5 331800 17532\n74  200135.0    3.9          3.5 315600 17622\n75  201342.0    4.1          1.4 330900 17713\n76  202638.9    3.8          1.8 322800 17805\n77  203918.9    3.6          1.6 313000 17897\n78  205047.8    3.7          2.2 322500 17987\n79  205932.3    3.1          1.3 318400 18078\n80  206612.8    3.3          1.5 327100 18170\n81  207058.4    3.2          1.6 329000 18262\n\n\n\n\nCode\ndat = ts(dd3[,c(1,2,3,4)])\n\n\n\nCross Validation For Home Value\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plot\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Home Value\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRSME Comparesions\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 170107.3\n\n\nCode\nerror2\n\n\n[1] 170105.8\n\n\n\n\n\nCross Validation For Saving Rate\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE PLOT\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 2.784849\n\n\nCode\nerror2\n\n\n[1] 4.109366\n\n\n\n\n\nCross Validation For GDP Deflator\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plots\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 3.594482\n\n\nCode\nerror2\n\n\n[1] 5.038616\n\n\n\n\n\nCross Validation For Sale Price\n\n\nCode\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(4)][1:(k-1)+i] \n  xtest <- dat[,c(4)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n\nRMSE Plots\n\n\nCode\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n\n\n\n\n\nRMSE Values Compares\n\n\nCode\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\n\n\n[1] 268955.8\n\n\nCode\nerror2\n\n\n[1] 268954.3\n\n\nThe second one VAR(14) is better for Home Value and Sale Price, However, overall, not too different The first one VAR(13) is better for GDP Deflator and Saving Rate Therefore, VAR(13) is relatively better than VAR(14)"
  },
  {
    "objectID": "ASV.html#forecasts",
    "href": "ASV.html#forecasts",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasts",
    "text": "Forecasts\n\n\nCode\nfit1 <- vars::VAR(dat[,1:4], p=13, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n\n\n         fcst     lower    upper       CI\n[1,] 3.718453 2.6102444 4.826661 1.108208\n[2,] 5.602610 4.4620373 6.743183 1.140573\n[3,] 7.872473 6.4972147 9.247731 1.375258\n[4,] 4.592520 3.0702843 6.114757 1.522236\n[5,] 5.953920 4.3310094 7.576831 1.622911\n[6,] 2.901753 0.8499155 4.953590 2.051837\n[7,] 4.187176 2.0589012 6.315450 2.128275\n[8,] 2.717952 0.3920197 5.043885 2.325933\n\n\n\n\nCode\nfcast1$fcst$sale\n\n\n         fcst    lower    upper       CI\n[1,] 348102.2 336133.1 360071.2 11969.06\n[2,] 353458.9 339669.3 367248.4 13789.55\n[3,] 350327.6 336109.4 364545.8 14218.21\n[4,] 321335.8 304967.9 337703.7 16367.93\n[5,] 309302.3 291867.5 326737.0 17434.72\n[6,] 286105.4 268004.7 304206.1 18100.70\n[7,] 271981.5 253762.0 290200.9 18219.43\n[8,] 248976.4 229541.9 268410.9 19434.55\n\n\n\n\nCode\nfcast1$fcst$saving\n\n\n          fcst     lower     upper       CI\n[1,]  3.774885  2.665978  4.883793 1.108907\n[2,]  4.885199  3.206902  6.563496 1.678297\n[3,]  9.928629  8.142069 11.715188 1.786559\n[4,] 10.166946  8.366419 11.967473 1.800527\n[5,] 12.688735 10.877602 14.499869 1.811134\n[6,] 10.564305  8.658945 12.469664 1.905360\n[7,]  9.590800  7.567340 11.614261 2.023461\n[8,]  8.682248  6.444347 10.920149 2.237901\n\n\n\n\nCode\nfcast1$fcst$homevalue\n\n\n         fcst    lower    upper         CI\n[1,] 207370.9 207313.3 207428.5   57.61206\n[2,] 207451.6 207280.8 207622.4  170.81360\n[3,] 207354.1 206979.7 207728.4  374.33551\n[4,] 207004.8 206375.3 207634.3  629.46283\n[5,] 206534.7 205611.8 207457.5  922.84057\n[6,] 206003.4 204767.3 207239.5 1236.10298\n[7,] 205538.6 203971.4 207105.8 1567.18724\n[8,] 205002.0 203098.4 206905.6 1903.59212\n\n\n\n\nCode\nfcast1$fcst$sale\n\n\n         fcst    lower    upper       CI\n[1,] 348102.2 336133.1 360071.2 11969.06\n[2,] 353458.9 339669.3 367248.4 13789.55\n[3,] 350327.6 336109.4 364545.8 14218.21\n[4,] 321335.8 304967.9 337703.7 16367.93\n[5,] 309302.3 291867.5 326737.0 17434.72\n[6,] 286105.4 268004.7 304206.1 18100.70\n[7,] 271981.5 253762.0 290200.9 18219.43\n[8,] 248976.4 229541.9 268410.9 19434.55\n\n\n\n\nCode\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")"
  }
]