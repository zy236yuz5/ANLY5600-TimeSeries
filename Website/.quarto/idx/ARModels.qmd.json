{"title":"ARMA/ARIMA/SARIMA Models","markdown":{"yaml":{"title":"ARMA/ARIMA/SARIMA Models","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","SAF.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]},"format":{"html":{"theme":"sandstone","css":"./styles/layout.css","code-fold":true,"toc":true}}},"headingText":"The Household Income Data Analysis","containsRefs":false,"markdown":"\n\nAfter Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation. \n\nTo be more specific, we need to understand the concepts about the models:\nCertainly! Here's a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:\n\n1. **ARMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term of order \\( p \\). It describes the relationship of the current observation with its previous values.\n     - **MA(q)**: Moving Average term of order \\( q \\). It captures the relationship between an observation and the white noise or error terms from previous time points.\n     \n   - **Equation**:\n     \\[\n     X_t = c + \\phi_1X_{t-1} + \\phi_2X_{t-2} + \\dots + \\phi_pX_{t-p} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + \\dots + \\theta_qa_{t-q} + a_t\n     \\]\n     where \\( a_t \\) is white noise.\n\n2. **ARIMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term.\n     - **I(d)**: Integrated term of order \\( d \\). It represents the number of differences needed to make the series stationary.\n     - **MA(q)**: Moving Average term.\n     \n   - **Equation**:\n     Once the series is differenced \\( d \\) times and becomes stationary, its ARIMA representation becomes the same as ARMA, but applied to the differenced series.\n\n3. **SARIMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term.\n     - **I(d)**: Integrated term.\n     - **MA(q)**: Moving Average term.\n     - **SAR(P)**: Seasonal AutoRegressive term of order \\( P \\).\n     - **SI(D)**: Seasonal Integrated term of order \\( D \\). It indicates the number of seasonal differences required.\n     - **SMA(Q)**: Seasonal Moving Average term of order \\( Q \\).\n     \n   - **Equation**:\n     The SARIMA model combines both non-seasonal and seasonal components. Once both seasonal and non-seasonal differencing are applied (if needed), the SARIMA equation becomes an extension of the ARMA equation but with additional seasonal terms.\n     \\[\n     X_t = c + \\phi_1X_{t-1} + \\dots + \\phi_pX_{t-p} + \\theta_1a_{t-1} + \\dots + \\theta_qa_{t-q} + \\Phi_1X_{t-P} + \\dots + \\Phi_PX_{t-PS} + \\Theta_1a_{t-Q} + \\dots + \\Theta_Qa_{t-QS} + a_t\n     \\]\n     where \\( S \\) represents the seasonal period and \\( a_t \\) is white noise.\n\n\n\n\n\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n```{r, echo = FALSE, message = FALSE, warning = FALSE}\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(gridExtra)\nlibrary(plotly)\nlibrary(TTR) # For the SMA function\n\n```\n\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/household_saving.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n```{r,warning=FALSE,echo=FALSE}\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = W398RC1A027NBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Income\",\n       x = \"Date\", \n       y = \"The Household Income\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n```\n\n\n\n\n\n\n### ACF & PACF\n```{r, echo=FALSE}\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$W398RC1A027NBEA) +\n  labs(title = \"ACF of Household Income Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# PACF Plot\nggPacf(df$W398RC1A027NBEA) +\n  labs(title = \"PACF of Household Income Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n### Validation with ADF Test\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(df$W398RC1A027NBEA)\n\n\n```\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\n\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\n### Detrended and Log-transformed\n```{r, echo=FALSE}\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$W398RC1A027NBEA))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Household Income Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\n\n```{r,warning=FALSE, echo=FALSE}\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$W398RC1A027NBEA))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Household Income\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that after detrending and transforming, the household income remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n### Test it stationary again\n```{r, echo=FALSE}\n\nadf.test(log_transformed_data$LogTransformed)\n\n\n```\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n```{r, echo = FALSE}\nfit = lm(df$W398RC1A027NBEA~time(df$W398RC1A027NBEA), na.action=NULL) \n\n```\n\n### Make it Stationary by using differencing\n\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n```{r, echo = FALSE}\n#install.packages(\"gridExtra\")\n\n```\n\n```{r, echo = FALSE}\n\nt = ts(diff(diff(df$W398RC1A027NBEA)))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"ACF for two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"PACF for two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n```{r, warning=FALSE, echo = FALSE}\nadf.test(diff(df$W398RC1A027NBEA, differences = 2))\n\n\n```\n\nHowever, the second order of differencing still cannot make it stationary.\nWe need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n### Third Differencing\n```{r, echo = FALSE}\n\nt = ts(diff(diff(diff(df$W398RC1A027NBEA))))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"ACF for three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"PACF for three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\n\n\n\n\n```{r, warning=FALSE, echo = FALSE}\n# Load required libraries\nlibrary(ggplot2)\n\n\n# First, perform the augmented Dickey-Fuller test\nadf_result <- adf.test(diff(df$W398RC1A027NBEA, differences = 3))\n\n# Print the ADF test result\nprint(adf_result)\n```\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n### Evaluate the values for p & q\n```{r, warning=FALSE, echo = FALSE}\n# Set the theme\ntheme_demo <- theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# Plot the ACF and PACF of the differenced series with the demo theme\npar(mfrow=c(2,1)) # Set up a 2x1 grid for plotting\ndiff_series = diff(df$W398RC1A027NBEA, differences = 3)\n# ACF Plot\nacf_plot <- acf(diff_series, plot = FALSE)\nprint(acf_plot, main=\"ACF of Differenced Household Income Time Series\")\nggAcf(diff_series) + theme_demo\n\n# PACF Plot\npacf_plot <- pacf(diff_series, plot = FALSE)\nprint(pacf_plot, main=\"PACF of Differenced Household Income Time Series\")\nggPacf(diff_series) + theme_demo\n\n```\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\n\n\n\n### Fit Model\n\nAfter we determing our parameters, we can start fitting the models.\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$W398RC1A027NBEA, differences = 3)\nmodel <- Arima(diff, order=c(1,3,2))\nsummary(model)\n\n```\n\nFrom here, we get a summary about the aic and bic values.\n\n### Equation of the Model\n\nGiven the specified values \\(p = 1\\), \\(q = 2\\), and \\(d = 3\\), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n\n\\[ X_t = c + \\phi_1X_{t-1} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + a_t \\]\n\nIn my case the third difference is represented as \\( \\nabla^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} \\). The ARIMA equation provided is built upon this differenced series.\n\n### Model diagnostics\n\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n```{r,warning=FALSE, echo = FALSE}\n## empty list to store model fits\nset.seed((150))\narma14 =arima.sim(list(order=c(1,3,1), ar=-.4, ma=c(-.3)), n=10000)\narma14 %>% ggtsdisplay()\nARMA_res <- list()\n\n## set counter\ncc <-1\n\n## loop over AR\nfor(p in 0:3){\n  ## loop over MA\n  for(q in 0:4){\n    \n  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,3,q))\n  cc<- cc+1\n  }\n}\n\n## get AIC values for model evaluation\n\nARMA_AIC<-sapply(ARMA_res,function(x) x$aic)\n\nARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]\n```\n\n\n\n```{r,warning=FALSE, echo = FALSE}\nd=3\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*17),nrow=17) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(df$W398RC1A027NBEA,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n```\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n```{r, echo = FALSE}\ntemp[which.min(temp$AIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$BIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$AICc),]\n```\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n### Model Compare\n\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n```{r,warning=FALSE, echo = FALSE}\n######### compare 3 models\nset.seed(345)\nmodel_output <- capture.output(sarima(df$W398RC1A027NBEA, 2,3,2))\n```\n\n```{r, echo = FALSE}\nmodel_output2 <- capture.output(sarima(df$W398RC1A027NBEA, 1,3,2))\n```\n\n```{r,warning=FALSE, echo = FALSE}\n#################### fitted vs. actual plot  ##############\nfit1=Arima(df$W398RC1A027NBEA,order=c(1,3,2),include.drift = TRUE)\nfit2=Arima(df$W398RC1A027NBEA,order=c(2,3,2),include.drift = TRUE) #warning for the drift, no drift \n# Set background color\npar(bg = \"#E0E0E0\")\n\n# Plotting the data and model fits with custom background and border colors\nplot(df$W398RC1A027NBEA, col=\"blue\", main=\"Time Series with SARIMA Fits\", \n     ylab=\"Value\", xlab=\"Time\", \n     panel.first = rect(par(\"usr\")[1], par(\"usr\")[3], par(\"usr\")[2], par(\"usr\")[4], \n                        col = \"#E0E0E0\", border = \"#E0E0E0\"))\n\nlines(fitted(fit1), col=\"green\")\nlines(fitted(fit2), col=\"red\") \n\n# Adding a legend\nlegend(\"topleft\", \n       legend = c(\"The Time Series Of the Data\", \"fit1\", \"fit2\"), \n       col = c(\"blue\", \"green\", \"red\"), \n       lty=1, # Type of the line (1 is for \"solid\")\n       cex=0.8, # Font size of the legend text\n       box.col=\"#E0E0E0\", bg=\"#E0E0E0\") \n\n```\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\n### Forecasting for the dataset using the best parameters\n```{r, echo = FALSE}\nforecast(fit2,10) # Forecasting for the next 10 minutes\n```\n\n```{r, echo = FALSE}\n\n\n# Autoplot with custom colors\nplot_fit2 <- autoplot(forecast(fit2)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit2)\n```\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n### BENCHMARK\n\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$W398RC1A027NBEA, differences = 3)\nmodel <- Arima(diff, order=c(1,3,2))\n\n```\n\n#### The meanf model with residual plot\n```{r, echo = FALSE}\nf1<-meanf(df$W398RC1A027NBEA, h=11) #mean\n\ncheckresiduals(f1)\n```\n\n#### The Arima model\n```{r, echo = FALSE}\ndf$W398RC1A027NBEA %>%\n  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% \n  residuals() %>% \n  ggtsdisplay()\n```\n\n```{r, echo = FALSE}\nfit=Arima(df$W398RC1A027NBEA, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) \nsummary(fit)\n```\n\n```{r, echo = FALSE}\nf1 <- meanf(df$W398RC1A027NBEA, h=10) \n\n#checkresiduals(f1)\n\nf2 <- naive(df$W398RC1A027NBEA, h=10) \n\n#checkresiduals(f2)\n\nf3 <- rwf(df$W398RC1A027NBEA,drift=TRUE, h=10) \n```\n\n#### Accuracy of the fitted models\n```{r, echo = FALSE}\npred=forecast(fit2,20)\naccuracy(pred)\npred[['mean']]\n```\n\n#### Accuracy of the based models\n```{r, echo = FALSE}\naccuracy(f1)\n```\n\n```{r, echo = FALSE}\naccuracy(f2)\n```\n\n```{r, echo = FALSE}\naccuracy(f3)\n```\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n```{r, echo = FALSE}\ngdp.ts=ts(df$W398RC1A027NBEA)\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp.ts, h=11),\n            series=\"fit\", PI=FALSE) +\n  autolayer(forecast(fit2, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for Household Income\") +\n  xlab(\"Year\") + ylab(\"Household Income\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=20),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=20),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf(gdp.ts, drift=TRUE, h=40),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(fit2,20), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"Household Income in US\") +\n  xlab(\"Time\") + ylab(\"Household Income\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending.\n\n## The Median House Sale Price Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/MSPUS.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n```{r,warning=FALSE,echo=FALSE}\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = MSPUS)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Sale Price\",\n       x = \"Date\", \n       y = \"The Household Sale Price\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n```\n\n\n\n\n\n\n### ACF & PACF\n```{r, echo=FALSE}\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$MSPUS) +\n  labs(title = \"ACF of Household Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# PACF Plot\nggPacf(df$MSPUS) +\n  labs(title = \"PACF of Household Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n### Validation with ADF Test\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(df$MSPUS)\n\n\n```\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\n\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\n### Detrended and Log-transformed\n```{r, echo=FALSE}\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$MSPUS))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Household Sale Price Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\n\n```{r,warning=FALSE, echo=FALSE}\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$MSPUS))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Household Sale Price\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n### Test it stationary again\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(log_transformed_data$LogTransformed)\n\n\n```\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n### Make it Stationary by using differencing\n\n\n```{r, echo = FALSE}\n\nt = ts(diff(df$MSPUS))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(df$MSPUS)) +\n  labs(title = \"ACF for first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(df$MSPUS)) +\n  labs(title = \"PACF for first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\n\nAfter the first order of differencing, we can see the changes regarding the plots as a whole.\n\n\n```{r, warning=FALSE, echo = FALSE}\nadf.test(diff(df$MSPUS, differences = 1))\n\n\n```\n\nWe can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.\n\n```{r, warning=FALSE, echo = FALSE}\n# Load required libraries\n\n# First, perform the augmented Dickey-Fuller test\nadf_result <- adf.test(diff(df$MSPUS, differences = 1))\n\n\n\n# Set the theme\ntheme_demo <- theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# Plot the ACF and PACF of the differenced series with the demo theme\npar(mfrow=c(2,1)) # Set up a 2x1 grid for plotting\ndiff_series = diff(df$MSPUS, differences = 3)\n# ACF Plot\nacf_plot <- acf(diff_series, plot = FALSE)\nprint(acf_plot, main=\"ACF of Differenced Household Sale Price Time Series\")\nggAcf(diff_series) + theme_demo\n\n# PACF Plot\npacf_plot <- pacf(diff_series, plot = FALSE)\nprint(pacf_plot, main=\"PACF of Differenced Household Sale Price Time Series\")\nggPacf(diff_series) + theme_demo\n\n```\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.\n\n### Fit model\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$MSPUS, differences = 1)\nmodel <- Arima(diff, order=c(1,1,1))\nsummary(model)\n\n```\n\n### Equation of the Model\n\nGiven the specified values \\(p = 1\\), \\(q = 1\\), and \\(d = 1\\), we can write out the ARIMA(1,1,1) model equation using the general equation based on the results of the model:\n\n\\[ X_t = c + \\phi_1X_{t-1} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + a_t \\]\n\nIn my case the third difference is represented as \\( \\nabla^1 X_t = X_t + 0.0092X_{t-1} - 0.9827Y_{t-1} \\). The ARIMA equation provided is built upon this differenced series.\n\n### Model diagnostics\n\n```{r,warning=FALSE, echo = FALSE}\n## empty list to store model fits\nset.seed((150))\narma14 =arima.sim(list(order=c(1,1,1), ar=-.1, ma=c(-.1)), n=10000)\narma14 %>% ggtsdisplay()\nARMA_res <- list()\n\n## set counter\ncc <-1\n\n## loop over AR\nfor(p in 0:3){\n  ## loop over MA\n  for(q in 0:4){\n    \n  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,1,q))\n  cc<- cc+1\n  }\n}\n\n## get AIC values for model evaluation\n\nARMA_AIC<-sapply(ARMA_res,function(x) x$aic)\n\nARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]\n```\n\n```{r,warning=FALSE, echo = FALSE}\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 2:3)# p=1,2,3 : 3\n{\n  for(q in 2:4)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(df$MSPUS,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n```\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.\n\n```{r, echo = FALSE}\ntemp[which.min(temp$AIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$BIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$AICc),]\n```\n\nSince we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n### Model Compare\n\nNow, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):\n```{r,warning=FALSE, echo = FALSE}\n######### compare 3 models\nset.seed(345)\nmodel_output <- capture.output(sarima(df$MSPUS, 2,2,3))\n```\n\n```{r, echo = FALSE}\nmodel_output2 <- capture.output(sarima(df$MSPUS, 1,1,1))\n```\n\n```{r,warning=FALSE, echo = FALSE}\n#################### fitted vs. actual plot  ##############\nfit1=Arima(df$MSPUS,order=c(2,2,3),include.drift = TRUE)\nfit2=Arima(df$MSPUS,order=c(1,1,1),include.drift = TRUE) #warning for the drift, no drift \n# Set background color\npar(bg = \"#E0E0E0\")\n\n# Plotting the data and model fits with custom background and border colors\nplot(df$MSPUS, col=\"blue\", main=\"Time Series with SARIMA Fits\", \n     ylab=\"Value\", xlab=\"Time\", \n     panel.first = rect(par(\"usr\")[1], par(\"usr\")[3], par(\"usr\")[2], par(\"usr\")[4], \n                        col = \"#E0E0E0\", border = \"#E0E0E0\"))\n\nlines(fitted(fit1), col=\"black\")\nlines(fitted(fit2), col=\"red\") \n\n# Adding a legend\nlegend(\"topleft\", \n       legend = c(\"The Time Series Of the Data\", \"fit1\", \"fit2\"), \n       col = c(\"blue\", \"black\", \"red\"), \n       lty=1, # Type of the line (1 is for \"solid\")\n       cex=0.8, # Font size of the legend text\n       box.col=\"#E0E0E0\", bg=\"#E0E0E0\") \n\n```\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.\n\n### Forecasting for the dataset \n```{r, echo = FALSE}\nforecast(fit2,50) # Forecasting for the next 10 minutes\n```\n\n```{r, echo = FALSE}\n\n\n# Autoplot with custom colors\nplot_fit2 <- autoplot(forecast(fit2)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit2)\n```\n\nFrom the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.\n\n### BENCHMARK\n\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$MSPUS, differences = 1)\nmodel <- Arima(diff, order=c(2,2,3))\n\n```\n\n#### The meanf model with residual plot\n\n```{r, echo = FALSE}\nf1<-meanf(df$MSPUS, h=11) #mean\n\ncheckresiduals(f1)\n```\n\n#### The Arima model\n\n```{r, echo = FALSE}\ndf$MSPUS %>%\n  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% \n  residuals() %>% \n  ggtsdisplay()\n```\n\n```{r, echo = FALSE}\nfit=Arima(df$MSPUS, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) \nsummary(fit)\n```\n\n```{r, echo = FALSE}\nf1 <- meanf(df$MSPUS, h=10) \n\n#checkresiduals(f1)\n\nf2 <- naive(df$MSPUS, h=10) \n\n#checkresiduals(f2)\n\nf3 <- rwf(df$MSPUS,drift=TRUE, h=10) \n```\n\n#### Accuracy of the fitted models\n```{r, echo = FALSE}\npred=forecast(fit2,20)\naccuracy(pred)\npred[['mean']]\n```\n\n#### Accuracy of the based models\n\n```{r, echo = FALSE}\naccuracy(f1)\n```\n\n```{r, echo = FALSE}\naccuracy(f2)\n```\n\n```{r, echo = FALSE}\naccuracy(f3)\n```\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n```{r, echo = FALSE}\ngdp.ts=ts(df$MSPUS)\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp.ts, h=11),\n            series=\"fit\", PI=FALSE) +\n  autolayer(forecast(fit2, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for Household Sale Price\") +\n  xlab(\"Year\") + ylab(\"Household Sale Price\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=20),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=20),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf(gdp.ts, drift=TRUE, h=40),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(fit2,20), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"Household Sale Price in US)\") +\n  xlab(\"Time\") + ylab(\"Household Sale Price\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the  trending pattern.\n\n\n\n\n\n# References and Codes\n- Codes: [Rmd, Python & Qmd](https://github.com/zy236yuz5/ANLY5600-TimeSeries)\n- U.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.\n- U.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.\n- U.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.\n- Zillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/."},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/layout.css","./styles/layout.css"],"toc":true,"output-file":"ARModels.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","theme":"sandstone","title":"ARMA/ARIMA/SARIMA Models","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","SAF.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]}},"extensions":{"book":{"multiFile":true}}}}}