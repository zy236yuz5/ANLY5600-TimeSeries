{"title":"ARMA/ARIMA/SARIMA Models","markdown":{"yaml":{"title":"ARMA/ARIMA/SARIMA Models","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]},"format":{"html":{"theme":"sandstone","css":"./styles/layout.css","code-fold":true,"toc":true}}},"headingText":"ARMA/ARIMA Analysis (With Covid-19 Impacts)","containsRefs":false,"markdown":"\n\nAfter Exploratory Data Analysis (EDA), we are going to have a deeper understanding of the time series data by applying different models. This section involves multiple medthods and datasets with different models such as ARMA, ARIMA, SARIMA for us to gain insights and understandings such as ACF, PACF, ADF tests, differencing, and comparision. For these methods, we are going to identify correlations, stationaries, and performance evaluation. \n\nTo be more specific, we need to understand the concepts about the models:\nCertainly! Here's a brief overview of the ARMA, ARIMA, and SARIMA models with their associated equations:\n\n1. **ARMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term of order \\( p \\). It describes the relationship of the current observation with its previous values.\n     - **MA(q)**: Moving Average term of order \\( q \\). It captures the relationship between an observation and the white noise or error terms from previous time points.\n     \n   - **Equation**:\n     \\[\n     X_t = c + \\phi_1X_{t-1} + \\phi_2X_{t-2} + \\dots + \\phi_pX_{t-p} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + \\dots + \\theta_qa_{t-q} + a_t\n     \\]\n     where \\( a_t \\) is white noise.\n\n2. **ARIMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term.\n     - **I(d)**: Integrated term of order \\( d \\). It represents the number of differences needed to make the series stationary.\n     - **MA(q)**: Moving Average term.\n     \n   - **Equation**:\n     Once the series is differenced \\( d \\) times and becomes stationary, its ARIMA representation becomes the same as ARMA, but applied to the differenced series.\n\n3. **SARIMA**:\n\n   - **Components**:\n     - **AR(p)**: AutoRegressive term.\n     - **I(d)**: Integrated term.\n     - **MA(q)**: Moving Average term.\n     - **SAR(P)**: Seasonal AutoRegressive term of order \\( P \\).\n     - **SI(D)**: Seasonal Integrated term of order \\( D \\). It indicates the number of seasonal differences required.\n     - **SMA(Q)**: Seasonal Moving Average term of order \\( Q \\).\n     \n   - **Equation**:\n     The SARIMA model combines both non-seasonal and seasonal components. Once both seasonal and non-seasonal differencing are applied (if needed), the SARIMA equation becomes an extension of the ARMA equation but with additional seasonal terms.\n     \\[\n     X_t = c + \\phi_1X_{t-1} + \\dots + \\phi_pX_{t-p} + \\theta_1a_{t-1} + \\dots + \\theta_qa_{t-q} + \\Phi_1X_{t-P} + \\dots + \\Phi_PX_{t-PS} + \\Theta_1a_{t-Q} + \\dots + \\Theta_Qa_{t-QS} + a_t\n     \\]\n     where \\( S \\) represents the seasonal period and \\( a_t \\) is white noise.\n\n\n\n## The Household Saving Data Analysis\n\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the household income data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n```{r, echo = FALSE, message = FALSE, warning = FALSE}\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(gridExtra)\nlibrary(plotly)\nlibrary(TTR) # For the SMA function\n\n```\n\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/household_saving.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n```{r,warning=FALSE,echo=FALSE}\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = W398RC1A027NBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Saving\",\n       x = \"Date\", \n       y = \"The Household Saving\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n```\n\n\n\n\n\n\n### ACF & PACF\n```{r, echo=FALSE}\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$W398RC1A027NBEA) +\n  labs(title = \"ACF of Household Saving Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# PACF Plot\nggPacf(df$W398RC1A027NBEA) +\n  labs(title = \"PACF of Household Saving Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n### Validation with ADF Test\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(df$W398RC1A027NBEA)\n\n\n```\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\n\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\n### Detrended and Log-transformed\n```{r, echo=FALSE}\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$W398RC1A027NBEA))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Household Saving Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\n\n```{r,warning=FALSE, echo=FALSE}\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$W398RC1A027NBEA))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Household Saving\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that after detrending and transforming, the household Saving remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n### Test it stationary again\n```{r, echo=FALSE}\n\nadf.test(log_transformed_data$LogTransformed)\n\n\n```\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n```{r, echo = FALSE}\nfit = lm(df$W398RC1A027NBEA~time(df$W398RC1A027NBEA), na.action=NULL) \n\n```\n\n### Make it Stationary by using differencing\n\nWe will try the second order differencing to see the result since the dataset seems to be overly not stationary.\n\n```{r, echo = FALSE}\n#install.packages(\"gridExtra\")\n\n```\n\n```{r, echo = FALSE}\n\nt = ts(diff(diff(df$W398RC1A027NBEA)))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"ACF for two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"PACF for two Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\nAfter the second orders of differencing, we can see the changes regarding the plots as a whole.\n\n\n```{r, warning=FALSE, echo = FALSE}\nadf.test(diff(df$W398RC1A027NBEA, differences = 2))\n\n\n```\n\nHowever, the second order of differencing still cannot make it stationary.\nWe need to do more. However, we should reach the limit with the third order since we do not want to over differencing the dataset.\n\n### Third Differencing\n```{r, echo = FALSE}\n\nt = ts(diff(diff(diff(df$W398RC1A027NBEA))))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"ACF for three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(diff(df$W398RC1A027NBEA)))) +\n  labs(title = \"PACF for three Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\n\n\n\n\n```{r, warning=FALSE, echo = FALSE}\n# Load required libraries\nlibrary(ggplot2)\n\n\n# First, perform the augmented Dickey-Fuller test\nadf_result <- adf.test(diff(df$W398RC1A027NBEA, differences = 3))\n\n# Print the ADF test result\nprint(adf_result)\n```\n\nNow, as we can see, the p-value is now smaller than the significant level. The dataset is stationary.\n\n### Evaluate the values for p & q\n```{r, warning=FALSE, echo = FALSE}\n# Set the theme\ntheme_demo <- theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# Plot the ACF and PACF of the differenced series with the demo theme\npar(mfrow=c(2,1)) # Set up a 2x1 grid for plotting\ndiff_series = diff(df$W398RC1A027NBEA, differences = 3)\n# ACF Plot\nacf_plot <- acf(diff_series, plot = FALSE)\nprint(acf_plot, main=\"ACF of Differenced Household Saving Time Series\")\nggAcf(diff_series) + theme_demo\n\n# PACF Plot\npacf_plot <- pacf(diff_series, plot = FALSE)\nprint(pacf_plot, main=\"PACF of Differenced Household Saving Time Series\")\nggPacf(diff_series) + theme_demo\n\n```\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 2 for the MA component. And we use d = 3 since we utilized third orders of differencing.\n\n\n\n\n\n### Fit Model\n\nAfter we determing our parameters, we can start fitting the models.\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$W398RC1A027NBEA, differences = 3)\nmodel <- Arima(diff, order=c(1,3,2))\nsummary(model)\n\n```\n\nFrom here, we get a summary about the aic and bic values.\n\n### Equation of the Model\n\nGiven the specified values \\(p = 1\\), \\(q = 2\\), and \\(d = 3\\), we can write out the ARIMA(1,3,2) model equation using the general equation based on the results of the model:\n\n\\[ X_t = c + \\phi_1X_{t-1} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + a_t \\]\n\nIn my case the third difference is represented as \\( \\nabla^3 Y_t = Y_t - 0.7911Y_{t-1} - 1.8982Y_{t-2} + 0.9996Y_{t-3} \\). The ARIMA equation provided is built upon this differenced series.\n\n### Model diagnostics\n\nModel diagnostics are very important for us to determine the performance of the parameters we choose. By setting different parameters, we can have a more general view and understanding regarding the model.\n\n```{r,warning=FALSE, echo = FALSE}\n## empty list to store model fits\nset.seed((150))\narma14 =arima.sim(list(order=c(1,3,1), ar=-.4, ma=c(-.3)), n=10000)\narma14 %>% ggtsdisplay()\nARMA_res <- list()\n\n## set counter\ncc <-1\n\n## loop over AR\nfor(p in 0:3){\n  ## loop over MA\n  for(q in 0:4){\n    \n  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,3,q))\n  cc<- cc+1\n  }\n}\n\n## get AIC values for model evaluation\n\nARMA_AIC<-sapply(ARMA_res,function(x) x$aic)\n\nARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]\n```\n\n\n\n```{r,warning=FALSE, echo = FALSE}\nd=3\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*17),nrow=17) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(df$W398RC1A027NBEA,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n```\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. Therefore, the new parameter sets are reasonable.\n\n```{r, echo = FALSE}\ntemp[which.min(temp$AIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$BIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$AICc),]\n```\n\nIn addition, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n### Model Compare\n\nNow, to be more specific, we compare the two different parameter sets: (2,3,2) and (1,3,2):\n\n```{r,warning=FALSE, echo = FALSE}\n######### compare 3 models\nset.seed(345)\nmodel_output <- capture.output(sarima(df$W398RC1A027NBEA, 2,3,2))\n```\n\n```{r, echo = FALSE}\nmodel_output2 <- capture.output(sarima(df$W398RC1A027NBEA, 1,3,2))\n```\n\n```{r,warning=FALSE, echo = FALSE}\n#################### fitted vs. actual plot  ##############\nfit1=Arima(df$W398RC1A027NBEA,order=c(1,3,2),include.drift = TRUE)\nfit2=Arima(df$W398RC1A027NBEA,order=c(2,3,2),include.drift = TRUE) #warning for the drift, no drift \n# Set background color\npar(bg = \"#E0E0E0\")\n\n# Plotting the data and model fits with custom background and border colors\nplot(df$W398RC1A027NBEA, col=\"blue\", main=\"Time Series with SARIMA Fits\", \n     ylab=\"Value\", xlab=\"Time\", \n     panel.first = rect(par(\"usr\")[1], par(\"usr\")[3], par(\"usr\")[2], par(\"usr\")[4], \n                        col = \"#E0E0E0\", border = \"#E0E0E0\"))\n\nlines(fitted(fit1), col=\"green\")\nlines(fitted(fit2), col=\"red\") \n\n# Adding a legend\nlegend(\"topleft\", \n       legend = c(\"The Time Series Of the Data\", \"fit1\", \"fit2\"), \n       col = c(\"blue\", \"green\", \"red\"), \n       lty=1, # Type of the line (1 is for \"solid\")\n       cex=0.8, # Font size of the legend text\n       box.col=\"#E0E0E0\", bg=\"#E0E0E0\") \n\n```\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations, but overall, the second fit is better as indicated from model diagoosis.\n\n\n### Forecasting for the dataset using the best parameters\n```{r, echo = FALSE}\nforecast(fit2,10) # Forecasting for the next 10 minutes\n```\n\n```{r, echo = FALSE}\n\n\n# Autoplot with custom colors\nplot_fit2 <- autoplot(forecast(fit2)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit2)\n```\n\nFrom the above graph, we can note that the forecasted number follows a pattern with time period from (28 to 30). This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. However, the model did capture the upward trending and certain seasonality. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough. Therefore, further action such as benchmarking should be made to compare the models to see if the model performs well.\n\n### BENCHMARK\n\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$W398RC1A027NBEA, differences = 3)\nmodel <- Arima(diff, order=c(1,3,2))\n\n```\n\n#### The meanf model with residual plot\n```{r, echo = FALSE}\nf1<-meanf(df$W398RC1A027NBEA, h=11) #mean\n\ncheckresiduals(f1)\n```\n\n#### The Arima model\n```{r, echo = FALSE}\ndf$W398RC1A027NBEA %>%\n  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% \n  residuals() %>% \n  ggtsdisplay()\n```\n\n```{r, echo = FALSE}\nfit=Arima(df$W398RC1A027NBEA, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) \nsummary(fit)\n```\n\n```{r, echo = FALSE}\nf1 <- meanf(df$W398RC1A027NBEA, h=10) \n\n#checkresiduals(f1)\n\nf2 <- naive(df$W398RC1A027NBEA, h=10) \n\n#checkresiduals(f2)\n\nf3 <- rwf(df$W398RC1A027NBEA,drift=TRUE, h=10) \n```\n\n#### Accuracy of the fitted models\n```{r, echo = FALSE}\npred=forecast(fit2,20)\naccuracy(pred)\npred[['mean']]\n```\n\n#### Accuracy of the based models\n```{r, echo = FALSE}\naccuracy(f1)\n```\n\n```{r, echo = FALSE}\naccuracy(f2)\n```\n\n```{r, echo = FALSE}\naccuracy(f3)\n```\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n```{r, echo = FALSE}\ngdp.ts=ts(df$W398RC1A027NBEA)\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp.ts, h=11),\n            series=\"fit\", PI=FALSE) +\n  autolayer(forecast(fit2, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for Household Saving\") +\n  xlab(\"Year\") + ylab(\"Household Saving\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=20),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=20),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf(gdp.ts, drift=TRUE, h=40),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(fit2,20), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"Household Saving in US\") +\n  xlab(\"Time\") + ylab(\"Household Saving\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the seaonal pattern and trending.\n\n## The Median House Sale Price Data Analysis\nDue to the differencing methods should be used when the dataset is not stationary. Here, as in EDA section, the House Sale Price data analysis provides results that this dataset is not stationary. To have a brief recap, I have provided the orginal dataset and the acf and pacf plots.\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/MSPUS.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n```{r,warning=FALSE,echo=FALSE}\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = MSPUS)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Sale Price\",\n       x = \"Date\", \n       y = \"The Household Sale Price\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n\n```\n\n\n\n\n\n\n### ACF & PACF\n```{r, echo=FALSE}\nlibrary(forecast)\n\n# ACF Plot\nggAcf(df$MSPUS) +\n  labs(title = \"ACF of Household Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# PACF Plot\nggPacf(df$MSPUS) +\n  labs(title = \"PACF of Household Sale Price Time Series\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that the dataset has a correlation, which also means that it is not stationary. In order to prove this conclusion, we utilize the adf test to ensure the results are the same.\n\n\n### Validation with ADF Test\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(df$MSPUS)\n\n\n```\n\nBased on the result, we can see that the p-value is greater than the thershold value, this means that we fail to reject the Null hypothesis. The time series dataset is not stationary.\n\nThen, to explore more, we utilize detrended and log-transformation to see about the patterns\n\n\n### Detrended and Log-transformed\n```{r, echo=FALSE}\ndetrended_data <- data.frame(Date = df$DATE[-1], Detrended = diff(df$MSPUS))\n\n# Enhanced Detrended Plot\nggplot(data = detrended_data, aes(x = Date, y = Detrended)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Detrended Household Sale Price Time Series\",\n       x = \"Date\",\n       y = \"Detrended Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\n\n```{r,warning=FALSE, echo=FALSE}\nlog_transformed_data <- data.frame(Date = df$DATE, LogTransformed = log(df$MSPUS))\n\n# Enhanced Log-transformed Plot with Custom Background\nggplot(data = log_transformed_data, aes(x = Date, y = LogTransformed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Log-transformed Time Series of Household Sale Price\",\n       x = \"Date\",\n       y = \"Log-transformed Value\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\nFrom here, we can see that after detrending and dfferencing, the household sale price remains fluctuations espectially during 2020 period, which could be the cause of the Covid-19. In addition, Log-transformed time series, in order to remove the hetroscadastisity, we can see that the trend remains. I think the reason is possibly because log-transformation can assuage issues related to non-constant variance, which means that the inherent trend within the data may persist.\n\n### Test it stationary again\n```{r, echo=FALSE}\nlibrary(tseries)\nadf.test(log_transformed_data$LogTransformed)\n\n\n```\n\nAfter the detrending and transformating, we can see that it is still not statinary. Which means that further action still needs to be done. I utilize differencing methods.\n\n### Make it Stationary by using differencing\n\n\n```{r, echo = FALSE}\n\nt = ts(diff(df$MSPUS))\n# Create the time series plot with the desired background and borders\nts_plot <- autoplot(t) +\n  labs(title = \"Time Series Plot With first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(df$MSPUS)) +\n  labs(title = \"ACF for first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(df$MSPUS)) +\n  labs(title = \"PACF for first Orders of Differencing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)\n\n```\n\n\nAfter the first order of differencing, we can see the changes regarding the plots as a whole.\n\n\n```{r, warning=FALSE, echo = FALSE}\nadf.test(diff(df$MSPUS, differences = 1))\n\n\n```\n\nWe can see that, after differencing, the dataset becomes stationary. Now, we can fit into the model with different parameters.\n\n```{r, warning=FALSE, echo = FALSE}\n# Load required libraries\n\n# First, perform the augmented Dickey-Fuller test\nadf_result <- adf.test(diff(df$MSPUS, differences = 1))\n\n\n\n# Set the theme\ntheme_demo <- theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n# Plot the ACF and PACF of the differenced series with the demo theme\npar(mfrow=c(2,1)) # Set up a 2x1 grid for plotting\ndiff_series = diff(df$MSPUS, differences = 3)\n# ACF Plot\nacf_plot <- acf(diff_series, plot = FALSE)\nprint(acf_plot, main=\"ACF of Differenced Household Sale Price Time Series\")\nggAcf(diff_series) + theme_demo\n\n# PACF Plot\npacf_plot <- pacf(diff_series, plot = FALSE)\nprint(pacf_plot, main=\"PACF of Differenced Household Sale Price Time Series\")\nggPacf(diff_series) + theme_demo\n\n```\n\nFrom the PACF plot, it seems that after the 1st lag, the correlation values drop into insignificance. Thus, we might consider p = 1 for the AR component. From the ACF plot you provided, the correlation seems to cut off after the 1st lag as well. Hence, you might consider q = 1 for the MA component. And we use d = 1 since we utilized third orders of differencing.\n\n### Fit model\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$MSPUS, differences = 1)\nmodel <- Arima(diff, order=c(1,1,1))\nsummary(model)\n\n```\n\n### Equation of the Model\n\nGiven the specified values \\(p = 1\\), \\(q = 1\\), and \\(d = 1\\), we can write out the ARIMA(1,1,1) model equation using the general equation based on the results of the model:\n\n\\[ X_t = c + \\phi_1X_{t-1} + \\theta_1a_{t-1} + \\theta_2a_{t-2} + a_t \\]\n\nIn my case the third difference is represented as \\( \\nabla^1 X_t = X_t + 0.0092X_{t-1} - 0.9827Y_{t-1} \\). The ARIMA equation provided is built upon this differenced series.\n\n### Model diagnostics\n\n```{r,warning=FALSE, echo = FALSE}\n## empty list to store model fits\nset.seed((150))\narma14 =arima.sim(list(order=c(1,1,1), ar=-.1, ma=c(-.1)), n=10000)\narma14 %>% ggtsdisplay()\nARMA_res <- list()\n\n## set counter\ncc <-1\n\n## loop over AR\nfor(p in 0:3){\n  ## loop over MA\n  for(q in 0:4){\n    \n  ARMA_res[[cc]]<-arima(x=arma14,order = c(p,1,q))\n  cc<- cc+1\n  }\n}\n\n## get AIC values for model evaluation\n\nARMA_AIC<-sapply(ARMA_res,function(x) x$aic)\n\nARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]\n```\n\n```{r,warning=FALSE, echo = FALSE}\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 2:3)# p=1,2,3 : 3\n{\n  for(q in 2:4)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(df$MSPUS,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n```\n\n\nFrom the table, we can see that p=2, q=2 is the best, which are different from my assumption. The reason is that although there are spike and great change at lag 1. For lag 2, it also have correpondsing spike and change. In addition, since we used first order differencing, the value for d is not the same. By using higher d, the dataset can be more stationary for the model but has the potential of over differencing. Therefore, the new parameter sets are reasonable as well as the initial assumption.\n\n```{r, echo = FALSE}\ntemp[which.min(temp$AIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$BIC),]\n```\n```{r, echo = FALSE}\ntemp[which.min(temp$AICc),]\n```\n\nSince we only used 1 differencing, we can see that for all AIC, BIC, and AICc values, these parameter sets are all the best.\n\n### Model Compare\n\nNow, to be more specific, we compare the two different parameter sets: (2,2,3) and (1,1,1):\n```{r,warning=FALSE, echo = FALSE}\n######### compare 3 models\nset.seed(345)\nmodel_output <- capture.output(sarima(df$MSPUS, 2,2,3))\n```\n\n```{r, echo = FALSE}\nmodel_output2 <- capture.output(sarima(df$MSPUS, 1,1,1))\n```\n\n```{r,warning=FALSE, echo = FALSE}\n#################### fitted vs. actual plot  ##############\nfit1=Arima(df$MSPUS,order=c(2,2,3),include.drift = TRUE)\nfit2=Arima(df$MSPUS,order=c(1,1,1),include.drift = TRUE) #warning for the drift, no drift \n# Set background color\npar(bg = \"#E0E0E0\")\n\n# Plotting the data and model fits with custom background and border colors\nplot(df$MSPUS, col=\"blue\", main=\"Time Series with SARIMA Fits\", \n     ylab=\"Value\", xlab=\"Time\", \n     panel.first = rect(par(\"usr\")[1], par(\"usr\")[3], par(\"usr\")[2], par(\"usr\")[4], \n                        col = \"#E0E0E0\", border = \"#E0E0E0\"))\n\nlines(fitted(fit1), col=\"black\")\nlines(fitted(fit2), col=\"red\") \n\n# Adding a legend\nlegend(\"topleft\", \n       legend = c(\"The Time Series Of the Data\", \"fit1\", \"fit2\"), \n       col = c(\"blue\", \"black\", \"red\"), \n       lty=1, # Type of the line (1 is for \"solid\")\n       cex=0.8, # Font size of the legend text\n       box.col=\"#E0E0E0\", bg=\"#E0E0E0\") \n\n```\n\nHere, we can see that the two fits are actuaclly simiarly within our expecations. The difference is that we used first order differencing but the model used second order.\n\n### Forecasting for the dataset \n```{r, echo = FALSE}\nforecast(fit2,50) # Forecasting for the next 10 minutes\n```\n\n```{r, echo = FALSE}\n\n\n# Autoplot with custom colors\nplot_fit2 <- autoplot(forecast(fit2)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit2)\n```\n\nFrom the above graph, we can note that the forecasting captures the trending very well. This performance is within expectation. Now, we can determine whether the fit is actually better than the base models through benchmarking.\n\n### BENCHMARK\n\nNow for benchmarking, we compare the model with the base models such as meanf, naive model, and rwf model.\n\n```{r, echo = FALSE}\nlibrary(forecast)\ndiff = diff(df$MSPUS, differences = 1)\nmodel <- Arima(diff, order=c(2,2,3))\n\n```\n\n#### The meanf model with residual plot\n\n```{r, echo = FALSE}\nf1<-meanf(df$MSPUS, h=11) #mean\n\ncheckresiduals(f1)\n```\n\n#### The Arima model\n\n```{r, echo = FALSE}\ndf$MSPUS %>%\n  Arima(order=c(0,0,1), seasonal=c(0,1,1)) %>% \n  residuals() %>% \n  ggtsdisplay()\n```\n\n```{r, echo = FALSE}\nfit=Arima(df$MSPUS, order = c(0,1,1),seasonal = list(order=c(0,0,1), period=4) ) \nsummary(fit)\n```\n\n```{r, echo = FALSE}\nf1 <- meanf(df$MSPUS, h=10) \n\n#checkresiduals(f1)\n\nf2 <- naive(df$MSPUS, h=10) \n\n#checkresiduals(f2)\n\nf3 <- rwf(df$MSPUS,drift=TRUE, h=10) \n```\n\n#### Accuracy of the fitted models\n```{r, echo = FALSE}\npred=forecast(fit2,20)\naccuracy(pred)\npred[['mean']]\n```\n\n#### Accuracy of the based models\n\n```{r, echo = FALSE}\naccuracy(f1)\n```\n\n```{r, echo = FALSE}\naccuracy(f2)\n```\n\n```{r, echo = FALSE}\naccuracy(f3)\n```\n\nBased on the results, it is clear that the fitted model has the lowest error. our fitted model is performing better than these simple benchmark methods.\n\n\n```{r, echo = FALSE}\ngdp.ts=ts(df$MSPUS)\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp.ts, h=11),\n            series=\"fit\", PI=FALSE) +\n  autolayer(forecast(fit2, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  ggtitle(\"Forecasts for Household Sale Price\") +\n  xlab(\"Year\") + ylab(\"Household Sale Price\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\n```{r, echo = FALSE}\nautoplot(gdp.ts) +\n  autolayer(meanf(gdp.ts, h=20),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive(gdp.ts, h=20),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf(gdp.ts, drift=TRUE, h=40),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(fit2,20), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"Household Sale Price in US)\") +\n  xlab(\"Time\") + ylab(\"Household Sale Price\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme(\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    panel.background = element_rect(fill = \"#E0E0E0\"),\n    panel.border = element_rect(fill = NA, color = \"#E0E0E0\")\n  )\n```\n\nBased on the plot, we can see that Our fit is better than the benchmark methods by capturing more about the  trending pattern.\n\n\n\n# SARIMA Analysis　(Before Covid-19)\n\n## GDP Deflator SARIMA Analysis\n\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/A191RI1Q225SBEA.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n\n\n### Before Covid Period\n```{r}\ngdp_df <- ts(df$A191RI1Q225SBEA[1:295],frequency = 4)\ngdp_df\n```\n\n```{r}\n\n\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = A191RI1Q225SBEA)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of GDP Deflator\",\n       x = \"Date\", \n       y = \"GDP Deflator\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n   \n```\n\n### Check Decomposition\n\n```{r}\n# Decompose the time series data\ndec <- decompose(gdp_df, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n```\n\n### Check Lag Plot\n```{r}\ngglagplot(gdp_df, do.lines=FALSE, set.lags = c(4, 8, 12, 16))\n```\n\n### Seasonal Difference AND ACF & PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 4 relatively.\n```{r}\nts_plot <- autoplot(gdp_df) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(gdp_df, lag=4), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n```\n\n\nMost spikes are within range, it is stationary. \nACF Plot :\n\nThe sharp drop after lag 1 and some lags across the range. This gives us q = 1,2.\nSince there's a noticeable autocorrelation at lag 4, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 4, which implies Q = 1.\nPACF Plot:\n\nThe sharp drop after lag 1 in the PACF plot indicates a possible AR(1) process. This gives us p = 1.\nThe seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1.\nOrder of Differencing:\n\nYou mentioned that you applied first order differencing, so d = 1.\nYou also mentioned seasonal differencing with a lag of 4, so D = 1.\nCombining these, we get:\n\nNon-seasonal parameters: p = 1, d = 1, q = 1,2\nSeasonal parameters: P = 1, D = 1, Q = 1, and the seasonal period (or frequency) is 4.\nTherefore, the ARIMA model can be represented as ARIMA(1,1,1)(1,1,1)[4].\n\nWE can continue analysis\n\n\n\n\n### Model Diagnostics\n\n```{r}\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n  \n\n```\n\n\n```{r}\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=gdp_df)\nknitr::kable(output)\n\n```\n\n### Compare the results\n```{r}\noutput[which.min(output$AIC),] \n```\n\n```{r}\noutput[which.min(output$BIC),]\n```\n\n```{r}\noutput[which.min(output$AICc),]\n```\n\n\n```{r}\n\nset.seed(236)\nmodel_output1 <- capture.output(sarima(gdp_df, 1,1,1,0,1,1,4))\nmodel_output2 <- capture.output(sarima(gdp_df, 0,1,1,0,1,1,4))\n```\nThe second one is a little better.\n\n```{r}\ncat(model_output1[50:80], model_output1[length(model_output1)], sep = \"\\n\") \n```\n\n```{r}\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n```\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\n\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\n\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\n\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n\n$ttable: all coefficients are significant.\n\n\n\n### Fit model & Forecasting\n\n```{r}\nfit2=arima(gdp_df, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\nsummary(fit2)\n\n```\n\n\n```{r}\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,120)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n```\n\n\n\n```{r}\nsarima.for(gdp_df, 36, 0,1,1,0,1,1,4)\n```\n\n### BenchMark Comparsion\n```{r}\nautoplot(gdp_df) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\",PI=FALSE) +\n  autolayer(meanf(gdp_df, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_df, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_df, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_df, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n```\n\n```{r}\nf2 <- snaive(gdp_df, h=36) \n\naccuracy(f2)\n```\n\n```{r}\nsummary(fit2)\n```\n\nOur model fitting is much better than benchmark methods\n\n### Cross validation\n\n#### One Step ahead\n```{r}\nn <- length(gdp_df)\nn \n```\n\n```{r}\nk <- 89 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n```\n\n```{r,warning=FALSE}\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_df[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_df[k+i] #76th observation as the test set\n\n  fit <- arima(xtrain, order = c(1,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n```\n\n```{r}\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n```\n\n```{r}\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n```\nWe can see that the corresponding results for model 2: (0,1,1)(0,1,1) is slightly better.\n\n\n#### 4 step ahead in my case\n```{r}\n\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=4) ),h=h)}\n\n# Compute cross-validated errors for up to 4 steps ahead\ne <- tsCV(gdp_df, forecastfunction = farima1, h = 4)\n \nlength(e) \n```\n\n```{r,warning=FALSE}\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:4, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n```\n\nFrom here we can see that the one step ahead has lower MSE, which is better than four step ahead in my case.\n\n## Real Median Household Income in the United States SARIMA Analsyis\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf <- read.csv(\"../Dataset/project/MEHOINUSA672N.csv\")\ndf$DATE <- as.Date(df$DATE)\nhead(df)\n```\n\n\n### Before Covid Period\n```{r}\nrmhi <- ts(df$MEHOINUSA672N,frequency = 2)\nrmhi\n```\n\n```{r}\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x = DATE, y = MEHOINUSA672N)) +\n  geom_line(color = \"DarkViolet\") +\n  labs(title = \"Time Series Plot of Household Income\",\n       x = \"Date\", \n       y = \"Household Income\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#E0E0E0\"),\n        panel.grid.major = element_line(color = \"grey\", size = 0.1),\n        panel.grid.minor = element_line(color = \"grey\", size = 0.05),\n        plot.background = element_rect(fill = \"#E0E0E0\"))\n\n```\n\n### Check with decomposition\n\n```{r}\n# Decompose the time series data\ndec <- decompose(rmhi, type = \"multiplicative\")  # Choose either \"additive\" or \"multiplicative\"\n\n# Set the graphical parameters for the plot\npar(bg = \"#E0E0E0\", col.axis = \"#E0E0E0\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n# Plot the decomposed object\nplot(dec)\n\n# Reset the graphical parameters to default\npar(bg = \"white\", col.axis = \"black\", col.lab = \"black\", col.main = \"black\", col.sub = \"black\")\n\n```\n\n### Check Seasonal Pattern using Lag\n```{r}\ngglagplot(rmhi, do.lines=FALSE, set.lags = c(2, 4, 8, 12))\n```\n\n### Seasonal Difference and ACF, PACF\nThis shows seasonality. Also the lag plot shows higher correlation at Seasonal lag 2 relatively.\n```{r}\nts_plot <- autoplot(rmhi) +\n  labs(title = \"Time Series Plot \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(diff(rmhi, lag=2), differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(ts_plot, acf_plot, pacf_plot, ncol = 1)             \n\n```\n\n\nMost spikes are within range, it is stationary. \nACF Plot (Autocorrelation Function Plot):\n\nThe sharp drop after lag 2. This gives us q = 1,2.\nSince there's a noticeable autocorrelation at lag 2,5, this suggests a seasonal component. The seasonal component in the ACF plot shows a sharp decline after lag 1, which implies Q = 1,2.\nPACF Plot (Partial Autocorrelation Function Plot):\n\nThe sharp drop after lag 2 in the PACF plot indicates a possible AR(1) process. This gives us p = 1,2.\nThe seasonal component in the PACF plot also has a significant spike at lag 4, which implies P = 1. However, the dataset did not have a strong seasonal pattern. We can also consider P to be 0\nOrder of Differencing:\n\nWe applied first order differencing, so d = 1.\nYou also mentioned seasonal differencing with a lag of 4, so D = 1.\nCombining these, we get:\n\nNon-seasonal parameters: p = 1,2, d = 1, q = 1,2\nSeasonal parameters: P = 0,1 D = 1, Q = 1, and the seasonal period (or frequency) is 2.\nTherefore, the ARIMA model can be represented as ARIMA(2,1,1)(0,1,1)[2].\n\nWE can continue analysis\n\n\n\n\n### Model Diagnostic\n\n```{r}\n######################## Check for different combinations ########\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  temp=c()\n  d=1\n  D=1\n  s=2\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*23),nrow=23)\n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\n\n  \n\n```\n\n\n### Compare Results\n\n```{r}\n# Based on the analysis:\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=rmhi)\nknitr::kable(output)\n\n```\n\n\n```{r}\noutput[which.min(output$AIC),] \n```\n\n```{r}\noutput[which.min(output$BIC),]\n```\n\n```{r}\noutput[which.min(output$AICc),]\n```\n\n\n```{r}\n\nset.seed(236)\nmodel_output1 <- capture.output(sarima(rmhi, 2,1,1,0,1,1,2))\nmodel_output2 <- capture.output(sarima(rmhi, 0,1,1,0,1,1,2))\n```\nThe second one is a little better.\n\n```{r}\ncat(model_output1[50:67], model_output1[length(model_output1)], sep = \"\\n\") \n```\n\n```{r}\ncat(model_output2[40:55], model_output2[length(model_output2)], sep = \"\\n\") \n```\n\nThe Standard Residual Plot appears good, displaying okay stationarity with a nearly constant mean and variation.\n\nThe Autocorrelation Function (ACF) plot shows almost no correlation indicating that the model has harnessed everything that left is white noise. This indicates a good model fit.\n\nThe Quantile-Quantile (Q-Q) Plot still demonstrates near-normality.\n\nThe Ljung-Box test results reveal values below the 0.05 (5% significance) threshold, indicating there’s some significant correlation left.\n\n$ttable: all coefficients are significant.\nThe second one is better indeed.\n\n\n### Fit model & Forecast\n\n```{r}\nfit2=arima(rmhi, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\nsummary(fit2)\n\n```\n\n\n```{r}\n# Autoplot with custom colors\nplot_fit_ <- autoplot(forecast(fit2,36)) + \n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    legend.key = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Display the plot\nprint(plot_fit_)\n\n```\n\n### BenchMark Comparsion\n\n```{r}\nsarima.for(rmhi, 36, 0,1,1,0,1,1,2)\n```\n\n\n```{r}\nautoplot(rmhi) +\n  autolayer(forecast(fit2,36), \n            series=\"fit\") +\n  autolayer(meanf(rmhi, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(rmhi, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(rmhi, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(rmhi, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  \n  guides(colour=guide_legend(title=\"Forecast\"))\n```\n\n```{r}\nf1 <- meanf(rmhi, h=36)\nf2 <- snaive(rmhi, h=36) \nf3 <-naive(rmhi, h=36)\naccuracy(f1)\naccuracy(f2)\naccuracy(f3)\n```\n\n```{r}\nsummary(fit2)\n```\n\nOur model fitting is better than benchmark methods with smaller RMSE\n\n### Cross validation\n\n#### One Step ahead\n```{r}\nn <- length(rmhi)\nn * 0.3\n```\n\n```{r}\nk <- 12 # Use enough number of data for model: 30% of my whole dataset\n\nn-k # rest of the observations\n```\n\n```{r,warning=FALSE}\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- rmhi[1:(k-1)+i] \n  xtest <- rmhi[k+i] \n  fit <- arima(xtrain, order = c(2,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- arima(xtrain, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) )\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n\n```\n\n```{r}\nMAE1=mean(err1) \nMAE2=mean(err2)\nMSE1=mean(err3)\nMSE2=mean(err4)\n```\n\n```{r}\n# Create a dataframe\nerror_metrics <- data.frame(\n  MAE1 = MAE1,\n  MAE2 = MAE2,\n  MSE1 = MSE1,\n  MSE2 = MSE2\n)\n\n# View the dataframe\nprint(error_metrics)\n```\nWe can see that the corresponding results for model 2: (2,1,1)(0,1,1) is slightly better. Which is not the same as the conclusion from above, the reason could be that the data points are relatively small. The train and test split can not capture the datasets effectively, which can cause the different conclusion. The model diagnotics from previsou section indicates that the (0,1,1)(0,1,1) is better with smaller errors.\n\n\n#### 2 step ahead in my case\n```{r}\n\nfarima1 <- function(x, h){forecast(arima(x, order = c(0,1,1),seasonal = list(order=c(0,1,1), period=2) ),h=h)}\n\n# Compute cross-validated errors for up to 2 steps ahead\ne <- tsCV(rmhi, forecastfunction = farima1, h = 2)\n \nlength(e) \n```\n\n```{r,warning=FALSE}\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:2, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()\n```\n\nFrom here we can see that the one step ahead has lower MSE, which is better than two step ahead in my case.\n\n\n\n\n\n\n\n# References and Codes\n- Codes: [Rmd, Python & Qmd](https://github.com/zy236yuz5/ANLY5600-TimeSeries)\n- U.S. Bureau of Economic Analysis, Household saving [W398RC1A027NBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/W398RC1A027NBEA, September 19, 2023.\n- U.S. Census Bureau, Real Median Household Income in the United States [MEHOINUSA672N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MEHOINUSA672N, September 19, 2023.\n- U.S. Bureau of Economic Analysis, Gross Domestic Product: Implicit Price Deflator [A191RI1Q225SBEA], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/A191RI1Q225SBEA, September 18, 2023.\n- Zillow Group. Accessed April 19, 2023. “Zillow Research Data.” https://www.zillow.com/research/data/."},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/layout.css","./styles/layout.css"],"toc":true,"output-file":"ARModels.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","theme":"sandstone","title":"ARMA/ARIMA/SARIMA Models","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]}},"extensions":{"book":{"multiFile":true}}}}}