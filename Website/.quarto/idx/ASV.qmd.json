{"title":"ARIMAX/SARIMAX/VAR","markdown":{"yaml":{"title":"ARIMAX/SARIMAX/VAR","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","SAF.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]},"format":{"html":{"theme":"sandstone","css":"./styles/layout.css","code-fold":true,"toc":true}}},"headingText":"Literatue Review to choose variables and models","containsRefs":false,"markdown":"\n\nDiscription:\n\n\n\n\n\n\n\n\n```{r, echo = FALSE, message = FALSE, warning = FALSE}\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(vars)\nlibrary(tseries)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(gridExtra)\nlibrary(plotly)\nlibrary(TTR) # For the SMA function\n\n```\n\n\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\ndf1 <- read.csv(\"../Dataset/project/household_saving.csv\")\ndf2 <- read.csv(\"../Dataset/project/MEHOINUSA672N.csv\")\ndf3 <- read.csv(\"../Dataset/project/MSPUS.csv\")\ndf4 <- read.csv(\"../Dataset/project/SIPOVGINIUSA.csv\")\ndf5 <- read.csv(\"../Dataset/project/FIXHAI.csv\")\ndf6 <- read.csv(\"../Dataset/project/PSAVERT.csv\")\ndf7 <- read.csv(\"../Dataset/project/A191RI1Q225SBEA.csv\")\ndf8 <- read.csv(\"../Dataset/project/Merged_downsample.csv\")\n\ndf1$DATE <- as.Date(df1$DATE)\ndf2$DATE <- as.Date(df2$DATE)\ndf3$DATE <- as.Date(df3$DATE)\ndf4$DATE <- as.Date(df4$DATE)\ndf5$DATE <- as.Date(df5$DATE)\ndf6$DATE <- as.Date(df6$DATE)\ndf7$DATE <- as.Date(df7$DATE)\ndf8$date <- as.Date(df8$date)\nhead(df8)\n```\n\n\n```{r}\nsaving =ts(df1$W398RC1A027NBEA)\nincome =ts(df2$MEHOINUSA672N)\ngini =ts(df4$SIPOVGINIUSA)\nafford =ts(df5$FIXHAI)\nsaverate =ts(df6$PSAVERT)\ngdp =ts(df7$A191RI1Q225SBEA)\n\nsaleprice =ts(df8$Mean.Sale.Price)\nhomevalue =ts(df8$Mean.Home.Value)\nrentalprice =ts(df8$mean)\n```\n\n# 1. (ARIMAX) Home value ~ Sale price + Rental price\n\n## Set Up The Variables\n```{r}\ndd1<-data.frame(homevalue,saleprice,rentalprice,df8$date)\n\ncolnames(dd1)<-c(\"homevalue\",\"saleprice\",\"rentalprice\",'date')\n\nknitr::kable(head(dd1))\n```\n\n```{r,warning=FALSE}\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"homevalue\"=log(dd1$homevalue),\"saleprice\"=log(dd1$saleprice),\n                                        \"rentalprice\"=log(dd1$rentalprice))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")\n```\n\n\n## Auto Fit The Model\n\n```{r}\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"homevalue\"], xreg = xreg1)\nsummary(fit1)\n```\n\n\n## Manual Fit The Model\n```{r}\nfit.reg1 <- lm( homevalue ~ saleprice+ rentalprice, data=lg.dd.ts1)\nsummary(fit.reg1)\n```\n\n\nWe can see that the variables are pretty significant.\n\n\n## Check The Residuals\n```{r}\ncheckresiduals(fit1)\n```\n\n\nBased on the output, there's no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model.\n\n## Check ACF and PACF\n```{r}\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n```\n\n```{r}\nggPacf(res.fit1)\n```\n\nSince there is no major seasonal pattern. We use differencing directly\n\n## Differencing ACF and PACF\n```{r}\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(diff(res.fit1, differences = 1)) +\n  labs(title = \"ACF for first Order of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(diff(res.fit1, differences = 1)) +\n  labs(title = \"PACF for first Orders of Differencing and Seasonal Differenceing\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n```\n\nNow, it is mostly stationary for us to continue\nFrom ACF and PACF, it seems that we can consider: p = 1,2,3 q = 1,2,3. We use first order of differencing so d = 1.\nThere is no major seasonal pattern, therefore, no P,Q,D in this case.\n\n## Model Diagnotistics\nFinding the model parameters.\n```{r,warning=FALSE}\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n```\n\n```{r}\ntemp[which.min(temp$AIC),]\n```\n\n```{r}\ntemp[which.min(temp$BIC),]\n```\n\n```{r}\ntemp[which.min(temp$AICc),]\n```\n\nFrom here, we have two potential ones: (2,1,2) and (2,1,4)\n\n## Compare The models\n\n```{r}\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 2,1,4)) \nmodel_output12 <- capture.output(sarima(res.fit1, 2,1,2)) \n```\n```{r}\ncat(model_output11[90:123], model_output11[length(model_output11)], sep = \"\\n\")\ncat(model_output12[40:70], model_output12[length(model_output12)], sep = \"\\n\")\n```\n\nBased on this, I think the second one (2,1,4) is slightly better with less correlation and smaller AIC value.\n\n\n## Cross validation\n```{r}\nn=length(res.fit1)\nn *0.3 \n```\n\n```{r}\nk=324\n \nrmse1 <- matrix(NA, 53,4)\nrmse2 <- matrix(NA,53,4)\nrmse3 <- matrix(NA,53,4)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:53)\n{\n  xtrain <- window(res.fit1, end=st + i-1)\n  xtest <- window(res.fit1, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,4),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:4, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n```\n\n```{r}\ncolMeans( rmse1,na.rm=TRUE)\ncolMeans( rmse2,na.rm=TRUE)\n```\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (2,1,4) performs better with smaller errors.\n\n\n## Fit the model \n\n```{r}\nxreg1 <- cbind(saleprice = lg.dd.ts1[, \"saleprice\"],\n              rentalprice = lg.dd.ts1[, \"rentalprice\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"homevalue\"],order=c(2,1,4),xreg=xreg1)\nsummary(fit1)\n```\n\n\n## Write Down The equation:\nGiven that `y_t` represents the log-transformed `homevalue` at time `t`, the ARIMA(2,1,4) can be:\n\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\n\n\nBased on the summary, my equation is\n\n(1 - 0.8151B - 0.0766B²)(1 - B)yₜ = α + 0.8651 * salepriceₜ + 0.1058 * rentalpriceₜ + (1 - 1.2454B + 0.1814B² - 0.1233B³ + 0.1903B⁴)εₜ\n\n## Forecasting\n```{r}\nspfit<-auto.arima(lg.dd.ts1[, \"saleprice\"]) \nsummary(spfit) \n```\n\n```{r}\nfsp<-forecast(spfit,80) #obtaining forecasts\n\nrpfit<-auto.arima(lg.dd.ts1[, \"rentalprice\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n```\n\n```{r}\nfrp<-forecast(rpfit,80)\n\nfxreg <- cbind(saleprice = fsp$mean, \n              rentalprice = frp$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")\n```\n\n\n\n\n\n# 2. (ARIMAX) Saving Rate ~ Household Income + Household Saving\n\n## Creating Ts Objects For Variables\n```{r}\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nrate <- ts(df6_sub$PSAVERT, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\n\n```\n\n\nsaving rate, income, saveing can be interesting to be evaluated\n\n\n\n## Combine the Varaibles\n```{r}\ndd2<-data.frame(rate,saving,income,df1_sub$DATE)\n\ncolnames(dd2)<-c(\"saving_rate\",\"household_saving\",\"household_income\",'date')\n\nknitr::kable(head(dd2))\n```\n\n## Make a Log transformation For Seasonal Pattern\n```{r,warning=FALSE}\nlg.dd2 <- data.frame(\"date\" =dd2$date,\"saving_rate\"=log(dd2$saving_rate),\"household_saving\"=log(dd2$household_saving),\n                                        \"household_income\"=log(dd2$household_income))\n\n#### converting to time series component #########\nlg.dd.ts2<-ts(lg.dd2,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts2[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")\n```\n\n\n## Auto Fit The Model\n\n```{r}\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\nfit2 <- auto.arima(lg.dd.ts2[, \"saving_rate\"], xreg = xreg2)\nsummary(fit2)\n```\n\n\n## Manual Fit The model\n```{r}\nfit.reg2 <- lm( saving_rate ~  household_saving + household_income, data=lg.dd.ts2)\nsummary(fit.reg2)\n```\n\nIt seems that the household income did not play any important role. Therefore, We can consider to remove it.\n\n## Manual Fit Again Without Insignificant Variable\n```{r}\nfit.reg2 <- lm( saving_rate ~  household_saving, data=lg.dd.ts2)\nsummary(fit.reg2)\n```\n\nWe can see that the variables are pretty significant.\n\n## Check residuals\n```{r}\ncheckresiduals(fit2)\n```\n\nBased on the output, there's no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model.\n\n## ACF and PACF Check\n```{r}\n########### Converting to Time Series component #######\n\nres.fit2<-ts(residuals(fit.reg2),frequency = 2)\n\n############## Then look at the residuals ############\nggAcf(res.fit2)\n\n```\n\n```{r}\nggPacf(res.fit2)\n```\n\nSince there is no major seasonal pattern. And it seems that the data is already stationary\n\n## No Need For Seasonal Differencing\n```{r}\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit2) +\n  labs(title = \"ACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit2) +\n  labs(title = \"PACF for data\") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n```\n\nNow, it is mostly stationary for us to continue\nFrom ACF and PACF, it seems that we can consider: p = 1,2 q = 1,2. We use first order of differencing so d = 0.\nThere is no major seasonal pattern, therefore, no P,Q,D in this case.\n\n## Model Diagnotistics\nFinding the model parameters.\n```{r,warning=FALSE}\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 3:5)# p=1,2,3 : 3\n{\n  for(q in 3:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit2,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n```\n\n```{r}\ntemp[which.min(temp$AIC),]\n```\n\n```{r}\ntemp[which.min(temp$BIC),]\n```\n\n```{r}\ntemp[which.min(temp$AICc),]\n```\n\nFrom here, we have two potential ones: (2,1,2) and (0,0,0) from part 1\n\n## Compare The Models\n\n```{r}\nset.seed(236)\n\nmodel_output21 <- capture.output(sarima(res.fit2, 2,1,2)) \nmodel_output22 <- capture.output(sarima(res.fit2, 0,0,0)) \n```\n```{r}\ncat(model_output21[145:160], model_output21[length(model_output21)], sep = \"\\n\")\n```\n\n```{r}\n\ncat(model_output22[20:38], model_output22[length(model_output22)], sep = \"\\n\")\n```\n\nBased on this, I think the first one (2,1,2) is slightly better with less correlation and closer to the significant level.\n\n\n## Cross validation\n```{r}\nn=length(res.fit2)\nn *0.3 \n```\n\n```{r,warning=FALSE}\nk=9\n \nrmse1 <- matrix(NA, 40,2)\nrmse2 <- matrix(NA,40,2)\nrmse3 <- matrix(NA,40,2)\n\nst <- tsp(res.fit1)[1]+(k-1)/4 \n\nfor(i in 1:12)\n{\n  xtrain <- window(res.fit2, end=st + i-1)\n  xtest <- window(res.fit2, start=st + (i-1) + 1/4, end=st + i)\n  \n\n  \n  fit <- Arima(xtrain, order=c(2,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n  \n  \n\n  rmse1[i,1:length(xtest)]  <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n\n  \n}\n\nplot(1:2, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:2, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\n\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n```\nHere, although\n\n```{r}\ncolMeans( rmse1,na.rm=TRUE)\ncolMeans( rmse2,na.rm=TRUE)\n```\nBased on the cross validation, overall, the two method is simiarly within expectation. We can see that although model (0,0,0) has smaller errors after, the dataset is relatively biased due to the size of the date overlapping. We should still use the fit1 which is (2,1,2) to perform later analysis.\n\n\n## Fit the model With The Best One\n\n```{r}\nxreg2 <- cbind(household_income = lg.dd.ts2[, \"household_income\"],\n              household_saving = lg.dd.ts2[, \"household_saving\"])\n\n\nfit2 <- Arima(lg.dd.ts2[, \"saving_rate\"],order=c(2,1,2),xreg=xreg2)\nsummary(fit2)\n```\n\n## Write Doen The equation:\nGiven that `y_t` represents the log-transformed `homevalue` at time `t`, the ARIMA(2,1,4) can be:\n\n(1 - φ₁B - φ₂B²)(1 - B)yₜ = α + β₁ * salepriceₜ + β₂ * rentalpriceₜ + (1 + θ₁B + θ₂B² + θ₃B³ + θ₄B⁴)εₜ\n\n\nBased on the summary, my equation for this model is\n\n(1 - 0.4885B - (-0.4602)B^2)(1 - B)y_t = \\alpha + 0.1188 \\cdot \\text{household_income}_t - 0.1330 \\cdot \\text{household_saving}_t + (1 - 1.2187B + 0.5136B^2)\\varepsilon_t\n\n\n## Forecasting\n```{r}\nhsfit<-auto.arima(lg.dd.ts2[, \"household_saving\"]) \nsummary(hsfit) \n```\n\n```{r}\nfhs<-forecast(hsfit,12) #obtaining forecasts\n\nhifit<-auto.arima(lg.dd.ts2[, \"household_income\"]) #fitting an ARIMA model to the Import variable\nsummary(rpfit)\n```\n\n```{r}\nfhi<-forecast(hifit,12)\n\nfxreg <- cbind(household_income = fhi$mean,household_saving = fhs$mean\n              ) #fimp$mean gives the forecasted values\n\nfcast <- forecast(fit2, xreg=fxreg,12) \nautoplot(fcast, main=\"Forecast of Saving Rate\") + xlab(\"Year\") +\n  ylab(\"Saving Rate\")\n```\n\n\n\n\n\n# 3. (VAR) Sale Price & Saving Rate & Gdp Deflator\n\n## Transform all to time series\n```{r}\nsaving =ts(df1$W398RC1A027NBEA)\nincome =ts(df2$MEHOINUSA672N)\nsale = ts(df3$MSPUS)\ngini =ts(df4$SIPOVGINIUSA)\nafford =ts(df5$FIXHAI)\nsaverate =ts(df6$PSAVERT)\ngdp =ts(df7$A191RI1Q225SBEA)\n\nsaleprice =ts(df8$Mean.Sale.Price)\nhomevalue =ts(df8$Mean.Home.Value)\nrentalprice =ts(df8$mean)\n```\n\n\n## Prepare Variables: saving rate, sale price, gdp deflator\n```{r}\n# Define start and end dates\nstart_date <- as.Date(\"1965-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 5\n\n# Create ts objects\nsale_price <- ts(df3_sub$MSPUS, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nsaving_rate <- ts(df6_sub$PSAVERT, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(1965, 1), end = c(2020, 1), frequency = frequency)\n```\n\n\n\n## Combine Variables\n```{r}\ndd3<-data.frame(sale_price,saving_rate,gdp,DATE)\n\ncolnames(dd3)<-c(\"sale_price\",\"saving_rate\",\"gdp_deflator\",'date')\n\nknitr::kable(head(dd3))\n```\n\n## Plot the Variables Together\n```{r,warning=FALSE}\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale_price\"=log(dd3$sale_price),\"saving_rate\"=log(dd3$saving_rate),\n                                        \"gdp_deflator\"=log(dd3$gdp_deflator))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 5)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate in USA\")\n```\n\n\n\n## Finding out the best p:\n```{r}\nVARselect(dd3[, c(2:4)], lag.max=10, type=\"both\")\n```\n\nFrom the results, we can see that 3,4 have relatively smaller criterias:\nWe can fit several models with p=1, 3, and 4.=> VAR(1), VAR(3), VAR(4)\n\n## Fitting a VAR model with different p:\n```{r}\nsummary(vars::VAR(dd3[, c(2:4)], p=1, type='both'))\n```\n```{r}\nsummary(vars::VAR(dd3[, c(2:4)], p=3, type='both'))\n```\n\n```{r}\nsummary(vars::VAR(dd3[, c(2:4)], p=4, type='both'))\n```\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one.\n\n\n## Cross Validations\n\n### Define length\n\n\n```{r}\nn=length(dd3$gdp_deflator)\nk=85 #19*4\n\nn*0.3\n```\n\n\n```{r}\ndat = ts(dd3[,c(1,2,3)])\n```\n\n### Cross Validation For gdp deflator\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n```\n\n#### Normalize the RMSE For Ploting\n\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### Compare the errors\n\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\nThe first one is better but not too different for GDP Deflator\n\n### Cross Validation For Saving rate\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$saving_rate-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$saving_rate-xtest)^2))\n\n}\n\n\n\n```\n\n#### Normalize the errors for ploting\n\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### Compare errors\nLet us reorganize the errors\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\nThe first one is also better for saving rate\n\n\n### Cross Validation For Sale price\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=3, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=4, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$sale_price-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$sale_price-xtest)^2))\n\n}\n\n\n\n```\n\n#### Normalize The Errors \n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\ne <- 0.03\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 + e, type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### Compare The Errors\n\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\nOverall, we can see that in my case, all variables have smaller RMSE with VAR(3)\n\n## Forecasting\n\nForecast using the best one which is VAR(3)\n\n```{r}\nfit1 <- vars::VAR(dat[,1:3], p=3, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n```\n\n### Check Corresponind Forecasting\n```{r}\nfcast1$fcst$sale_price\n```\n\n\n```{r}\nfcast1$fcst$saving_rate\n```\n\n\n## Forecast Results\n```{r}\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")\n```\n\n\n\n# 4. (ARIMAX) GINI Index ~ Household Income + Household Saving + Sale Price\n\n## Prepare the Variables\n```{r}\n# Define start and end dates\nstart_date <- as.Date(\"1992-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data\ndf1_sub <- subset(df1, DATE >= start_date & DATE <= end_date)\ndf4_sub <- subset(df4, DATE >= start_date & DATE <= end_date)\ndf2_sub <- subset(df2, DATE >= start_date & DATE <= end_date)\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 1\n\n# Create ts objects\nsaving <- ts(df1_sub$W398RC1A027NBEA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nincome <- ts(df2_sub$MEHOINUSA672N, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\nsale <- ts(df3_sub$MSPUS, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\ngini <- ts(df4_sub$SIPOVGINIUSA, start = c(1992, 1), end = c(2020, 1), frequency = frequency)\n\n```\n\n\n## Combine the Variables\n```{r}\ndd1<-data.frame(gini,saving,income,sale,df1_sub$DATE)\n\ncolnames(dd1)<-c(\"gini\",\"saving\",\"income\",'sale','date')\ndd1$gini <- ts(as.numeric(dd1$gini))\nknitr::kable(head(dd1))\n```\n\n\n```{r,warning=FALSE}\nlg.dd1 <- data.frame(\"date\" =dd1$date,\"gini\"=log(dd1$gini),\"saving\"=log(dd1$saving),\n                                        \"income\"=log(dd1$income),\"sale\"=log(dd1$sale))\n\n#### converting to time series component #########\nlg.dd.ts1<-ts(lg.dd1,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts1[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The Household Prices in USA\")\n```\n\n\n## Auto Fit the Model\n\n```{r}\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              income = lg.dd.ts1[, \"income\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\nfit1 <- auto.arima(lg.dd.ts1[, \"gini\"], xreg = xreg1)\nsummary(fit1)\n```\n\n\n## Manual Fit The Model\n```{r}\nfit.reg1 <- lm( gini ~ saving+ income + sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n```\n\nWe can see that the sale and saving are pretty significant. For income, it seems that it does not provide too much impact.\nWe can consider to remove it\n\n## Manual Fit Again\n```{r}\nfit.reg1 <- lm( gini ~ saving+ sale, data=lg.dd.ts1)\nsummary(fit.reg1)\n```\n\n## Check Residuals\n```{r}\ncheckresiduals(fit1)\n```\nBased on the output, there's no indication that seasonal terms are being used and the datasets did not have too much seasonal pattern. Therefore, this is an ARIMAX model.\n\n## Check ACF and PACF\n\n```{r}\n########### Converting to Time Series component #######\n\nres.fit1<-ts(residuals(fit.reg1),frequency = 4)\n\n############## Then look at the residuals ############\nggAcf(res.fit1)\n\n```\n\n```{r}\nggPacf(res.fit1)\n```\n\nSince there is no major seasonal pattern. And the dataset is stationary, We can actually use it as it is\n\n## NO Need To Differencing\n```{r}\n\n# Extract the ACF and PACF plots without the theme\nacf_plot <- ggAcf(res.fit1) +\n  labs(title = \"ACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\npacf_plot <- ggPacf(res.fit1) +\n  labs(title = \"PACF \") +\n  theme(\n    panel.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\"),\n    plot.background = element_rect(fill = \"#E0E0E0\", color = \"#E0E0E0\")\n  )\n\n# Combine the plots\ngrid.arrange(acf_plot, pacf_plot, ncol = 1)             \n\n```\nNow, it is  stationary for us to continue\n\nFrom ACF and PACF, although there are not too many spikes and seasonal patterns, it seems that we can still consider: p = 1,2 q = 1,2. We use d = 0.\nThere is no major seasonal pattern, therefore, no P,Q,D in this case.\n\n## Model Diagnotistics\nFinding the model parameters.\n```{r,warning=FALSE}\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*70),nrow=70) \n\n\nfor (p in 1:5)# p=1,2,3 : 3\n{\n  for(q in 1:5)# q=1,2,3,4 :4\n  {\n    for(d in 1:3)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(res.fit1,order=c(p-1,d-1,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n```\n\n```{r}\ntemp[which.min(temp$AIC),]\n```\n\n```{r}\ntemp[which.min(temp$BIC),]\n```\n\n```{r}\ntemp[which.min(temp$AICc),]\n```\n\nFrom here, we can see that (4,0,2) and (0,0,2) is better. Therefore, we will compare them\n\n## Compare with the two models\n\n```{r}\nset.seed(236)\n\nmodel_output11 <- capture.output(sarima(res.fit1, 4,0,2)) \nmodel_output12 <- capture.output(sarima(res.fit1, 0,0,2)) \n```\n```{r}\ncat(model_output11[150:173], model_output11[length(model_output11)], sep = \"\\n\")\ncat(model_output12[40:67], model_output12[length(model_output12)], sep = \"\\n\")\n```\n\nBased on this, I think the second one (0,0,2) is slightly better with less correlation and smaller AIC value.\n\n\n## Cross validation\n```{r}\nn=length(res.fit1)\nn *0.3 \ndat = ts(dd1[,c(1,2,4)])\n\n```\n\n\n```{r}\n# The number of folds for cross-validation\nn_folds <- 5\nhorizon <- 4  # Forecasting horizon\nwindow_size <- length(dat[, 3]) - (n_folds * horizon)\n\n# Initialize an empty list to store forecasts\nforecasts <- list()\nforecasts2 <- list()\n\n# Initialize vectors to store error metrics for each model\nrmse1 <- numeric(n_folds)\nrmse2 <- numeric(n_folds)\n\nfor (i in 1:n_folds) {\n  # Define the training set for this fold\n  train_set <- window(dat[, 3], end = c(window_size + ((i - 1) * horizon)))\n\n  # Fit the ARIMA models on the training set\n  fit <- Arima(train_set, order = c(4, 0, 2), include.drift = TRUE, method = \"ML\")\n  fit2 <- Arima(train_set, order = c(0, 0, 2), include.drift = TRUE, method = \"ML\")\n\n  # Forecast on the horizon\n  fcast <- forecast(fit, h = horizon)\n  fcast2 <- forecast(fit2, h = horizon)\n\n  # Store forecasts\n  forecasts[[i]] <- fcast\n  forecasts2[[i]] <- fcast2\n\n  # Define the test set for this fold\n  test_set <- window(dat[, 3], start = window_size + ((i - 1) * horizon) + 1, end = window_size + (i * horizon))\n\n  # Calculate and store the RMSE for each model\n  rmse1[i] <- sqrt(mean((fcast$mean - test_set)^2, na.rm = TRUE))\n  rmse2[i] <- sqrt(mean((fcast2$mean - test_set)^2, na.rm = TRUE))\n}\n\n# Calculate the average RMSE for each model\nmean_rmse1 <- mean(rmse1)\nmean_rmse2 <- mean(rmse2)\n\n\n\n# Plot RMSE values for both models\nplot(rmse1, type = \"b\", col = \"blue\", ylim = range(c(rmse1, rmse2)), \n     xlab = \"Fold\", ylab = \"RMSE\", pch = 19, \n     main = \"Cross-Validation RMSE for ARIMA Models\")\nlines(rmse2, type = \"b\", col = \"red\", pch = 18)\npoints(rmse2, type = \"b\", col = \"red\", pch = 18)\n\n# Add a legend to the plot\nlegend(\"topright\", legend = c(\"Model 1 (ARIMA(4,0,2))\", \"Model 2 (ARIMA(0,0,2))\"), \n       col = c(\"blue\", \"red\"), pch = c(19, 18), lty = 1)\n```\n\n```{r}\n# Output \nmean_rmse1\nmean_rmse2\n```\n\nBased on the cross validation, we can see that the conclusion aligns, the fit1 which is (0,0,2) performs better with smaller errors.\n\n\n## Fit the model\n\n```{r}\nxreg1 <- cbind(saving = lg.dd.ts1[, \"saving\"],\n              sale = lg.dd.ts1[, \"sale\"])\n\n\nfit1 <- Arima(lg.dd.ts1[, \"gini\"],order=c(0,0,2),xreg=xreg1)\nsummary(fit1)\n```\n\nThe equation:\nGiven that `y_t` represents the log-transformed `homevalue` at time `t`, the ARIMA(0,0,2) can be:\n\n\\( y_t = c + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\beta_1 \\text{saving}_t + \\beta_2 \\text{sale}_t + e_t \\)\n\nBased on the summary, my equation is\n\n\\( y_t = 0.5375 - 0.4973 e_{t-1} - 0.5027 e_{t-2} - 0.0204 \\times \\text{saving}_t + 0.2789 \\times \\text{sale}_t + e_t \\)\n\n\n## Forecast\n```{r}\nsfit<-auto.arima(lg.dd.ts1[, \"sale\"]) \nsummary(sfit) \n```\n\n```{r}\nfs<-forecast(sfit,80) #obtaining forecasts\n\ns2fit<-auto.arima(lg.dd.ts1[, \"saving\"]) #fitting an ARIMA model to the Import variable\nsummary(s2fit)\n```\n\n```{r,warning=FALSE}\nfs2<-forecast(s2fit,80)\n\nfxreg <- cbind(\n              sale = fs2$mean,saving = fs$mean) #fimp$mean gives the forecasted values\n\n\n\nfcast <- forecast(fit1, xreg=fxreg,80) \nautoplot(fcast, main=\"Forecast of Home Values\") + xlab(\"Year\") +\n  ylab(\"Home\")\n```\n\n\n\n# 5. (VAR) Home Value ~ Saving Rate + Gdp Deflator + Sale Price\n\n\n## Time Series Transformation\n```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}\n\ndf9 <- read.csv(\"../Dataset/project/USAUCSFRCONDOSMSAMID.csv\")\n\ndf9$DATE <- as.Date(df9$DATE)\n\n\n\n# Transform all to time series\n\nsaving =ts(df1$W398RC1A027NBEA)\nincome =ts(df2$MEHOINUSA672N)\nsale = ts(df3$MSPUS)\ngini =ts(df4$SIPOVGINIUSA)\nafford =ts(df5$FIXHAI)\nsaverate =ts(df6$PSAVERT)\ngdp =ts(df7$A191RI1Q225SBEA)\n\n\nhomevalue =ts(df9$USAUCSFRCONDOSMSAMID)\n\n```\n\n\n## Select the Variables with common Dates\nHome Value ~ Saving Rate + Gdp Deflator + Sale Price\n```{r}\n# Define start and end dates\nstart_date <- as.Date(\"2000-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Subset data frames to include only data from 1992 through 2020\ndf3_sub <- subset(df3, DATE >= start_date & DATE <= end_date)\ndf6_sub <- subset(df6, DATE >= start_date & DATE <= end_date)\ndf7_sub <- subset(df7, DATE >= start_date & DATE <= end_date)\ndf9_sub <- subset(df9, DATE >= start_date & DATE <= end_date)\n\n# Assuming the data is annual for this example. If it's quarterly, you'd use frequency = 4; if monthly, frequency = 12.\nfrequency <- 4\n\n# Create ts objects\nsale <- ts(df3_sub$MSPUS, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nsaving <- ts(df6_sub$PSAVERT, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\ngdp <- ts(df7_sub$A191RI1Q225SBEA, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nhomevalue <- ts(df9_sub$USAUCSFRCONDOSMSAMID, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\nDATE <- ts(df7_sub$DATE, start = c(2000, 1), end = c(2020, 1), frequency = frequency)\n```\n\n\n\n## Combine the Variables\n```{r}\ndd3<-data.frame(homevalue,saving,gdp,sale,DATE)\n\ncolnames(dd3)<-c(\"homevalue\",\"saving\",\"gdp_deflator\", \"sale\",'date')\n\nknitr::kable(head(dd3))\n```\n\n## Plot the Figure Together\n\n```{r,warning=FALSE}\nlg.dd3 <- data.frame(\"date\" =dd3$date,\"sale\"=log(dd3$sale),\"saving\"=log(dd3$saving),\"homevalue\"=log(dd3$homevalue),\n                                        \"gdp_deflator\"=log(dd3$gdp))\n\n#### converting to time series component #########\nlg.dd.ts3<-ts(lg.dd3,frequency = 4)\n\n##### Facet Plot #######################\nautoplot(lg.dd.ts3[,c(2:5)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"The GDP Deflator, Sale Price, Saving Rate, Home value in USA\")\n```\n\n## Fitting a VAR model\nFinding out the best p:\n```{r}\nVARselect(dd3[, c(2:5)], lag.max=14, type=\"both\")\n```\nFrom the results, we can see that 13,14 have relatively smaller criterias:\n\nWe can fit several models with p=13, 14.=>  VAR(13), VAR(14)\n```{r}\nsummary(vars::VAR(dd3[, c(2:5)], p=13, type='both'))\n```\n```{r}\nsummary(vars::VAR(dd3[, c(2:5)], p=14, type='both'))\n```\n\n\nWe can see that some models have some variables significant but not all. We will keep the variables and make comparison in the next steps to get the better one.\n\n\n## Cross Validations\n\n```{r}\nn=length(dd3$gdp_deflator)\nn*0.3\n```\n```{r}\nk=25 #19*4\nn-k\n```\n\n```{r}\ndd3\n```\n\n\n\n\n\n\n\n\n```{r}\ndat = ts(dd3[,c(1,2,3,4)])\n```\n\n### Cross Validation For Home Value\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(1)][1:(k-1)+i] \n  xtest <- dat[,c(1)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n```\n\n\n#### RMSE Plot\n\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Home Value\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### RSME Comparesions\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\n### Cross Validation For Saving Rate\n\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(2)][1:(k-1)+i] \n  xtest <- dat[,c(2)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n```\n\n\n#### RMSE PLOT\n\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Saving Rate\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### RMSE Compares\n\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\n\n### Cross Validation For GDP Deflator\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(3)][1:(k-1)+i] \n  xtest <- dat[,c(3)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n```\n\n\n#### RMSE Plots\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For GDP Deflator\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### RMSE Compares\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\n### Cross Validation For Sale Price\n\n```{r}\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- dat[,c(4)][1:(k-1)+i] \n  xtest <- dat[,c(4)][k+i] \n  \n  fit <- vars::VAR(dat, p=13, type='both')\n  fcast1 <- forecast(fit, h=4)\n  fcast1 = predict(fit, n.ahead = 12, ci = 0.95)\n \n  \n  fit2 <- vars::VAR(dat, p=14, type='both')\n  fcast2 <- forecast(fit2, h=4)\n  fcast2 = predict(fit2, n.ahead = 12, ci = 0.95)\n  \n  #capture error for each iteration\n  # This is RMSE\n  err1 = c(err1, sqrt((fcast1$fcst$gdp_deflator-xtest)^2))\n  err2 = c(err2, sqrt((fcast2$fcst$gdp_deflator-xtest)^2))\n\n}\n\n\n```\n\n\n#### RMSE Plots\n\n```{r}\n# Normalize function\nnormalize <- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\n# Normalize e1 and e2\nnormalized_e1 <- normalize(err1)\nnormalized_e2 <- normalize(err2)\n\n\n\n# Set the background color of the plot region to #E0E0E0\npar(bg = \"#E0E0E0\")\n\n# Plot the normalized e1 with a red line\nplot(normalized_e1+ e, type=\"l\", col=\"red\", ylim=range(c(normalized_e1, normalized_e2 + e)), xlab=\"Index\", ylab=\"Normalized Values\", main=\"Normalized RMSE Plot of e1 and e2 For Sale Price\")\n\n# Add the normalized e2 with an offset and a blue line\nlines(normalized_e2 , type=\"l\", col=\"blue\")\n\n# Add a legend to distinguish the lines\nlegend(\"topright\", legend=c(\"e1\", \"e2\"), col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n\n\n```\n\n#### RMSE Values Compares\n```{r}\nerror1 <- as.numeric(err1)\nerror1 <- error1[!is.na(error1) & !is.nan(error1)]\nerror1 <- mean(error1)\n\nerror2 <- as.numeric(err2)\nerror2 <- error2[!is.na(error2) & !is.nan(error2)]\nerror2 <- mean(error2)\n\n\nerror1\nerror2\n\n```\n\n\n\nThe second one VAR(14) is better for Home Value and Sale Price, However, overall, not too different\nThe first one VAR(13) is better for GDP Deflator and Saving Rate\nTherefore, VAR(13) is relatively better than VAR(14)\n\n\n\n\n\n## Forecasts\n```{r}\n\nfit1 <- vars::VAR(dat[,1:4], p=13, type=\"const\")\nfcast1 = predict(fit1, n.ahead = 8, ci = 0.95)\nfcast1$fcst$gdp_deflator\n```\n\n```{r}\nfcast1$fcst$sale\n```\n\n\n```{r}\nfcast1$fcst$saving\n```\n\n```{r}\nfcast1$fcst$homevalue\n```\n\n```{r}\nfcast1$fcst$sale\n```\n\n```{r}\nforecast(fit1,48) %>%\n  autoplot() + xlab(\"Year\")\n```\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/layout.css","./styles/layout.css"],"toc":true,"output-file":"ASV.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","theme":"sandstone","title":"ARIMAX/SARIMAX/VAR","navbar":{"left":["about.qmd","Introduction.qmd","DataSources.qmd","DataVis.qmd","EDA.qmd","ARModels.qmd","ASV.qmd","SAF.qmd","GARCH.qmd","TS.qmd","conclusion.qmd","dv.qmd"]}},"extensions":{"book":{"multiFile":true}}}}}