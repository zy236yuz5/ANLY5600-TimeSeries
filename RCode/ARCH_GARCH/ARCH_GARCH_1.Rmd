---
title: "datavis"
author: "Zonghong Yu"
date: "2023-09-18"
output: html_document
---

```{r}
library(reticulate)
library(ggplot2)
library(forecast)
library(astsa) 
library(gridExtra)
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(lubridate)
library(plotly)
library(TTR) # For the SMA function

```


```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
df1 <- read.csv("../../Dataset/project/household_saving.csv")
df2 <- read.csv("../../Dataset/project/MEHOINUSA672N.csv")
df3 <- read.csv("../../Dataset/project/MSPUS.csv")
df4 <- read.csv("../../Dataset/project/SIPOVGINIUSA.csv")
df5 <- read.csv("../../Dataset/project/FIXHAI.csv")
df6 <- read.csv("../../Dataset/project/PSAVERT.csv")
df7 <- read.csv("../../Dataset/project/A191RI1Q225SBEA.csv")
df8 <- read.csv("../../Dataset/project/Merged_downsample.csv")

df1$DATE <- as.Date(df1$DATE)
df2$DATE <- as.Date(df2$DATE)
df3$DATE <- as.Date(df3$DATE)
df4$DATE <- as.Date(df4$DATE)
df5$DATE <- as.Date(df5$DATE)
df6$DATE <- as.Date(df6$DATE)
df7$DATE <- as.Date(df7$DATE)
df8$date <- as.Date(df8$date)
head(df8)
```

# Calculate the Returns
```{r}
# Load necessary library
library(dplyr)



# Calculate returns
data <- df8 %>%
  mutate(
    Home_Value_Return = (Mean.Home.Value - lag(Mean.Home.Value)) / lag(Mean.Home.Value) * 100,
    Rental_Price_Return = (mean - lag(mean)) / lag(mean) * 100,
    Sale_Price_Return = (Mean.Sale.Price - lag(Mean.Sale.Price)) / lag(Mean.Sale.Price) * 100
  )



```


```{r}
san_jose_data <- data %>% 
  filter(Metropolitan.Area == "San Jose, CA")
head(san_jose_data)
```

# Plot For Rental, Sale, Home Values For San Jose, CA
```{r,message=FALSE}
library(plotly)

# Your existing plot_ly code
p <- plot_ly(data = san_jose_data, x = ~date) %>%
     add_trace(y = ~Rental_Price_Return, name = 'Mean Rental Price', mode = 'lines') 

# Adding background color and title
p <- p %>% layout(
    title = "Mean Rental Price Over Time",  # Add your title here
    paper_bgcolor = '#E0E0E0',  # Set the background color of the plotting area
    plot_bgcolor = '#E0E0E0'  # Set the background color of the graph
)

# Display the plot
p

```

```{r}
htmlwidgets::saveWidget(p, "../../image/GARCH/rental.html")
```

```{r,message=FALSE}
p <- plot_ly(data = san_jose_data, x = ~date) %>%
     add_trace(y = ~Mean.Sale.Price, name = 'Mean Sale Price', mode = 'lines') %>%
     add_trace(y = ~Mean.Home.Value, name = 'Mean Home Value', mode = 'lines') 
# Adding background color and title
p <- p %>% layout(
    title = "Mean Sale Price and Home Value Over Time",  # Add your title here
    paper_bgcolor = '#E0E0E0',  # Set the background color of the plotting area
    plot_bgcolor = '#E0E0E0'  # Set the background color of the graph
)

p
```

```{r}
htmlwidgets::saveWidget(p, "../../image/GARCH/hs.html")
```


# Return Plots
```{r,warning=FALSE}
library(ggplot2)

p <- ggplot(san_jose_data, aes(x = date)) +
  geom_line(aes(y = Sale_Price_Return), color = "blue") +
  geom_line(aes(y = Home_Value_Return), color = "red") +
  geom_line(aes(y = Rental_Price_Return), color = "green") +
  labs(title = "Time Series Plot Of Returns", x = "Date", y = "Value") +
  theme_minimal()

# Modify the background color
p <- p + theme(
    plot.background = element_rect(fill = "#E0E0E0", color = NA), # Background of the entire plot
    panel.background = element_rect(fill = "#E0E0E0", color = NA)  # Background of the plotting area
)

# Display the plot
p

```



## Stationarity
The plot shows a clear upward trend in both Mean Sale Price and Mean Home Value over time until what appears to be a sharp drop in the most recent data point. This trend indicates that the series is likely non-stationary because the mean of the series is changing over time. The presence of a trend is a strong indication that at least the mean is not constant.
The drop at the end could be due to the incident of Covid-19, an extreme value, or a real market crash. 
## Volatility
The series seems to exhibit periods of different volatility levels. Initially, there is some fluctuation, but the variations appear relatively stable and small. However, the spike at the end of the series suggests a sudden increase in volatility.
Volatility in financial time series is often clustered; periods of high volatility are followed by periods of high volatility, and periods of low volatility are followed by periods of low volatility. This plot suggests such clustering might be present, although the spike at the end may skew this perception.




```{r}

df1 = san_jose_data

df1$date <- as.Date(df1$date, format = "%Y-%m-%d")
df1

```
# First Fit a Linear Model

I think that the home value can be consisted of the sale price and rental price
```{r,warning=FALSE}
library(caret)
```


```{r}
set.seed(236) # for reproducibility
splitIndex <- createDataPartition(df1$Home_Value_Return, p = 0.8, list = FALSE)
train <- df1[splitIndex, ]
test <- df1[-splitIndex, ]
head(train)
```

```{r}
# Linear model and check coefficients
model <- lm(Mean.Home.Value ~ Mean.Sale.Price + mean, data = df1)
summary(model)
```



The coefficients are significant

```{r,warning=FALSE}
library(car)
vif(model)
```

We can see that the Mean sale price and rental price have much relatively low VIF scores. Therefore, we can keep them in modeling:

# Evaluations of the model
```{r}
predictions <- predict(model, newdata = test)
SSE <- sum((predictions - test$Mean.Home.Value)^2)
SST <- sum((test$Mean.Home.Value - mean(train$Mean.Home.Value))^2)
rsquared_test <- 1 - SSE/SST
rsquared_test 
```

## Prepare to fit the model and get residuals

```{r}
lm.residuals <- residuals(model)
plot(lm.residuals, ylab = "Residuals", main = "Residuals ")
```

## check correlation in these residuals using an ACF plot 

```{r}

acf_plot <- ggAcf(lm.residuals) +
  labs(title = "ACF for Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(lm.residuals) +
  labs(title = "PACF for Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(acf_plot, pacf_plot, ncol = 1)             

```



Based on the acf and pacf, we can choose p = 0,1,2,3 q = 0,1,2

```{r,warning=FALSE}
library(tseries)
adf.test(lm.residuals)
```

WE can see that there are little correlations left. The lm residuals are stationary now.


##  Fit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH


### First, determine the ARIMA model using model diagnostic


```{r,warning=FALSE}
# Reference from the lab 6 part 1 demo:
temp.ts = log(diff(lm.residuals))
d=0
i=1
temp= data.frame()
ls=matrix(rep(NA,6*71),nrow=71)

for (p in 1:5)
{
  for(q in 1:5)
  {
    for(d in 0:2)#
    {
      
      if(p-1+d+q-1<=8)
      {
        
        model<- Arima(temp.ts,order=c(p-1,d,q-1),include.drift=FALSE) 
        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)
        i=i+1
        #print(i)
        
      }
      
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

knitr::kable(temp)
```
## Evaluations using AIC, BIC, and AICc

```{r}
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]
```

We can see that  for (0,1,1) and (0,1,4), the AIC, BIC, and AICc are different. We should choose from them.



## Compare ARIMA Models for (4,0,4) and ARIMA (0,0,0)
```{r,warning=FALSE}
set.seed(236)

model_output21 <- capture.output(sarima(lm.residuals, 0,1,1)) 
model_output22 <- capture.output(sarima(lm.residuals, 0,1,4)) 
```

```{r}
cat(model_output21[120:147], model_output21[length(model_output21)], sep = "\n")
```

```{r}

cat(model_output22[50:80], model_output22[length(model_output22)], sep = "\n")
```

Based on the model comparsion and diagnostic, I think that ARIMA Models for (0,1,1) is better with smaller evaluation matrices and it is more suitable and proper based on acf and pacf plots. In addition, the acf of residule is also more stationary.


## Fit the best one ARIMA(0,1,1)

```{r}

mean_value <- mean(temp.ts, na.rm = TRUE)

# Replace NaN values with the mean
temp.ts[is.na(temp.ts)] <- mean_value

```

```{r}
fit2 <- arima(temp.ts, order = c(0,1,1))
summary(fit2)
```
## get the residuals of the arima model
```{r}
arima.res <- residuals(fit2)
# Plot the residuals


acf_plot <- ggAcf(arima.res) +
  labs(title = "ACF for Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(arima.res) +
  labs(title = "PACF for Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(acf_plot, pacf_plot, ncol = 1)             
```




We can see that mostly, the residual is stationary however the residuals of ARIMA model indicates some flutucations. 
In this case, I think that we can try to fit an ARCH/GARCH model.

Therefore, I think we can further making analysis by adding GARCH models to see if we should use it.

## Model Diagnostics For ARCH/GARCH model 


```{r}


acf_plot <- ggAcf(arima.res^2) +
  labs(title = "ACF for Squared Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )
pacf_plot <- ggPacf(arima.res^2) +
  labs(title = "PACF for Squared Model Residuals") +
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Combine the plots
grid.arrange(acf_plot, pacf_plot, ncol = 1)             

```

We can see that it acutally already sufficient to just use ARIMA model. GARCH model is not required since it is already pretty good in removing correlations. The residual and squared residual are stationary.

However, I still think that I can at least try to make a further diagnostic and compare the results to verify that I do not need to fit GARCH model.



```{r}
arima.res[is.na(arima.res)] <- mean(arima.res, na.rm = TRUE)  # Or median


```

```{r,warning=FALSE}
model <- list() ## set counter
cc <- 1
for (p in 1:7) {
  for (q in 1:7) {
  
model[[cc]] <- garch(arima.res,order=c(q,p),trace=F)
cc <- cc + 1
}
} 

## get AIC values for model evaluation
GARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best
```

```{r}
which(GARCH_AIC == min(GARCH_AIC))
model[[which(GARCH_AIC == min(GARCH_AIC))]]
```

From here, I think that garch(5,1) is a good choice

```{r,warning=FALSE}
library(fGarch)
summary(garchFit(~garch(5,1), arima.res,trace = F)) 
```

The results shows that (5,1) is not an optimal fit. The coefficients are not significant.

Therefore, let us try (1,0)

## Try to compare with Garch(1,0)
```{r}
summary(garchFit(~garch(1,0), arima.res,trace = F)) 
```

Now, it seems that the alpha1 is still not significant.
Therefore, in this case, we can stop with arima model. No additional Garch is needed.

However, we still can compare the prediction fits.

## Forecast 

### ARIMA (0,1,1)

```{r}

# Autoplot with custom colors
plot_fit_ <- autoplot(forecast(fit2,10)) + 
  theme(
    panel.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    plot.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.background = element_rect(fill = "#E0E0E0", color = "#E0E0E0"),
    legend.key = element_rect(fill = "#E0E0E0", color = "#E0E0E0")
  )

# Display the plot
print(plot_fit_)
```



By comparing the fits, I think that the ARIMA model alone is better. We do not need the GArch in my case

## Final model: ARIMA (0,1,1) and equation

### Box Ljung Test
```{r}
box_ljung_test <- Box.test(arima.res, lag = 10, type = "Ljung-Box")

# Display the test results
box_ljung_test
```

the p-value is above a significance level 0.05, therefore, I do not reject the null hypothesis. This suggests that the residuals do not exhibit autocorrelation and that the model is adequate. This conclusion alignes with my model diagnostics and forecasting comparesion. The ARIMA(0,2,3) alone is enough in my case.


### Equation

The ARIMA(0,1,1) model is defined as:

$$
(1 - B) X_t = (1 + \theta_1B)a_t
$$

where:
- \( B \) is the backshift operator,
- \( X_t \) is the time series observation at time t,
- \( (1 - B) \) denotes first-order differencing,
- \( \theta_1 \) is the parameter of the moving average part of the model,
- \( a_t \) is the error term at time t.

This model is a simplification of higher-order ARIMA models and is used when the data exhibits a level of temporal dependency that can be explained with a single differencing and a single lag in the moving average component.


No Garch components since in my case the ARIMA is better and sufficient.
